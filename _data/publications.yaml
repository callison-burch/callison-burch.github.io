-
   title: CLIN&colon; A Continually Learning Language Agent for Rapid Task Adaptation and Generalization
   authors: Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, Peter Clark
   venue: COLM
   type: conference
   year: 2024
   url: https://www.cis.upenn.edu/~ccb/publications/faithful-model-explanations-survey.pdf
   page_count: 23
   id: clin-continual-learning-from-interactions
   data: 
   code: 
   website:  https://allenai.github.io/clin/
   abstract: | 
      Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. While recent work, e.g., Reflexion, has demonstrated how such agents can also self-improve by adding a textual memory of “hints” learned from prior experience, such improvements have been limited both in size and scope. In contrast, our goal is a language agent that can robustly improve performance over time, including when both the task and environment are varied. Our approach is to have the agent learn a textual representation of how the world works (rather than just isolated hints), expressed as a memory of causal abstractions, to guide future decision-making. In experiments, we find CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 points in ScienceWorld and 1.4 points in ALFWorld benchmarks. CLIN can also transfer its learning to new environments and tasks, enhancing performance by 21 points in ScienceWorld and 11 points in ALFWorld. This suggests that language agents with a textual causal memory can play a significant role in interactive environments, including being able to rapidly improve over time.
   bibtex: |
      @inproceedings{clin-continual-learning-from-interactions,
        title = {{CLIN}: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization},
        author = {Majumder, Bodhisattwa Prasad and Dalvi Mishra, Bhavana and Jansen, Peter and Tafjord, Oyvind and Tandon, Niket and Zhang, Li and Callison-Burch, Chris and Clark, Peter},
        booktitle = {Conference on Language Modeling ({COLM})},
        year = {2024},
        url = {https://www.cis.upenn.edu/~ccb/publications/faithful-model-explanations-survey.pdf},
        address = {Philadelphia, PA},
        month = {October}
      }
-
   title: DataDreamer&colon; A Tool for Synthetic Data Generation and Reproducible LLM Workflows
   authors: Ajay Patel, Colin Raffel, Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2024
   url: https://arxiv.org/abs/2402.10379
   page_count: 29
   id: datadreamer
   data: 
   code: https://datadreamer.dev/docs/latest/
   website:  https://datadreamer.dev/docs/latest/
   abstract: | 
      Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at this https URL .
   bibtex: |
      @inproceedings{patel2024datadreamer,
          title={{DataDreamer}: A Tool for Synthetic Data Generation and Reproducible LLM Workflows},
          author={Ajay Patel and Colin Raffel and Chris Callison-Burch},
          booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)},
          year={2024},
          address={Bangkok, Thailand},
          month={August 11--16},
          eprint={2402.10379},
          archivePrefix={arXiv},
          primaryClass={cs.CL}
      }
-
   title: FanOutQA&colon; Multi-Hop, Multi-Document Question Answering for Large Language Models
   authors: Andrew Zhu, Alyssa Hwang, Liam Dugan, Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2024
   url: https://arxiv.org/abs/2402.14116
   page_count: 18
   id: fanoutqa
   data: https://github.com/zhudotexe/fanoutqa/tree/main/fanoutqa/data
   code: https://github.com/zhudotexe/fanoutqa
   website: https://fanoutqa.com
   abstract: | 
      One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at fanoutqa.com
   bibtex: |
      @inproceedings{zhu2024fanoutqa,
          title={{FanOutQA}: Multi-Hop, Multi-Document Question Answering for Large Language Models},
          author={Andrew Zhu and Alyssa Hwang and Liam Dugan and Chris Callison-Burch},
          booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)},
          year={2024},
          address={Bangkok, Thailand},
          month={August 11--16},
          eprint={2402.14116},
          archivePrefix={arXiv},
          primaryClass={cs.CL}
      }
-
   title: RAID&colon; A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors
   authors: Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh Magnus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ippolito, and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2024
   url: https://arxiv.org/abs/2405.07940
   page_count: 26
   id: raid-dataset
   video: https://www.youtube.com/watch?v=80xSB3gGJx8
   data: 
   code: 
   website: 
   abstract: | 
      Many commercial and open-source models claim to detect machine-generated text with very high accuracy (99\% or higher). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging -- lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machine-generated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-of-domain and adversarial robustness of 8 open- and 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our dataset and tools to encourage further exploration into detector robustness.
   bibtex: |
      @inproceedings{dugan2024raid,
          title={{RAID}: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors},
          author={Liam Dugan and Alyssa Hwang and Filip Trhlik and Josh Magnus Ludan and Andrew Zhu and Hainiu Xu and Daphne Ippolito and Chris Callison-Burch},
          booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)},
          year={2024},
          address={Bangkok, Thailand},
          month={August 11--16},
          eprint={2405.07940},
          archivePrefix={arXiv},
          primaryClass={cs.CL}
      }
   press: 
    - 
      title: Detecting Machine-Generated Text&colon; An Arms Race With the Advancements of Large Language Models
      source: Penn Engineering Today
      by_line: Melissa Pappas
      date: August 12, 2024
      url: https://blog.seas.upenn.edu/detecting-machine-generated-text-an-arms-race-with-the-advancements-of-large-language-models/
-
   title: Holodeck&colon; Language Guided Generation of 3D Embodied AI Environments
   authors: Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark 
   venue: CVPR
   type: conference
   year: 2024
   url: https://yueyang1996.github.io/papers/holodeck.pdf
   page_count: 20
   id: holodeck
   data: 
   code: https://github.com/allenai/Holodeck
   website: https://yueyang1996.github.io/holodeck/
   abstract: |
      3D simulated environments play a critical role in Embodied AI, but their creation requires expertise and extensive manual effort, restricting their diversity and scope. To mitigate this limitation, we present Holodeck, a system that generates 3D environments to match a user-supplied prompt fully automatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and museums, adjust the designs for styles, and can capture the semantics of complex queries such as "apartment for a researcher with a cat" and "office of a professor who is a fan of Star Wars". Holodeck leverages a large language model (GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects. To address the challenge of positioning objects correctly, we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints. Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types. We also demonstrate an exciting application of Holodeck in Embodied AI, training agents to navigate in novel scenes like music rooms and daycares without human-constructed data, which is a significant step forward in developing general-purpose embodied agents.
   bibtex: |
      @inproceedings{yang-etal-2024-holodeck,
        title = {Holodeck: Language Guided Generation of 3D Embodied AI Environments},
        author = {Yue Yang and Fan-Yun Sun and Luca Weihs and Eli VanderBilt and Alvaro Herrasti and Winson Han and Jiajun Wu and Nick Haber and Ranjay Krishna and Lingjie Liu and Chris Callison-Burch and Mark Yatskar and Aniruddha Kembhavi and Christopher Clark },
        booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)},
        year = {2024},
        address = {Seattle, Washington},
        publisher = "IEEE/CVF",
        url = {https://yueyang1996.github.io/papers/holodeck.pdf}
      }
   press: 
    - 
      title: Recreating ‘Star Trek’ Virtual Environments&colon; Holodeck generates a virtually limitless range of indoor environments, using AI to interpret users’ requests.
      source: Tech Briefs
      date: June 11, 2024
      url: https://www.techbriefs.com/component/content/article/50943-recreating-star-trek-virtual-environments
-
   title: CoMo&colon; Controllable Motion Generation through Language Guided Pose Code Editing
   authors: Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch, Mark Yatskar, Lingjie Liu
   venue: ECCV
   type: conference
   year: 2024
   url: https://arxiv.org/abs/2403.13900
   page_count: 27
   id: controllable-motion-generation
   data: 
   code: 
   website: https://yh2371.github.io/como/
   abstract: |
      Text-to-motion models excel at efficient human motion generation, but existing approaches lack fine-grained controllability over the generation process. Consequently, modifying subtle postures within a motion or inserting new actions at specific moments remains a challenge, limiting the applicability of these methods in diverse scenarios. In light of these challenges, we introduce CoMo, a Controllable Motion generation model, adept at accurately generating and editing motions by leveraging the knowledge priors of large language models (LLMs). Specifically, CoMo decomposes motions into discrete and semantically meaningful pose codes, with each code encapsulating the semantics of a body part, representing elementary information such as "left knee slightly bent". Given textual inputs, CoMo autoregressively generates sequences of pose codes, which are then decoded into 3D motions. Leveraging pose codes as interpretable representations, an LLM can directly intervene in motion editing by adjusting the pose codes according to editing instructions. Experiments demonstrate that CoMo achieves competitive performance in motion generation compared to state-of-the-art models while, in human studies, CoMo substantially surpasses previous work in motion editing abilities.
   bibtex: |
      @inproceedings{huang2024comocontrollablemotiongeneration,
            title={{CoMo}: Controllable Motion Generation through Language Guided Pose Code Editing}, 
            author={Yiming Huang and Weilin Wan and Yue Yang and Chris Callison-Burch and Mark Yatskar and Lingjie Liu},
            booktitle={Proceedings of the 18th European Conference on Computer Vision (ECCV 2024)},
            year={2024},
            month={September},
            address={Milan, Italy},
            eprint={2403.13900},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={https://arxiv.org/abs/2403.13900}, 
      }
-
   title: ReDel&colon; A Toolkit for LLM-Powered Recursive Multi-Agent Systems
   authors: Andrew Zhu, Liam Dugan, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2024
   url: https://arxiv.org/abs/2408.02248
   page_count: 
   id: recursive-multi-agent-llms
   data: https://datasets.mechanus.zhu.codes/redel-dist.zip
   code: https://github.com/zhudotexe/redel
   website: 
   abstract: |
      Recently, there has been increasing interest in using Large Language Models (LLMs) to construct complex multi-agent systems to perform tasks such as compiling literature reviews, drafting consumer reports, and planning vacations. Many tools and libraries exist for helping create such systems, however none support recursive multi-agent systems -- where the models themselves flexibly decide when to delegate tasks and how to organize their delegation structure. In this work, we introduce ReDel: a toolkit for recursive multi-agent systems that supports custom tool-use, delegation schemes, event-based logging, and interactive replay in an easy-to-use web interface. We show that, using ReDel, we are able to achieve significant performance gains on agentic benchmarks and easily identify potential areas of improvements through the visualization and debugging tools. Our code, documentation, and PyPI package are open-source and free to use under the MIT license.
   bibtex: |
      @misc{zhu2024redeltoolkitllmpoweredrecursive,
            title={{ReDel}: A Toolkit for LLM-Powered Recursive Multi-Agent Systems}, 
            author={Andrew Zhu and Liam Dugan and Chris Callison-Burch},
            year={2024},
            eprint={2408.02248},
            archivePrefix={arXiv},
            primaryClass={cs.CL},
            url={https://arxiv.org/abs/2408.02248}, 
      }
-
   title: A Textbook Remedy for Domain Shifts&colon; Knowledge Priors for Medical Image Analysis
   authors: Yue Yang, Mona Gandhi, Yufei Wang, Yifan Wu, Michael S. Yao, Chris Callison-Burch, James C. Gee, Mark Yatskar
   venue: arXiv
   type: preprint
   year: 2024
   url: https://arxiv.org/abs/2405.14839
   page_count: 
   id: knowledge-priors-for-medical-images
   data: 
   code: 
   website:
   abstract: |
      While deep networks have achieved broad success in analyzing natural images, when applied to medical scans, they often fail in unexcepted situations. We investigate this challenge and focus on model sensitivity to domain shifts, such as data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc, in the context of chest X-rays and skin lesion images. A key finding we show empirically is that existing visual backbones lack an appropriate prior from the architecture for reliable generalization in these settings. Taking inspiration from medical training, we propose giving deep networks a prior grounded in explicit medical knowledge communicated in natural language. To this end, we introduce Knowledge-enhanced Bottlenecks (KnoBo), a class of concept bottleneck models that incorporates knowledge priors that constrain it to reason with clinically relevant factors found in medical textbooks or PubMed. KnoBo uses retrieval-augmented language models to design an appropriate concept space paired with an automatic training procedure for recognizing the concept. We evaluate different resources of knowledge and recognition architectures on a broad range of domain shifts across 20 datasets. In our comprehensive evaluation with two imaging modalities, KnoBo outperforms fine-tuned models on confounded datasets by 32.4% on average. Finally, evaluations reveal that PubMed is a promising resource for making medical models less sensitive to domain shift, outperforming other resources on both diversity of information and final prediction performance.
   bibtex: |
      @misc{yang2024textbookremedydomainshifts,
            title={A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis}, 
            author={Yue Yang and Mona Gandhi and Yufei Wang and Yifan Wu and Michael S. Yao and Chris Callison-Burch and James C. Gee and Mark Yatskar},
            year={2024},
            eprint={2405.14839},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={https://arxiv.org/abs/2405.14839}, 
      }
-
   title: Evaluating Vision-Language Models on Bistable Images
   authors: Artemis Panagopoulou, Coby Melkin, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2024
   url: https://arxiv.org/abs/2405.19793
   page_count: 
   id: bistable-images-with-vlms
   data: 
   code: 
   website:
   abstract: |
      Bistable images, also known as ambiguous or reversible images, present visual stimuli that can be seen in two distinct interpretations, though not simultaneously by the observer. In this study, we conduct the most extensive examination of vision-language models using bistable images to date. We manually gathered a dataset of 29 bistable images, along with their associated labels, and subjected them to 116 different manipulations in brightness, tint, and rotation. We evaluated twelve different models in both classification and generative tasks across six model architectures. Our findings reveal that, with the exception of models from the Idefics family and LLaVA1.5-13b, there is a pronounced preference for one interpretation over another among the models, and minimal variance under image manipulations, with few exceptions on image rotations. Additionally, we compared the model preferences with humans, noting that the models do not exhibit the same continuity biases as humans and often diverge from human initial interpretations. We also investigated the influence of variations in prompts and the use of synonymous labels, discovering that these factors significantly affect model interpretations more than image manipulations showing a higher influence of the language priors on bistable image interpretations compared to image-text training data. All code and data is open sourced.
   bibtex: |
      @misc{panagopoulou2024evaluatingvisionlanguagemodelsbistable,
            title={Evaluating Vision-Language Models on Bistable Images}, 
            author={Artemis Panagopoulou and Coby Melkin and Chris Callison-Burch},
            year={2024},
            eprint={2405.19423},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={https://arxiv.org/abs/2405.19423}, 
      }
-
   title: PROC2PDDL&colon; Open-Domain Planning Representations from Texts
   authors: Tianyi Zhang, Harry Li Zhang, Zhaoyi Hou, Ziyu Wang, Yuling Gu, Peter Clark, Chris Callison-Burch, Niket Tandon
   venue: Natural Language Reasoning and Structured Explanations Workshop
   type: workshop
   year: 2024
   url: https://arxiv.org/abs/2403.00092
   page_count: 
   id: proc2pddl
   data: 
   code: 
   website:
   abstract: |
      Planning in a text-based environment continues to be a major challenge for AI systems. Recent approaches have used language models to predict a planning domain definition (e.g., PDDL) but have only been evaluated in closed-domain simulated environments. To address this, we present Proc2PDDL , the first dataset containing open-domain procedural texts paired with expert-annotated PDDL representations. Using this dataset, we evaluate state-of-the-art models on defining the preconditions and effects of actions. We show that Proc2PDDL is highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around 35%. Our analysis shows both syntactic and semantic errors, indicating LMs' deficiency in both generating domain-specific prgorams and reasoning about events. We hope this analysis and dataset helps future progress towards integrating the best of LMs and formal planning.
   bibtex: |
      @inproceedings{zhang2024proc2pddlopendomainplanningrepresentations,
            title={{PROC2PDDL}: Open-Domain Planning Representations from Texts}, 
            author={Tianyi Zhang and Li Zhang and Zhaoyi Hou and Ziyu Wang and Yuling Gu and Peter Clark and Chris Callison-Burch and Niket Tandon},
            booktitle={Proceedings of the Natural Language Reasoning and Structured Explanations Workshop at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)},
            year={2024},
            month={August},
            address={Bangkok, Thailand},
            eprint={2403.00092},
            archivePrefix={arXiv},
            primaryClass={cs.CL},
            url={https://arxiv.org/abs/2403.00092}, 
      }
-
   title: PDDLEGO&colon; Iterative Planning in Textual Environments
   authors: Harry Li Zhang, Peter Jansen, Tianyi Zhang, Peter Clark, Chris Callison-Burch, Niket Tandon
   venue: arXiv
   type: preprint
   year: 2024
   url: https://arxiv.org/abs/2405.19793
   page_count: 
   id: pddlego
   data: 
   code: 
   website:
   abstract: |
      Planning in textual environments have been shown to be a long-standing challenge even for current models. A recent, promising line of work uses LLMs to generate a formal representation of the environment that can be solved by a symbolic planner. However, existing methods rely on a fully-observed environment where all entity states are initially known, so a one-off representation can be constructed, leading to a complete plan. In contrast, we tackle partially-observed environments where there is initially no sufficient information to plan for the end-goal. We propose PDDLEGO that iteratively construct a planning representation that can lead to a partial plan for a given sub-goal. By accomplishing the sub-goal, more information is acquired to augment the representation, eventually achieving the end-goal. We show that plans produced by few-shot PDDLEGO are 43% more efficient than generating plans end-to-end on the Coin Collector simulation, with strong performance (98%) on the more complex Cooking World simulation where end-to-end LLMs fail to generate coherent plans (4%).
   bibtex: |
      @misc{zhang2024pddlegoiterativeplanningtextual,
            title={{PDDLEGO}: Iterative Planning in Textual Environments}, 
            author={Li Zhang and Peter Jansen and Tianyi Zhang and Peter Clark and Chris Callison-Burch and Niket Tandon},
            year={2024},
            eprint={2405.19793},
            archivePrefix={arXiv},
            primaryClass={cs.CL},
            url={https://arxiv.org/abs/2405.19793}, 
      }
-
   title: PaCE&colon; Parsimonious Concept Engineering for Large Language Models
   authors: Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, René Vidal
   venue: arXiv
   type: preprint
   year: 2024
   url: https://arxiv.org/abs/2406.04331
   page_count: 
   id: parsimonious-concept-engineering
   data: 
   code: 
   website:
   abstract: |
      Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable output, via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Then, given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Finally, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activation as a linear combination of the benign and undesirable components. By removing the latter ones from the activation, we reorient the behavior of LLMs towards alignment goals. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.
   bibtex: |
      @misc{luo2024paceparsimoniousconceptengineering,
            title={{PaCE}: Parsimonious Concept Engineering for Large Language Models}, 
            author={Jinqi Luo and Tianjiao Ding and Kwan Ho Ryan Chan and Darshan Thaker and Aditya Chattopadhyay and Chris Callison-Burch and René Vidal},
            year={2024},
            eprint={2406.04331},
            archivePrefix={arXiv},
            primaryClass={cs.CL},
            url={https://arxiv.org/abs/2406.04331}, 
      }
-
   title: Large Language Models Can Self-Improve At Web Agent Tasks
   authors: Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, Sepp Hochreiter
   venue: arXiv
   type: preprint
   year: 2024
   url: https://arxiv.org/abs/2405.20309
   page_count: 
   id: llm-web-agents
   data: 
   code: 
   website:
   abstract: |
      Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.
   bibtex: |
      @misc{patel2024largelanguagemodelsselfimprove,
            title={Large Language Models Can Self-Improve At Web Agent Tasks}, 
            author={Ajay Patel and Markus Hofmarcher and Claudiu Leoveanu-Condrei and Marius-Constantin Dinu and Chris Callison-Burch and Sepp Hochreiter},
            year={2024},
            eprint={2405.20309},
            archivePrefix={arXiv},
            primaryClass={cs.LG},
            url={https://arxiv.org/abs/2405.20309}, 
      }
-
   title: TinyStyler&colon; Efficient Few-Shot Text Style Transfer with Authorship Embeddings
   authors: Zachary Horvitz, Ajay Patel, Kanishk Singh, Chris Callison-Burch, Kathleen McKeown, Zhou Yu
   venue: arXiv
   type: preprint
   year: 2024
   url: https://arxiv.org/abs/2406.15586
   page_count: 
   id: tinystyler
   data: 
   code: 
   website: https://huggingface.co/tinystyler/tinystyler
   abstract: |
      The goal of text style transfer is to transform the style of texts while preserving their original meaning, often with only a few examples of the target style. Existing style transfer methods generally rely on the few-shot capabilities of large language models or on complex controllable text generation approaches that are inefficient and underperform on fluency metrics. We introduce TinyStyler, a lightweight but effective approach, which leverages a small language model (800M params) and pre-trained authorship embeddings to perform efficient, few-shot text style transfer. We evaluate on the challenging task of authorship style transfer and find TinyStyler outperforms strong approaches such as GPT-4. We also evaluate TinyStyler's ability to perform text attribute style transfer (formal ↔ informal) with automatic and human evaluations and find that the approach outperforms recent controllable text generation methods. 
   bibtex: |
      @misc{horvitz2024tinystylerefficientfewshottext,
            title={TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship Embeddings}, 
            author={Zachary Horvitz and Ajay Patel and Kanishk Singh and Chris Callison-Burch and Kathleen McKeown and Zhou Yu},
            year={2024},
            eprint={2406.15586},
            archivePrefix={arXiv},
            primaryClass={cs.CL},
            url={https://arxiv.org/abs/2406.15586}, 
      }
-
   title: Calibrating Large Language Models with Sample Consistency
   authors: Veronica Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2024
   url: https://arxiv.org/abs/2402.13904
   page_count: 
   id: calibrating-llms
   data: 
   code: 
   website: 
   abstract: |
      Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.
   bibtex: |
      @misc{lyu2024calibrating,
            title={Calibrating Large Language Models with Sample Consistency}, 
            author={Qing Lyu and Kumar Shridhar and Chaitanya Malaviya and Li Zhang and Yanai Elazar and Niket Tandon and Marianna Apidianaki and Mrinmaya Sachan and Chris Callison-Burch},
            year={2024},
            eprint={2402.13904},
            archivePrefix={arXiv},
            primaryClass={cs.CL}
      }
-
   title: Choice-75&colon; A Dataset on Decision Branching in Script Learning
   authors: Zhaoyi Joey Hou, Li Zhang, Chris Callison-Burch
   venue: COLING
   type: conferece
   year: 2024
   url: https://arxiv.org/abs/2309.11737
   page_count: 9
   id: choice-75-dataset
   data: 
   code: 
   website:
   abstract: |
      Script learning studies how daily events unfold. Previous works tend to consider a script as a linear sequence of events while ignoring the potential branches that arise due to people's circumstantial choices. We hence propose Choice-75, the first benchmark that challenges intelligent systems to predict decisions given descriptive scenarios, containing 75 scripts and more than 600 scenarios. While large language models demonstrate overall decent performances, there is still notable room for improvement in many hard scenarios.
   bibtex: |
      @inproceedings{hou2024choice75,
            title={Choice-75: A Dataset on Decision Branching in Script Learning},
            author={Zhaoyi Joey Hou and Li Zhang and Chris Callison-Burch},
            booktitle={Proceedings of The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
            year={2024},
            address={Torino, Italy},
            month={May},
            pages={20--25}
      }
-
   title: Towards Faithful Model Explanation in NLP&colon; A Survey
   authors: Qing Lyu, Marianna Apidianaki, Chris Callison-Burch
   venue: Computational Linguistics
   type: journal
   year: 2024
   url: https://www.cis.upenn.edu/~ccb/publications/faithful-model-explanations-survey.pdf
   page_count: 65
   id: faithful-model-explanations-survey
   figures:
      -
         img: figures/faithful-model-explanations-survey/faithful-model-explanations-survey-table-1.png
         label: Table 1
         caption: Comparison of different model explanation methods in terms of their proper- ties. Different colors denote different values of a property. See Section 1.1.3 for details.
   data: 
   code: 
   abstract: |
      End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, i.e. an explanation should accurately represent the reasoning process behind the model’s prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation- based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.
   bibtex: |
      @article{Lyu-et-al:CL:2024,
         author    = {Qing Lyu and Marianna Apidianaki and Chris Callison-Burch},
         title =   {Towards Faithful Model Explanation in {NLP}: A Survey},
         journal = {Computational Linguistics},
         year =    {2024},
         volume = {50},
         number = {2},
         pages = {657–723},
         url = {https://doi.org/10.1162/coli_a_00511}
       }
-
   title: ParaGuide&colon; Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer
   authors: Zachary Horvitz, Ajay Patel, Chris Callison-Burch, Zhou Yu, Kathleen McKeown
   venue: AAAI
   type: conference
   year: 2024
   url: https://arxiv.org/abs/2308.15459
   page_count: 17
   id: paraguide
   data: 
   code: 
   website:
   abstract: | 
      Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, sentiment, and even authorship style transfer.
   bibtex: |
      @inproceedings{horvitz2023paraguide,
            title={{ParaGuide}: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer}, 
            author={Zachary Horvitz and Ajay Patel and Chris Callison-Burch and Zhou Yu and Kathleen McKeown},
            year={2024},
            address={Vancouver, Canada},
            date={February 20-27, 2024},
            eprint={2308.15459},
            archivePrefix={arXiv},
            primaryClass={cs.CL}
      }
-
   title: Understanding Generative Artificial Intelligence and Its Relationship to Copyright
   authors: Chris Callison-Burch
   venue: The U.S. House of Representatives Judiciary Committee Subcommittee on Courts, Intellectual Property, and the Internet Hearing on Artificial Intelligence and Intellectual Property&colon; Part I – Interoperability of AI and Copyright Law
   type: Testimony
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/understanding-generative-AI-and-its-relationship-to-copyright.pdf
   page_count: 24
   id: copyright-and-ai-testimony
   data: 
   code: 
   video: https://youtube.com/playlist?list=PL0S5TKwqfRKKUNWzp7rEe5uuLV-o9VC2f&si=z5cNeqF4_5xgAyHs
   video_alt: https://www.youtube.com/live/Mm1NQ_Kqumw?feature=share&t=887
   press: 
    - 
      title: Copyright Law and Generative AI&colon; What a mess
      source: American Bar Association Journal
      date: August 30, 2023
      url: https://www.abajournal.com/web/article/copyright-generative-ai-what-a-mess
    - 
      title: What does Congress need to do amid AI boom?
      source: Fox News
      date: May 20, 2023
      url: https://www.foxnews.com/video/6327934087112
   abstract: |
      Chairman Issa, Ranking Member Johnson, and distinguished Members of the Subcommittee, thank you for the opportunity to testify on the topic of artificial intelligence and intellectual property. Generative AI had its breakthrough moment last November with the release of OpenAI’s ChatGPT, bringing my field of research into the public eye and generating a huge amount of excitement. I had access to OpenAI's large language model about a year and a half before the public. Despite having been working in this field for 20 years, I was shocked at its capabilities.

      My first encounter with it pitched me into a career existential crisis. This technology had seemingly solved many of the research problems that I was working on. It could translate texts from Russian into English. It could write coherent summaries of long documents, and answer questions about them. I wondered if there was any room left in the field for academic research, because training these large language models required Google-sized data centers, and I simply can’t compete at that scale. So I asked myself, "Should I just drop out of computer science and become a poet?" But then I trained the model to write better poetry than me.

      I have subsequently calmed down, and I do not think I’m at imminent risk of losing my job to ChatGPT. But I understand that many other people are experiencing that same sense of panic that I had. Artists and writers are worried that their work will be devalued. I worry that a career as a paralegal may go the way of the lamplighter. I think that at its core, what we are talking about today goes far beyond copyright. It is about the value of work. This is a truly transformative technology that will shape many aspects of our lives. I hope that it is for the better. 

      I optimistically believe that AI will enable us all to become more productive workers, and more creative in our artistic pursuits. In my testimony today, I hope to offer the Subcommittee: 1. My expertise in the technical aspects of generative AI, I promise to explain it in a way that is understandable without requiring a PhD in computer science. 2. Answers to any questions you may have about the emerging capabilities of this technology or what impact legislation might have on innovation in this field 3. Advocacy to retain fair use for the purposes of training AI systems.

      In my written testimony, I have provided you and your staff with an overview of how Generative AI works. I’m happy to review any details about it, either during this hearing or 1-on-1 anytime in the future. To briefly summarize written testimony: Generative AI is trained on huge amounts of data. Large Language Models are now trained on roughly 1 trillion words. Image generators are trained on hundreds of millions of images. Much or most of that data consists of copyrighted works that have been gathered by automatically crawling the web.

      From this data, AI systems learn. Their learning process is called “pre-training”. Pre-training an AI system is different from how children learn, but the effect is similar. AI systems learn how to use language. They learn facts about the world, ideas and opinions, visual concepts, and they even learn some rudimentary common sense reasoning skills. After this pre-training on the huge amount of copyrighted data, that data is set aside. They are then trained to specialize in different tasks – often using custom built data sets that are much smaller. For instance, a large language model could be specialized or “fine tuned” to become an intelligent tutoring system, or a computer vision system could be “fine tuned” to recognize cancerous growths in mammograms.

      These systems could not be as easily adapted to these specialized tasks without the general knowledge that they acquired from the copyrighted texts that they were pre-trained on. I believe that pre-training squarely falls under fair use of copyrighted texts, and that Internet-era case law like Google books are good precedents that support this. I do believe that the output of Generative AI systems can infringe copyright, and that it is worth congress considering legislation to better shape copyright to govern things like copyrightable characters, or to extend it to cover Right-of-Publicity. I look forward to discussing this topic with you.
   bibtex: |
      @misc{CallisonBurch_2023,
        author = {Callison-Burch, Chris},
        title = {Understanding Generative Artificial Intelligence and Its Relationship to Copyright},
        howpublished = {Testimony before The U.S. House of Representatives Judiciary Committee, Subcommittee on Courts, Intellectual Property, and the Internet},
        month = {May},
        year = {2023},
        note = {Hearing on Artificial Intelligence and Intellectual Property: Part I – Interoperability of AI and Copyright Law},
        institution = {University of Pennsylvania, School of Engineering and Applied Sciences, Department of Computer and Information Science}
      }
-
   title: AI2's Response to the US Copyright Requence for Comments on Artificial Intelligence and Copyright
   authors: Ali Farhadi, David Atkinson, Chris Callison-Burch, Nicole DeCario, Jennifer Dumas, Kyle Lo,  Luca Soldiani
   venue: US Copyright Office Docket No. 2023-6
   type: Comment
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/AI2-response-to-US-copyright-office-2023.pdf
   page_count: 15
   id: AI2-response-to-US-copyright-office-2023
   data: 
   code: 
   abstract: |
      We are AI researchers, engineers, policy advisors, and legal counsel from The Allen Institute for Artificial Intelligence (AI2). We offer the following submission in response to the U.S. Copyright Office, Library of Congress Notice of inquiry and request for comments (RFC). Our response centers on the use of copyrighted materials to train AI Models, as defined in the RFC, and the Output derived from the Training Materials via the AI Models or AI Systems (Output).1 In this response, we provide background and details on the technical aspects of training AI Models, as outlined in the Training section of the RFC, and we offer two recommendations for consideration.
      AI2 is a non-profit research institute founded in 2014 with the mission of conducting high-impact AI research and engineering in service of the common good. AI2 is the creation of the late Paul G. Allen, philanthropist and Microsoft co-founder, and is led by Dr. Ali Farhadi. Headquartered in Seattle, AI2 employs over 200 world-class AI researchers and engineers from across the globe. We share Paul Allen’s vision and belief that AI can transform lives in positive ways.
      Generative AI has potential applications that will benefit society, including medical diagnosis, treatment, and cure research; assistive technologies for people with disabilities; intelligent tutoring systems for personalized and more equitable education; and climate modeling to predict impacts in specific regions. We also recognize the inherent and potential challenges that exist with this technology. Our focus at AI2 is to work not only on cutting edge AI research, but also at the intersection of AI ethics, AI policy, and AI literacy to create solutions that enable a future where AI is universally designed, developed, and deployed responsibly.
      Starting in March 2023, researchers at AI2 have been building a state-of-the-art generative language model called OLMo (Open Language Model). AI2 expects to publicly release OLMo in early 2024. Our goal is to produce an AI Model designed for scientific research that provides access and education around all aspects of AI Model creation. This summer we released Dolma, the Training Dataset used to create OLMo. Dolma2, consists of 3 trillion tokens from a diverse mix of web content, academic publications, software code, books, and encyclopedic materials.
      We offer our feedback here as a nonprofit research institute with first-hand experience training generative AI Models from scratch. We will describe aspects of model training, collection of Training Material for AI Models, and related copyright considerations.
   bibtex: |
      @misc{ai2response2023,
        title = {{AI2's Response to the US Copyright Requence for Comments on Artificial Intelligence and Copyright}},
        author = {Farhadi, Ali and Atkinson, David and Callison-Burch, Chris and DeCario, Nicole and Dumas, Jennifer and Lo, Kyle and Soldiani, Luca},
        howpublished = {US Copyright Office Docket No. 2023-6},
        year = {2023},
        url = {https://www.cis.upenn.edu/~ccb/publications/understanding-generative-AI-and-its-relationship-to-copyright.pdf},
        note = {Comment}
      }
-
   title: The Gender Wage Gap in an Online Labor Market&colon; The Cost of Interruptions 
   authors: Abi Adams-Prassl,  Kotaro Hara,  Kristy Milland,  Chris Callison-Burch
   venue: The Review of Economics and Statistics 
   type: journal
   year: 2023
   url: https://doi.org/10.1162/rest_a_01282
   page_count: 23
   id: gender-wage-gap-on-mturk
   abstract: |
      This paper analyzes gender differences in working patterns and wages on Amazon Mechanical Turk, a popular online labor platform. Using information on 2 million tasks, we find no gender differences in task selection nor experience. Nonetheless, women earn 20% less per hour on average. Gender differences in working patterns are a significant driver of this wage gap. Women are more likely to interrupt their working time on the platform with consequences for their task completion speed. A follow-up survey shows that the gender differences in working patterns and hourly wages are concentrated amongst workers with children.
   bibtex: |
      @article{10.1162/rest_a_01282,
          author = {Adams-Prassl, Abi and Hara, Kotaro and Milland, Kristy and Callison-Burch, Chris},
          title = {The Gender Wage Gap in an Online Labor Market: The Cost of Interruptions},
          journal = {The Review of Economics and Statistics},
          pages = {1-23},
          year = {2023},
          month = {02},
          abstract = {This paper analyses gender differences in working patterns and wages on Amazon Mechanical Turk, a popular online labour platform. Using information on 2 million tasks, we find no gender differences in task selection nor experience. Nonetheless, women earn 20\\% less per hour on average. Gender differences in working patterns are a significant driver of this wage gap. Women are more likely to interrupt their working time on the platform with consequences for their task completion speed. A follow-up survey shows that the gender differences in working patterns and hourly wages are concentrated amongst workers with children.},
          issn = {0034-6535},
          doi = {10.1162/rest_a_01282},
          url = {https://doi.org/10.1162/rest\_a\_01282},
          eprint = {https://direct.mit.edu/rest/article-pdf/doi/10.1162/rest\_a\_01282/2070066/rest\_a\_01282.pdf}
      }
-
   title: CLIN&colon; A Continually Learning Language Agent for Rapid Task Adaptation and Generalization
   authors: Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, Peter Clark
   venue: Agent Learning in Open-Endedness (ALOE)  Workshop
   type: workshop
   year: 2023
   url: https://arxiv.org/abs/2310.10134
   page_count: 20
   id: continuous-learning-agent-for-task-adaptation
   data: 
   code: 
   website: https://allenai.github.io/clin/
   abstract: | 
      Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory centered on causal abstractions (rather than general "helpful hints") that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points (13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points (7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.
   bibtex: |
      @inproceedings{majumder2023clin,
            title={{CLIN}: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization}, 
            author={Bodhisattwa Prasad Majumder and Bhavana Dalvi Mishra and Peter Jansen and Oyvind Tafjord and Niket Tandon and Li Zhang and Chris Callison-Burch and Peter Clark},
            booktitle={Proceedings of the Agent Learning in Open-Endedness (ALOE) Workshop, NeurIPS 2023},
            year={2023},
            address={New Orleans},
            date={December 15, 2023},
      }
-
   title: Grounded Intuition of GPT-Vision's Abilities with Scientific Images
   authors: Alyssa Hwang, Andrew Head, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2023
   url: https://arxiv.org/abs/2311.02069
   page_count: 29
   id: analysis-of-gpt-4-vision-on-scientific-images
   data: 
   code: 
   website:
   abstract: | 
      GPT-Vision has impressed us on a range of vision-language tasks, but it comes with the familiar new challenge: we have little idea of its capabilities and limitations. In our study, we formalize a process that many have instinctively been trying already to develop "grounded intuition" of this new model. Inspired by the recent movement away from benchmarking in favor of example-driven qualitative evaluation, we draw upon grounded theory and thematic analysis in social science and human-computer interaction to establish a rigorous framework for qualitative evaluation in natural language processing. We use our technique to examine alt text generation for scientific figures, finding that GPT-Vision is particularly sensitive to prompting, counterfactual text in images, and relative spatial relationships. Our method and analysis aim to help researchers ramp up their own grounded intuitions of new models while exposing how GPT-Vision can be applied to make information more accessible.
   bibtex: |
      @misc{hwang2023grounded,
            title={Grounded Intuition of GPT-Vision's Abilities with Scientific Images}, 
            author={Alyssa Hwang and Andrew Head and Chris Callison-Burch},
            year={2023},
            eprint={2311.02069},
            archivePrefix={arXiv},
            primaryClass={cs.CL}
      }
-
   title: Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck
   authors: Josh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan, Mark Yatskar, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2023
   url: https://arxiv.org/abs/2310.19660
   page_count: 25
   id: text-concept-bottlenecks-with-LLMs
   data: 
   code: 
   website:
   abstract: | 
      Deep neural networks excel in text classification tasks, yet their application in high-stakes domains is hindered by their lack of interpretability. To address this, we propose Text Bottleneck Models (TBMs), an intrinsically interpretable text classification framework that offers both global and local explanations. Rather than directly predicting the output label, TBMs predict categorical values for a sparse set of salient concepts and use a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM), without the need for human curation. On 12 diverse datasets, using GPT-4 for both concept generation and measurement, we show that TBMs can rival the performance of established black-box baselines such as GPT-4 fewshot and finetuned DeBERTa, while falling short against finetuned GPT-3.5. Overall, our findings suggest that TBMs are a promising new framework that enhances interpretability, with minimal performance tradeoffs, particularly for general-domain text.
   bibtex: |
      @misc{ludan2023interpretablebydesign,
            title={Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck}, 
            author={Josh Magnus Ludan and Qing Lyu and Yue Yang and Liam Dugan and Mark Yatskar and Chris Callison-Burch},
            year={2023},
            eprint={2310.19660},
            archivePrefix={arXiv},
            primaryClass={cs.CL}
      }
-
   title: Kani &#129408;&colon; A Lightweight and Highly Hackable Framework for Building Language Model Applications
   authors: Andrew Zhu, Liam Dugan, Alyssa Hwang, Chris Callison-Burch
   venue: 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS)
   type: workshop
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/kani-python-framework-for-building-llm-applications.pdf
   page_count: 13
   id: kani-python-framework-for-building-llm-applications
   data: 
   code: https://github.com/zhudotexe/kani
   website:
   abstract: | 
      Language model applications are becoming increasingly popular and complex, often including features like tool usage and retrieval augmentation. However, existing frameworks for such applications are often opinionated, deciding for developers how their prompts ought to be formatted and imposing limitations on customizability and reproducibility. To solve this we present Kani: a lightweight, flexible, and model-agnostic open-source framework for building language model applications. Kani helps developers implement a variety of complex features by supporting the core building blocks of chat interaction: model interfacing, chat management, and robust function calling. All Kani core functions are easily overridable and well documented to empower developers to customize functionality for their own needs. Kani thus serves as a useful tool for researchers, hobbyists, and industry professionals alike to accelerate their development while retaining interoperability and fine-grained control.
   bibtex: |
      @inproceedings{zhu2023kani,
            title={Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications}, 
            author={Andrew Zhu and Liam Dugan and Alyssa Hwang and Chris Callison-Burch},
            year={2023},
            booktitle={3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS)},
            address={Singapore}
            year = {2023}
      }
-
   title: Learning Interpretable Style Embeddings via Prompting LLMs
   authors: Ajay Patel, Delip Rao, Ansh Kothary, Kathleen McKeown, Chris Callison-Burch
   venue: EMNLP Findings
   type: conference
   year: 2023
   url:  https://www.cis.upenn.edu/~ccb/publications/interpretable-style-embeddings.pdf
   page_count: 21
   id: interpretable-style-embeddings
   data: https://ajayp.app/posts/2023/11/learning-interpretable-embeddings-via-llms/
   code: https://ajayp.app/posts/2023/11/learning-interpretable-embeddings-via-llms/
   website:
   abstract: | 
      Style representation learning builds content independent representations of author style in text. To date, no large dataset of texts with stylometric annotations on a wide range of style dimensions has been compiled, perhaps because the linguistic expertise to perform such annotation would be prohibitively expensive. Therefore, current style representation approaches make use of unsupervised neural methods to disentangle style from content to create style vectors. These approaches, however, result in uninterpretable representations, complicating their usage in downstream applications like authorship attribution where auditing and explainability is critical. In this work, we use prompting to perform stylometry on a large number of texts to generate a synthetic stylometry dataset. We use this synthetic data to then train human interpretable style representations we call LISA embeddings. We release our synthetic dataset (STYLEGENOME) and our interpretable style embedding model (LISA) as resources.
   bibtex: |
      @inproceedings{patel2023learning,
            title={Learning Interpretable Style Embeddings via Prompting LLMs}, 
            author={Ajay Patel and Delip Rao and Ansh Kothary and Kathleen McKeown and Chris Callison-Burch},
            booktitle={Findings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023 Findings)},
            address={Singapore}
            year = {2023}
      }
-
   title: PAXQA&colon; Generating Cross-lingual Question Answering Examples at Training Scale
   authors: Bryan Li, Chris Callison-Burch
   venue: EMNLP Findings
   type: conference
   year: 2023
   url:  https://www.cis.upenn.edu/~ccb/publications/generating-cross-lingual-question-answering-examples.pdf
   page_count: 21
   id: generating-cross-lingual-question-answering-examples
   data: 
   code: 
   website:
   abstract: | 
      Existing question answering (QA) systems owe much of their success to large, high-quality training data. Such annotation efforts are costly, and the difficulty compounds in the cross-lingual setting. Therefore, prior cross-lingual QA work has focused on releasing evaluation datasets, and then applying zero-shot methods as baselines. In this work, we propose a synthetic data generation method for cross-lingual QA which leverages indirect supervision from existing parallel corpora. Our method termed paxqa (Projecting annotations for cross-lingual (x) QA) decomposes cross-lingual QA into two stages. In the first stage, we apply a question generation (QG) model to the English side. In the second stage, we apply annotation projection to translate both the questions and answers. To better translate questions, we propose a novel use of lexically-constrained machine translation, in which constrained entities are extracted from the parallel bitexts. We release cross-lingual QA datasets across 4 languages, totaling 662K QA examples. We then show that extractive QA models fine-tuned on these datasets outperform both zero-shot and prior synthetic data generation models, showing the sufficient quality of our generations. We find that the largest performance gains are for cross-lingual directions with non-English questions and English contexts. Ablation studies show that our dataset generation method is relatively robust to noise from automatic word alignments.
   bibtex: |
      @inproceedings{li2023crosslingualqa,
            title={{PAXQA} Generating Cross-lingual Question Answering Examples at Training Scale}, 
            author={Bryan Li and Chris Callison-Burch},
            booktitle={Findings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023 Findings)},
            address={Singapore}
            year = {2023}
      }
-
   title: Faithful Chain-of-Thought Reasoning
   authors: Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch
   venue: AACL-IJCNLP
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/faithful-chain-of-thought-reasoning.pdf
   page_count: 25
   id: faithful-chain-of-thought-reasoning
   data: https://github.com/veronica320/Faithful-COT
   code: https://github.com/veronica320/Faithful-COT
   award: Area Chair Award (Interpretability and Analysis of Models for NLP) 
   website:
   abstract: |
      While Chain-of-Thought (CoT) prompting boosts Language Models’ (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query → symbolic reasoning chain) and Problem Solving (reasoning chain → answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.
   bibtex: |
      @inproceedings{lyu2023faithful,
          author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
          title = {Faithful Chain-of-Thought Reasoning},
          booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023)},
          year = {2023},
          location = {Bali, Indonesia},
          date = {November 1--4}
      }
   figures:
      -
         img: figures/faithful-chain-of-thought-reasoning/faithful-chain-of-thought-reasoning-figure-3.png
         label: Figure 3
         caption: Examples from each task (Math Word Problems, Multi-hop QA, Planning, Logical Inference) showing our 2-stage Translation and Problem Solving pipeline.

-
   title: Rewriting the Script&colon; Adapting Text Instructions for Voice Interaction
   authors: Alyssa Hwang, Natasha Oza, Andrew Head, Chris Callison-Burch
   venue: DIS
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/adapting-text-instructions-for-voice-interaction.pdf
   page_count: 16
   id: adapting-text-instructions-for-voice-interaction
   data: 
   code: 
   website:
   abstract: | 
      Voice assistants have sharply risen in popularity in recent years, but their use has been limited mostly to simple applications like music, hands-free search, or control of internet-of-things devices. What would it take for voice assistants to guide people through more complex tasks? In our work, we study the limitations of the dominant approach voice assistants take to complex task guidance: reading aloud written instructions. Using recipes as an example, we observe twelve participants cook at home with a state-of-the-art voice assistant. We learn that the current approach leads to nine challenges, including obscuring the bigger picture, overwhelming users with too much information, and failing to communicate affordances. Instructions delivered by a voice assistant are especially difcult because they cannot be skimmed as easily as written instructions. Alexa in particular did not surface crucial details to the user or answer questions well. We draw on our observations to propose eight ways in which voice assistants can “rewrite the script”—summarizing, signposting, splitting, elaborating, volunteering, reordering, redistributing, and visualizing—to transform written sources into forms that are readily communicated through spoken conversation. We conclude with a vision of how modern advancements in natural language processing can be leveraged for intelligent agents to guide users efectively through complex tasks.
   bibtex: |
      @inproceedings{10.1145/3563657.3596059,
      author = {Hwang, Alyssa and Oza, Natasha and Callison-Burch, Chris and Head, Andrew},
      title = {Rewriting the Script: Adapting Text Instructions for Voice Interaction},
      year = {2023},
      isbn = {9781450398930},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3563657.3596059},
      doi = {10.1145/3563657.3596059},
      booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
      pages = {2233–2248},
      numpages = {16},
      keywords = {splitting, complex task guidance, remixing, reordering, instructions, voice assistants, voice user interfaces, summarization},
      location = {Pittsburgh, PA, USA},
      series = {DIS '23}
      }
-
   title: Learning When to Speak&colon; Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models
   authors: Liam Dugan, Anshul Wadhawan, Kyle Spence, Chris Callison-Burch, Morgan McGuire, Victor Zordan
   venue: Interspeech 
   type: conference
   year: 2023
   url: https://arxiv.org/pdf/2306.01201.pdf
   page_count: 2
   id: learning-when-to-speak
   data: 
   code: https://github.com/liamdugan/speech-to-speech
   website:
   abstract: | 
      Recent work in speech-to-speech translation (S2ST) has focused primarily on offline settings, where the full input utterance is available before any output is given. This, however, is not reasonable in many real-world scenarios. In latency-sensitive applications, rather than waiting for the full utterance, translations should be spoken as soon as the information in the input is present. In this work, we introduce a system for simultaneous S2ST targeting real-world use cases. Our system supports translation from 57 languages to English with tunable parameters for dynamically adjusting the latency of the output—including four policies for determining when to speak an output sequence. We show that these policies achieve offline-level accuracy with minimal increases in latency over a Greedy (wait-k) baseline. We open-source our evaluation code and interactive test script to aid future SimulS2ST research and application development.
   bibtex: |
      @inproceedings{dugan2023learning,
        title={Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models},
        author={Dugan, Liam and Wadhawan, Anshul and Spence, Kyle and Callison-Burch, Chris and McGuire, Morgan and Zordan, Victor},
        booktitle={Proceedings of INTERSPEECH 2023},
        year={2023},
        address={Dublin, Ireland},
        month={August}
        pages={5265-5266}
      }
-
   title: This Land is {Your, My} Land&colon; Evaluating Geopolitical Biases in Language Models
   authors: Bryan Li, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2023
   url: https://arxiv.org/abs/2305.14610
   page_count: 9
   id: geopolitical-biases-in-multilingual-llms
   data: 
   code: 
   website:
   abstract: | 
      We introduce the notion of geopolitical bias -- a tendency to report different geopolitical knowledge depending on the linguistic context. As a case study, we consider territorial disputes between countries. For example, for the widely contested Spratly Islands, would an LM be more likely to say they belong to China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To evaluate if such biases exist, we first collect a dataset of territorial disputes from Wikipedia, then associate each territory with a set of multilingual, multiple-choice questions. This dataset, termed BorderLines, consists of 250 territories with questions in 45 languages. We pose these question sets to language models, and analyze geopolitical bias in their responses through several proposed quantitative metrics. The metrics compare between responses in different question languages as well as to the actual geopolitical situation. The phenomenon of geopolitical bias is a uniquely cross-lingual evaluation, contrasting with prior work's monolingual (mostly English) focus on bias evaluation. Its existence shows that the knowledge of LMs, unlike multilingual humans, is inconsistent across languages.
   bibtex: |
      @misc{li2023land,
            title={This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models}, 
            author={Bryan Li and Chris Callison-Burch},
            year={2023},
            eprint={2305.14610},
            archivePrefix={arXiv},
            primaryClass={cs.CL}
      }
-
   title: Representation of Lexical Stylistic Features in Language Models’ Embedding Space
   authors: Qing Lyu, Marianna Apidianaki, Chris Callison-Burch
   venue: StarSEM
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/calypso-llms-for-dms.pdf
   page_count: 18
   id: representation-of-Lexical-stylistic-features
   data: 
   code: https://github.com/veronica320/Lexical-Stylistic-Features
   website:
   abstract: |
    The representation space of pretrained Language Models (LMs) encodes rich information about words and their relationships (e.g., similarity, hypernymy, polysemy) as well as abstract semantic notions (e.g., intensity). In this paper, we demonstrate that lexical stylistic notions such as complexity, formality, and figurativeness, can also be identified in this space. We show that it is possible to derive a vector representation for each of these stylistic notions from only a small number of seed pairs. Using these vectors, we can characterize new texts in terms of these dimensions by performing simple calculations in the corresponding embedding space. We conduct experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences. The lower performance of contextualized representations at the word level is partially attributable to the anisotropy of their vector space, which can be corrected to some extent using techniques like standardization.
   bibtex: |
      @inproceedings{lyu-etal-2023-representation,
          title = "Representation of Lexical Stylistic Features in Language Models{'} Embedding Space",
          author = "Lyu, Qing  and
            Apidianaki, Marianna  and
            Callison-Burch, Chris",
          booktitle = "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
          month = jul,
          year = "2023",
          address = "Toronto, Canada",
          publisher = "Association for Computational Linguistics",
          url = "https://aclanthology.org/2023.starsem-1.32",
          doi = "10.18653/v1/2023.starsem-1.32",
          pages = "370--387"
      }
-
   title: CALYPSO&colon; LLMs as Dungeon Masters' Assistants
   authors: Andrew Zhu, Lara J. Martin, Andrew Head, Chris Callison-Burch
   venue: AAID
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/calypso-llms-for-dms.pdf
   page_count: 11
   id: calypso-llms-for-dms
   data: 
   code: https://github.com/northern-lights-province/calypso-aiide-artifact
   website:
   press: 
    - 
      title: Hallucinating ChatGPT finds a role playing Dungeons & Dragons
      source: The Register
      by_line: Thomas Claburn
      date: August 19, 2023
      url: https://www.theregister.com/2023/08/19/chatgpt_dnd_dm/
   abstract: |
    The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to perform multiple tasks simultaneously. The DM must digest information about the game setting and monsters, synthesize scenes to present to other players, and respond to the players' interactions with the scene. Doing all of these tasks while maintaining consistency within the narrative and story world is no small feat of human cognition, making the task tiring and unapproachable to new players. Large language models (LLMs) like GPT-3 and ChatGPT have shown remarkable abilities to generate coherent natural language text. In this paper, we conduct a formative evaluation with DMs to establish the use cases of LLMs in D&D and tabletop gaming generally. We introduce calypso{}, a system of LLM-powered interfaces that support DMs with information and inspiration specific to their own scenario.  calypso{} distills game context into bite-sized prose and helps brainstorm ideas without distracting the DM from the game. When given access to calypso{}, DMs reported that it generated high-fidelity text suitable for direct presentation to players, and low-fidelity ideas that the DM could develop further while maintaining their creative agency. We see calypso{} as exemplifying a paradigm of AI-augmented tools that provide synchronous creative assistance within established game worlds, and tabletop gaming more broadly.
   bibtex: |
      @inproceedings{zhu2023calypso,
         title={{CALYPSO}: {LLMs} as Dungeon Masters' Assistants},
         author={Zhu, Andrew and Martin, Lara J. and Head, Andrew and Callison-Burch, Chris},
         booktitle={The 19th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE 2023)},
         year={2023}
      }
-
   title: I Cast Detect Thoughts&colon; Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons
   authors: Pei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang Ren, Chris Callison-Burch, Yejin Choi, Prithviraj Ammanabrolu
   venue: ACL
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/dnd-theory-of-mind.pdf
   page_count: 18
   id: dnd-theory-of-mind
   data: 
   code: 
   abstract: |
      We propose a novel task, G4C (Goal-driven Guidance Generation in Grounded Communication), for studying goal-driven and grounded natural language interactions. Specifically, we choose Dungeons and Dragons (D&D) -- a role-playing game consisting of multiple player characters and a Dungeon Master (DM) who collaborate to achieve a set of goals that are beneficial to the players -- as a testbed for this task. Here, each of the player characters is a student, with their own personas and abilities, and the DM is the teacher, an arbitrator of the rules of the world and responsible for assisting and guiding the students towards a global goal. We propose a theory-of-mind-inspired methodology for training such a DM with reinforcement learning (RL), where a DM: (1) learns to predict how the players will react to its utterances using a dataset of D&D dialogue transcripts; and (2) uses this prediction as a reward function providing feedback on how effective these utterances are at guiding the players towards a goal. Human and automated evaluations show that a DM trained with RL to generate guidance by incorporating a theory-of-mind of the players significantly improves the players' ability to achieve goals grounded in their shared world.
   bibtex: |
      @inproceedings{Pei-et-al-2023-dnd-theory-of-mind,
        author = {Zhou, Pei and Zhu, Andrew and Hu, Jennifer and Pujara, Jay and Ren, Xiang and Callison-Burch, Chris and Choi, Yejin and Ammanabrolu, Prithviraj},
        keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
        title = {An AI Dungeon Master's Guide: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons},
        booktitle={Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)},
        address={Toronto, Canada}
        year = {2023},  
        copyright = {Creative Commons Attribution 4.0 International}
      }
   figures:
      -
         img: figures/dnd-theory-of-mind/dnd-theory-of-mind-figure-1.png
         label: Figure 1
         caption: A motivating example. The (human) Dungeon Master (DM), knowing the desired story path, intends the players to perform actions to find out about the goblins—the first plot point in a storyline that will eventually lead the players to treasure. They generate the guidance “You notice some movements in the bushes” using theory-of-mind by inferring that the players will perform the desired actions upon hearing their words.
-
   title: FIREBALL&colon; A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information
   authors: Andrew Zhu, Karmanya Aggarwal, Alexander Feng, Lara J. Martin, and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/fireball-dataset.pdf
   page_count: 21
   id: fireball-dataset
   data: https://datasets.mechanus.zhu.codes/fireball-anonymized-nov-28-2022-kfdjl.tar.gz
   code: 
   abstract: |
      Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work  has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D&D gameplay on Discord with true game state info.  We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information.  We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate executable Avrae commands, particularly after finetuning. 
   bibtex: |
      @inproceedings{zhu-et-al-2023-fireball-dataset,
        author = {Zhu, Andrew and Aggarwal, Karmanya and Feng, Alexander and Martin, Lara and Callison-Burch, Chris and Choi, Yejin and Ammanabrolu, Prithviraj},
        title = {FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information},
        booktitle={Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)},
        address={Toronto, Canada}
        year = {2023},  
        copyright = {Creative Commons Attribution 4.0 International}
      }
-
   title: Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification
   authors: Zoey Sha Li, Ruining Zhao, Manling Li, Heng Ji, Chris Callison-Burch, Jiawei Han
   venue: ACL
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/creating-event-schema-with-llms.pdf
   page_count: 17
   id: creating-event-schema-with-llms
   data: 
   code: 
   abstract: |
      Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method INCPROMPT to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, INCPROMPT can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ∼10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.
   bibtex: |
      @inproceedings{sha-et-al-2023-creating-event-schema-with-llms,
        author = {Li, Sha and  Zhao, Ruining and Li, Manling and Ji, Heng and Callison-Burch, Chris and Han, Jiawei},
        title = {Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification},
        booktitle={Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)},
        address={Toronto, Canada}
        year = {2023},  
        copyright = {Creative Commons Attribution 4.0 International}
      }
-
   title: Explanation-based Finetuning Makes Models More Robust to Spurious Cues
   authors: Josh Magnus Ludan, Yixuan Meng, Tai Nguyen, Saurabh Shah, Qing Lyu, Marianna Apidianaki and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/explanation-based-finetuning.pdf
   page_count: 17
   id: explanation-based-finetuning
   data: 
   code: 
   abstract: |
      Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs’ reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implying its applicability to more datasets without human-written explanations.
   bibtex: |
      @inproceedings{ludan-et-al-2023-explanation-based-finetuning,
        author = {Ludan, Josh Magnus and Meng, Yixuan and Nguyen, Tai and Shah, Saurabh and Lyu, Qing and Apidianaki, Marianna and Callison-Burch, Chris},
        title = {Explanation-based Finetuning Makes Models More Robust to Spurious Cues},
        booktitle={Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)},
        address={Toronto, Canada}
        year = {2023},  
        copyright = {Creative Commons Attribution 4.0 International}
      }
-
   title: Human-in-the-Loop Schema Induction
   authors: Tianyi Zhang, Isaac Tham, Zhaoyi Hou, Jiaxuan Ren, Liyang Zhou, Hainiu Xu, Li Zhang, Lara J. Martin, Rotem Dror, Sha Li, Heng Ji, Martha Palmer, Susan Brown, Reece Suchocki, Chris Callison-Burch
   venue: ACL
   type: demo
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/human-in-the-loop-schema-induction.pdf
   page_count: 10
   id: human-in-the-loop-schema-induction
   data:
   code:
   website: https://www.youtube.com/watch?v=myru-fozVWI
   abstract: |
      Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3. We first describe the different modules of our system, including prompting to generate schematic elements, manual edit of those elements, and conversion of those into a schema graph. By qualitatively comparing our system to previous ones, we show that our system not only transfers to new domains more easily than previous approaches, but also reduces efforts of human curation thanks to our interactive interface.
   bibtex: |
      @inproceedings{human-in-the-loop-schema-induction,
        author = {Zhang, Tianyi and Tham, Isaac and Hou, Zhaoyi and Ren, Jiaxuan and Zhou, Liyang and Xu, Hainiu and Zhang, Li and Martin, Lara J. and Dror, Rotem and Li, Sha and Ji, Heng and Palmer, Martha and Brown, Susan and Suchocki, Reece and Callison-Burch, Chris},
        keywords = {Human-Computer Interaction (cs.HC), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
        title = {Human-in-the-Loop Schema Induction},
        booktitle={Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL 2023 demos)},
        address={Toronto, Canada},
        year = {2023}
      }
   figures:
      -
         img: figures/human-in-the-loop-schema-induction/human-in-the-loop-schema-induction-figure-4.png
         label: Figure 4
         caption: A sample of a constructed graph after human- curation for scenario ’cyber attack’


-
   title: CORRPUS&colon; Code-based Structured Prompting for Neurosymbolic Story Understanding
   authors: Yijiang River Dong, Lara J. Martin, Chris Callison-Burch
   venue: ACL Findings
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/detecting-story-inconsistencies.pdf
   page_count: 15
   id: detecting-story-inconsistencies
   data: 
   code: 
   abstract: |
      Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI task 2 and Re^3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some guidance for performing reasoning tasks properly.
   bibtex: |
      @inproceedings{Dong-et-al-2023-detecting-story-inconsistencies,
        doi = {10.48550/ARXIV.2212.10754},
        url = {https://arxiv.org/abs/2212.10754},
        author = {Dong, Yijiang River and Martin, Lara J. and Callison-Burch, Chris},
        keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
        title = {{CORRPUS}: Code-based Structured Prompting for Neurosymbolic Story Understanding},
        booktitle={Findings of the The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)},
        address={Toronto, Canada}
        year = {2023},  
        copyright = {Creative Commons Attribution 4.0 International}
      }
   figures:
      -
         img: figures/detecting-story-inconsistencies/detecting-story-inconsistencies-figure-1.png
         label: Figure 1
         caption: An example prompt used for bAbI Task 2. All three prompting methods have the same prompt initializa- tion (a) followed by their respective additional functions (b, c, or d), found inside the World class (end of a). That is, for the 1-shot example, CoRRPUS would be provided (a) + (b, c, or d) depending on the prompting method. To prompt for the next story, CoRRPUS is given (a) + the non-highlighted of (b, c, or d). The highlighted section would then be generated by CoRRPUS.

-
   title: Language in a Bottle&colon; Language Model Guided Concept Bottlenecks for Interpretable Image Classification
   authors: Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, Mark Yatskar
   venue: CVPR
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/language-in-a-bottle.pdf
   page_count: 18
   id: language-in-a-bottle
   data: 
   code: 
   abstract: |
      Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are first to show how to construct high-performance CBMs without manual specification of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to define a large space of possible bottlenecks. Given a problem domain, LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. Ultimately, GPT-3's sentential concepts can be aligned to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for concepts important to visual recognition. In the evaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot classification: they are 11.7% more accurate than black box linear probes at 1 shot and comparable with more data. Overall, LaBo demonstrates that inherently interpretable models can be widely applied at similar, or better, performance than black box approaches.
   bibtex: |
      @inproceedings{yang-etal-2023-language-in-a-bottle,
        title = {Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification},
        author = {Yang, Yue and Panagopoulou, Artemis and Zhou, Shenghao and Jin, Daniel and Callison-Burch, Chris and Yatskar, Mark},
        booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023)},
        year = {2023},
        address = {Vancouver, Canada},
        publisher = "IEEE/CVF",
        url = {https://www.cis.upenn.edu/~ccb/publications/language-in-a-bottle.pdf}
      }
   figures:
      -
         img: figures/language-in-a-bottle/language-in-a-bottle-figure-1.png
         label: Figure 1
         caption: Our proposed high-performance Concept Bottleneck Model alleviates the need for human designed concepts by prompting large language models (LLMs) such as GPT-3.
-
   title: Real or Fake Text?&colon; Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text
   authors: Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, Chris Callison-Burch
   venue: AAAI
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/real-or-fake-text-analysis.pdf
   page_count: 13
   data: https://github.com/liamdugan/human-detection/tree/main/data
   code: https://github.com/liamdugan/human-detection
   website: https://roft.io
   video: https://www.youtube.com/watch?v=gsZOBadj5t4
   press: 
    - 
      title: Bot or not? How to tell when you’re reading something written by AI
      source: CNN
      by_line: Clare Duffy and Kenneth Uzquiano
      date: July 11, 2023
      url: https://www.cnn.com/interactive/2023/07/business/detect-ai-text-human-writing/
    - 
      title: NewsChannel 12 Investigates&colon; Artificial Intelligence Part 3 
      source: Channel 12 News - ABC affiliate in North Carolina
      by_line: Tyler Hardin
      date: May 19, 2023
      url: https://wcti12.com/news/newschannel-12-investigates/newschannel-12-investigates-artificial-intelligence-part-3
    - 
      title: Real or Fake Text? We can learn to spot the difference
      source: Penn Today
      date: March 10, 2023
      url: https://penntoday.upenn.edu/news/penn-seas-real-or-fake-text-we-can-learn-spot-difference
    - 
      title: A Bot Isn’t Going to Take Your Place, But AI Will Make Your Job Harder
      source: Corporate Compliance Insights 
      date: March 8, 2023
      url: https://www.corporatecomplianceinsights.com/ai-compliance-jobs/
    - 
      title: How can humans detect AI writing? These Penn researchers have some tips
      source: Technical.ly
      date: March 6, 2023
      url: https://technical.ly/software-development/how-humans-detect-ai-writing-penn-research/
   id: real-or-fake-text-analysis
   abstract: |
      As text generated by large language models proliferates, it becomes vital to understand how humans engage with such text, and whether or not they are able to detect when the text they are reading did not originate with a human writer. Prior work on human detection of generated text focuses on the case where an entire passage is either human-written or machine-generated. In this paper, we study a more realistic setting where text begins as human-written and transitions to being generated by state-of-the-art neural language models. We show that, while annotators often struggle at this task, there is substantial variance in annotator skill and that given proper incentives, annotators can improve at this task over time. Furthermore, we conduct a detailed comparison study and analyze how a variety of variables (model size, decoding strategy, fine-tuning, prompt genre, etc.) affect human detection performance. Finally, we collect error annotations from our participants and use them to show that certain textual genres influence models to make different types of errors and that certain sentence-level features correlate highly with annotator selection. We release the RoFT dataset: a collection of over 21,000 human annotations paired with error classifications to encourage future work in human detection and evaluation of generated text.
   bibtex: |
      @inproceedings{dugan-ippolito-et-al-2023,
        title={Real or Fake Text? Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text},
        author={Liam Dugan and Daphne Ippolito and Arun Kirubarajan and Sherry Shi and Chris Callison-Burch},
        booktitle={The 37th AAAI Conference on Artificial Intelligence (AAAI 2023)},
        address={Washington DC, USA}
        year={2023}
      }
   figures:
      -
         img: figures/real-or-fake-text-analysis/real-or-fake-text-analysis-figure-1.png
         label: Figure 1
         caption: In the boundary detection task, players see one sentence at a time from a passage and try to guess when the passage transitions from human-written to machine-generated.
-
   title: Exploring the Curious Case of Code Prompts
   authors: Li Zhang, Liam Dugan, Hainiu Xu, Chris Callison-Burch
   venue: Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)
   type: workshop
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/the-curious-case-of-code-prompts.pdf
   page_count: 9
   id: the-curious-case-of-code-prompts
   abstract: |
      Recent work has shown that prompting language models with code-like representations of natural language leads to performance improvements on structured reasoning tasks. However, such tasks comprise only a small subset of all natural language tasks. In our work, we seek to answer whether or not code-prompting is the preferred way of interacting with language models in general. We compare code and text prompts across three popular GPT models (davinci, code-davinci-002, and text-davinci-002) on a broader selection of tasks (e.g., QA, sentiment, summarization) and find that with few exceptions, code prompts do not consistently outperform text prompts. Furthermore, we show that the style of code prompt has a large effect on performance for some (but not all) tasks and that fine-tuning on text instructions leads to better relative performance of code prompts.
   bibtex: |
      @inproceedings{zhang-etal-2023-exploring,
        title={Exploring the Curious Case of Code Prompts},
        author={Li Zhang and Liam Dugan and Hainiu Xu and Chris Callison-Burch},
        booktitle={Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)},
        address={Toronto, Canada},
        year={2023}
      }
-
   title: Automatically Generated Summaries of Video Lectures May Enhance Students’ Learning Experience
   authors: Hannah Gonzalez, Jiening Li, Helen Jin, Jiaxuan Ren, Hongyu Zhang, Ayotomiwa Akinyele, Adrian Wang, Eleni Miltsakaki, Ryan Baker, Chris Callison-Burch
   venue: Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)
   type: workshop
   year: 2023
   url: https://aclanthology.org/2023.bea-1.31
   page_count: 12
   id: automatically-generated-summaries-of-lecture-videos
   abstract: |
      We introduce a novel technique for automatically summarizing lecture videos using large language models such as GPT-3 and we present a user study investigating the effects on the studying experience when automatic summaries are added to lecture videos. We test students under different conditions and find that the students who are shown a summary next to a lecture video perform better on quizzes designed to test the course materials than the students who have access only to the video or the summary. Our findings suggest that adding automatic summaries to lecture videos enhances the learning experience. Qualitatively, students preferred summaries when studying under time constraints.
   bibtex: |
      @inproceedings{gonzalez-etal-2023-automatically,
        title={Automatically Generated Summaries of Video Lectures May Enhance Students’ Learning Experience},
        author={Hannah Gonzalez and Jiening Li and Helen Jin and Jiaxuan Ren and Hongyu Zhang and Ayotomiwa Akinyele and Adrian Wang and Eleni Miltsakaki and Ryan Baker and Chris Callison-Burch},
        booktitle={Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)},
        address={Toronto, Canada},
        year={2023}
      }
-
   title: Enhancing Human Summaries for Question-Answer Generation in Education
   authors: Hannah Gonzalez, Liam Dugan, Eleni Miltsakaki, Zhiqi Cui, Jiaxuan Ren, Bryan Li, Shriyash Upadhyay, Etan Ginsberg, Chris Callison-Burch
   venue: Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)
   type: workshop
   year: 2023
   url: https://aclanthology.org/2023.bea-1.9
   page_count: 11
   id: enhancing-human-summaries
   abstract: |
      We address the problem of generating high-quality question-answer pairs for educational materials. Previous work on this problem showed that using summaries as input improves the quality of question generation (QG) over original textbook text and that human-written summaries result in higher quality QG than automatic summaries. In this paper, a) we show that advances in Large Language Models (LLMs) are not yet sufficient to generate quality summaries for QG and b) we introduce a new methodology for enhancing bullet point student notes into fully fledged summaries and find that our methodology yields higher quality QG. We conducted a large-scale human annotation study of generated question-answer pairs for the evaluation of our methodology. In order to aid in future research, we release a new dataset of 9.2K human annotations of generated questions.
   bibtex: |
      @inproceedings{gonzalez-etal-2023-enhancing,
        title={Enhancing Human Summaries for Question-Answer Generation in Education},
        author={Hannah Gonzalez and Liam Dugan and Eleni Miltsakaki and Zhiqi Cui and Jiaxuan Ren and Bryan Li and Shriyash Upadhyay and Etan Ginsberg and Chris Callison-Burch},
        booktitle={Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)},
        address={Toronto, Canada},
        year={2023}
      }


-
   title: Improving Mathematics Tutoring With A Code Scratchpad
   authors: Shriyash Upadhyay, Etan Ginsberg, Chris Callison-Burch
   venue: Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)
   type: workshop
   year: 2023
   url: https://aclanthology.org/2023.bea-1.2
   page_count: 9
   id: imporving-mathematics-tutoring-with-a-code-scratchpad
   abstract: |
      Large language models can solve reasoning tasks (like math problems) more effectively when they are allowed to generate rationales. However, a good tutoring system should not just generate solutions, but should also generate explanations and should be able to correct and guide students. We show that providing a code scratchpad improves performance on each tutoring step with a gradeschool mathematics dataset. On these tutoring tasks, GPT-3 models provided with a code scratchpad significantly outperform those given only a language scratchpad (77.7% vs 48.7% cumulative accuracy).
   bibtex: |
      @inproceedings{upadhyay-etal-2023-improving,
        title={Improving Mathematics Tutoring With A Code Scratchpad},
        author={Shriyash Upadhyay and Etan Ginsberg and Chris Callison-Burch},
        booktitle={Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)},
        address={Toronto, Canada},
        year={2023}
      }

-
   title: Language Models are Drummers&colon; Drum Composition with Natural Language Pre-Training
   authors: Harry Li Zhang, Chris Callison-Burch
   venue: AAAI 2023 Workshop on Creative AI Across Modalities
   type: workshop
   year: 2023
   url: https://arxiv.org/abs/2301.01162
   page_count: 8
   data: https://github.com/zharry29/drums-with-llm
   code: https://github.com/zharry29/drums-with-llm
   id: language-models-are-drummers
   abstract: |
      Automatic music generation with artificial intelligence typically requires a large amount of data which is hard to obtain for many less common genres and musical instruments. To tackle this issue, we present ongoing work and preliminary findings on the possibility for deep models to transfer knowledge from language to music, by finetuning large language models pre-trained on a massive text corpus on only hundreds of MIDI files of drum performances. We show that by doing so, one of the largest, state-of-the-art models (GPT3) is capable of generating reasonable drum grooves, while models that are not pre-trained (Transformer) shows no such ability beyond naive repetition. Evaluating generated music is a challenging task, more so is evaluating drum grooves with little precedence in literature. Hence, we propose a tailored structural evaluation method and analyze drum grooves produced by GPT3 compared to those played by human professionals, exposing the strengths and weaknesses of such generation by language-to-music transfer. Our findings suggest that language-to-music transfer learning with large language models is viable and promising.
   bibtex: |
      @inproceedings{Zhang-2023-llms-are-drummers,
        title={Language Models are Drummers&colon; Drum Composition with Natural Language Pre-Training},
        author={Li Zhang and Chris Callison-Burch},
        booktitle={AAAI 2023 Workshop on Creative AI Across Modalities},
        address={Washington DC, USA}
        year={2023}
      }
   figures:
      -
         img: figures/language-models-are-drummers/language-models-are-drummers-figure-1.png
         label: Figure 1
         caption: The sheet music of an example of our generated drum track, demonstrating our model’s ability to somewhat musically follow the motif and make variations. The first two measures are provided while the rest are generated.
-
   title: Learn With Martian&colon; A Tool For Creating Assignments That Can Write And Re-Write Themselves
   authors: Shriyash Upadhyay, Etan Ginsberg, Chris Callison-Burch
   venue: EACL Demos
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/learn-with-martian.pdf
   page_count: 10
   id: learn-with-martian
   data: 
   code: 
   website: https://learn.withmartian.com
   abstract: |
      n this paper, we propose Learn, a unified, easy-to-use tool to apply question generation and selection in classrooms. The tool lets instructors and TAs create assignments that can write and re-write themselves. Given existing course materials, for example a reference textbook, Learn can generate questions, select the highest quality questions, show the questions to students, adapt question difficulty to student knowledge, and generate new questions based on how effectively old questions help students learn. The modular, composable nature of the tools for handling each sub-task allow instructors to use only the parts of the tool necessary to the course, allowing for integration in a large number of courses with varied teaching styles. We also report on the adoption of the tool in classes at the University of Pennsylvania with over 1000 students. Learn is publicly released at https://learn.withmartian.com.
   bibtex: |
      @inproceedings{upadhyay-etal-2023-learn,
          title = "Learn With Martian: A Tool For Creating Assignments That Can Write And Re-Write Themselves",
          author = "Upadhyay, Shriyash  and
            Callison-Burch, Chris  and
            Ginsberg, Etan",
          booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
          month = may,
          year = "2023",
          address = "Dubrovnik, Croatia",
          publisher = "Association for Computational Linguistics",
          url = "https://aclanthology.org/2023.eacl-demo.30",
          pages = "267--276",
          abstract = "In this paper, we propose Learn, a unified, easy-to-use tool to apply question generation and selection in classrooms. The tool lets instructors and TAs create assignments that can write and re-write themselves. Given existing course materials, for example a reference textbook, Learn can generate questions, select the highest quality questions, show the questions to students, adapt question difficulty to student knowledge, and generate new questions based on how effectively old questions help students learn. The modular, composable nature of the tools for handling each sub-task allow instructors to use only the parts of the tool necessary to the course, allowing for integration in a large number of courses with varied teaching styles. We also report on the adoption of the tool in classes at the University of Pennsylvania with over 1000 students. Learn is publicly released at https://learn.withmartian.com.",
      }
-
   title: Causal Reasoning About Entities and Events in Procedural Texts
   authors: Li Zhang, Hainiu Xu, Yue Yang, Shuyan Zhou, Weiqiu You, Manni Arora, Chris Callison-Burch
   venue: Findings of EACL
   type: conference
   year: 2023
   url: https://www.cis.upenn.edu/~ccb/publications/casual-reasoning-about-entities-and-events-in-procedural-texts.pdf
   page_count: 14
   id: casual-reasoning-about-entities-and-events-in-procedural-texts
   data: https://github.com/zharry29/causal_reasoning_of_entities_and_events
   code: https://github.com/zharry29/causal_reasoning_of_entities_and_events
   website: 
   abstract: |
      Entities and events have long been regarded as the crux of machine reasoning. Procedural texts have received increasing attention due to the dynamic nature of involved entities and events. Existing work has focused either on entity state tracking (e.g., the temperature of a pan) or on counterfactual event reasoning (e.g., how likely am I to burn myself by touching the pan), while these two tasks are tightly intertwined. In this work, we propose CREPE, the first benchmark on causal reasoning about event plausibility based on entity states. We experiment with strong large language models and show that most models, including GPT3, perform close to chance at .30 F1, lagging far behind the human performance of .87 F1. Inspired by the finding that structured representations such as programming languages benefit event reasoning as a prompt to code language models such as Codex, we creatively inject the causal relations between entities and events through intermediate variables and boost the performance to .67 to .72 F1. Our proposed event representation not only allows for knowledge injection but also marks the first successful attempt of chain-of-thought reasoning with code language models.
   bibtex: |
      @inproceedings{zhang-etal-2023-causal,
        title = "Causal Reasoning About Entities and Events in Procedural Texts",
        author = "Li Zhang and Hainiu Xu and Yue Yang and Shuyan Zhou and Weiqiu You and Manni Arora and Chris Callison-Burch"
        booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
        year = "2023",
        address = "Dubrovnik, Croatia",
        publisher = "Association for Computational Linguistics",
        url = "https://arxiv.org/pdf/2301.10896.pdf"
        abstract = "Entities and events have long been regarded as the crux of machine reasoning. Procedural texts have received increasing attention due to the dynamic nature of involved entities and events. Existing work has focused either on entity state tracking (e.g., the temperature of a pan) or on counterfactual event reasoning (e.g., how likely am I to burn myself by touching the pan), while these two tasks are tightly intertwined. In this work, we propose CREPE, the first benchmark on causal reasoning about event plausibility based on entity states. We experiment with strong large language models and show that most models, including GPT3, perform close to chance at .30 F1, lagging far behind the human performance of .87 F1. Inspired by the finding that structured representations such as programming languages benefit event reasoning as a prompt to code language models such as Codex, we creatively inject the causal relations between entities and events through intermediate variables and boost the performance to .67 to .72 F1. Our proposed event representation not only allows for knowledge injection but also marks the first successful attempt of chain-of-thought reasoning with code language models."
      }
   figures:
      -
         img: figures/casual-reasoning-about-entities-and-events-in-procedural-texts/casual-reasoning-about-entities-and-events-in-procedural-texts-figure-5.png
         label: Figure 5
         caption: Our Codex prompt with a soft representation of entity state changes as strings.
-
   title: Bidirectional Language Models Are Also Few-shot Learners
   authors: Ajay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, Chris Callison-Burch
   venue: ICLR
   type: conference
   year: 2023
   url: https://arxiv.org/abs/2209.14500
   page_count: 25
   id: bidirectional-lms-are-few-shot-leaners
   data:
   code:
   website: https://www.cis.upenn.edu/~ccb/publications/bidirectional-lms-are-few-shot-leaners.pdf
   abstract: |
      Large language models such as GPT-3 (Brown et al., 2020) can perform arbitrary tasks without undergoing fine-tuning after being prompted with only a few labeled examples. An arbitrary task can be reformulated as a natural language prompt, and a language model can be asked to generate the completion, indirectly performing the task in a paradigm known as prompt-based learning. To date, emergent prompt-based learning capabilities have mainly been demonstrated for unidirectional language models. However, bidirectional language models pre-trained on denoising objectives such as masked language modeling produce stronger learned representations for transfer learning. This motivates the possibility of prompting bidirectional models, but their pre-training objectives have made them largely incompatible with the existing prompting paradigm. We present SAP (Sequential Autoregressive Prompting), a technique that enables the prompting of bidirectional models. Utilizing the machine translation task as a case study, we prompt the bidirectional mT5 model (Xue et al., 2021) with SAP and demonstrate its few-shot and zero-shot translations outperform the few-shot translations of unidirectional models like GPT-3 and XGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We further show SAP is effective on question answering and summarization. For the first time, our results demonstrate prompt-based learning is an emergent property of a broader class of language models, rather than only unidirectional models.
   bibtex: |
      @inproceedings{Patel-ICLR-2023,
        url = {https://arxiv.org/abs/2209.14500},
        author = {Patel, Ajay and Li, Bryan and Rasooli, Mohammad Sadegh and Constant, Noah and Raffel, Colin and Callison-Burch, Chris},
        keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
        title = {Bidirectional Language Models Are Also Few-shot Learners},
        booktitle={Eleventh International Conference on Learning Representations (ICLR 2023)},
        address={Kigali, Rwanda}
        year={2023}
      }
   figures:
      -
         img: figures/bidirectional-lms-are-few-shot-leaners/bidirectional-lms-are-few-shot-leaners-figure-1.png
         label: Figure 1
         caption: A visualization of our SAP technique extracting high-quality translations from mT5. In the zero-shot setting, the examples used in the prompt are synthetic examples retrieved in a fully unsupervised manner.

-
   title: Low-Resource Authorship Style Transfer with In-Context Learning
   authors: Ajay Patel, Nicholas Andrews, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2022
   url: https://arxiv.org/abs/2212.08986
   page_count: 19
   id: low-resource-authorship-style-transfer
   data: 
   code: 
   abstract: |
      Authorship style transfer involves altering the style of text to match the style of some target author whilst preserving the semantic meaning of the original text. Existing approaches to unsupervised authorship style transfer like STRAP have largely focused on style transfer for target authors with many examples of their writing style through books, speeches, or other published works (Krishna et al., 2020). Due to this high-resource training data requirement (often greater than 100,000 words), these approaches are often only useful for style transfer to the style of published authors, politicians, or other well-known figures and authorship styles. In this paper, we attempt to perform low-resource authorship style transfer, a more challenging class of authorship style transfer where only a limited amount of text in the target author’s style may exist. In our experiments, we specifically choose source and target authors from Reddit to perform style transfer over their Reddit posts, limiting ourselves to just 16 posts (on average ≈ 500 words) of the target author’s style. We then propose a method for automatic evaluation on the lowresource authorship style transfer task utilizing authorship and style representation embeddings (Rivera-Soto et al., 2021; Wegmann et al., 2022). We evaluate our style transferred outputs with the proposed automatic evaluation method and find that our method, STYLL, is able to outperform STRAP and a comprehensive set of baselines.
   bibtex: |
      @misc{https://doi.org/10.48550/arxiv.2212.08986,
        doi = {10.48550/ARXIV.2212.08986},
        url = {https://arxiv.org/abs/2212.08986},
        author = {Patel, Ajay and Andrews, Nicholas and Callison-Burch, Chris},
        keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
        title = {Low-Resource Authorship Style Transfer with In-Context Learning},
        publisher = {arXiv},
        year = {2022},
        copyright = {arXiv.org perpetual, non-exclusive license}
      }
   figures:
      -
         img: figures/low-resource-authorship-style-transfer/low-resource-authorship-style-transfer-figure-1.png
         label: Figure 1
         caption: An actual output of STYLL on the unsupervised low-resource authorship style transfer task between two Reddit users using just 16 Reddit posts as examples of the target style.
-
   title: A Deep Learning Method to Detect Opioid Prescription and Opioid Use Disorder from Electronic Health Records
   authors: Aditya Kashyap, Chris Callison-Burch, Mary Regina Boland
   venue: International Journal of Medical Informatics
   type: journal
   year: 2022
   url: https://doi.org/10.1016/j.ijmedinf.2022.104979
   page_count: 44
   id: deep-learning-to-detect-opioid-use-disorder
   abstract: |
      Objective
      As the opioid epidemic continues across the United States, methods are needed to accurately and quickly identify patients at risk for opioid use disorder (OUD). The purpose of this study is to develop two predictive algorithms: one to predict opioid prescription and one to predict OUD.
      Materials and Methods
      We developed an informatics algorithm that trains two deep learning models over patient EHRs using the MIMIC-III database. We utilize both the structured and unstructured parts of the EHR and show that it is possible to predict both challenging outcomes.
      Results
      Our deep learning models incorporate elements from the EHRs to predict opioid prescription with an F1-score of 0.88 ± 0.003 and an AUC-ROC of 0.93 ± 0.002. We also constructed a model to predict OUD diagnosis achieving an F1-score of 0.82 ± 0.05 and AUC-ROC of 0.94 ± 0.008.
      Discussion
      Our model for OUD prediction outperformed prior algorithms for specificity, F1 score and AUC-ROC while achieving equivalent sensitivity. This demonstrates the importance of a) deep learning approaches in predicting OUD and b) incorporating both structured and unstructured data for this prediction task. No prediction models for opioid prescription as an outcome were found in the literature and therefore our model is the first to predict opioid prescribing behavior.
      Conclusion
      Algorithms such as those described in this paper will become increasingly important to understand the drivers underlying this national epidemic.
   bibtex: |
      @article{KASHYAP2022104979,
         title = {A Deep Learning Method to Detect Opioid Prescription and Opioid Use Disorder from Electronic Health Records},
         journal = {International Journal of Medical Informatics},
         pages = {104979},
         year = {2022},
         issn = {1386-5056},
         doi = {https://doi.org/10.1016/j.ijmedinf.2022.104979},
         url = {https://www.sciencedirect.com/science/article/pii/S1386505622002933},
         author = {Aditya Kashyap and Chris Callison-Burch and Mary {Regina Boland}},
         keywords = {opioid, machine learning, electronic health records, data mining, natural language processing},
         abstract = {Objective
         As the opioid epidemic continues across the United States, methods are needed to accurately and quickly identify patients at risk for opioid use disorder (OUD). The purpose of this study is to develop two predictive algorithms: one to predict opioid prescription and one to predict OUD.
         Materials and Methods
         We developed an informatics algorithm that trains two deep learning models over patient EHRs using the MIMIC-III database. We utilize both the structured and unstructured parts of the EHR and show that it is possible to predict both challenging outcomes.
         Results
         Our deep learning models incorporate elements from the EHRs to predict opioid prescription with an F1-score of 0.88 ± 0.003 and an AUC-ROC of 0.93 ± 0.002. We also constructed a model to predict OUD diagnosis achieving an F1-score of 0.82 ± 0.05 and AUC-ROC of 0.94 ± 0.008.
         Discussion
         Our model for OUD prediction outperformed prior algorithms for specificity, F1 score and AUC-ROC while achieving equivalent sensitivity. This demonstrates the importance of a) deep learning approaches in predicting OUD and b) incorporating both structured and unstructured data for this prediction task. No prediction models for opioid prescription as an outcome were found in the literature and therefore our model is the first to predict opioid prescribing behavior.
         Conclusion
         Algorithms such as those described in this paper will become increasingly important to understand the drivers underlying this national epidemic.}
      }
   figures:
      -
         img: figures/deep-learning-to-detect-opioid-use-disorder/deep-learning-to-detect-opioid-use-disorder.jpg
         label: Figure 1
         caption: The architecture for a Hierarchical Attention Model using Transformers to process Clinical Notes.
-
   title: Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence
   authors: Chris Callison-Burch, Gaurav Singh Tomar, Lara Martin, Daphne Ippolito, Suma Bailis and David Reitter
   venue: EMNLP
   type: conference
   year: 2022
   url: https://www.cis.upenn.edu/~ccb/publications/dungeons-and-dragons-as-a-dialog-challenge-for-artificial-intelligence.pdf
   page_count: 15
   id: dungeons-and-dragons-as-a-dialog-challenge-for-artificial-intelligence
   data: 
   code: 
   abstract: |
      AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D&D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict the state of the game given the dialogue history. We create a gameplay dataset consisting of nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns, 500,000 dice rolls, and 58 million words. We automatically annotate the data with partial state information about the game play. We train a large language model (LM) to generate the next game turn, conditioning it on different information. The LM can respond as a particular character or as the player who runs the game—i.e., the Dungeon Master (DM). It is trained to produce dialogue that is either in-character (roleplaying in the fictional world) or out-of-character (discussing rules or strategy). We perform a human evaluation to determine what factors make the generated output plausible and interesting. We further perform an automatic evaluation to determine how well the model can predict the game state given the history and examine how well tracking the game state improves its ability to produce plausible conversational output.
   bibtex: |
      @inproceedings{callison-burch-tomar-et-al-2022,
        title={Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence},
        author={Chris Callison-Burch and Gaurav Singh Tomar and Lara Martin and Daphne Ippolito and Suma Bailis and David Reitter},
        booktitle={The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)},
        address={Abu Dhabi, UAE}
        year={2022}
      }
   figures:
      -
         img: figures/dungeons-and-dragons-as-a-dialog-challenge-for-artificial-intelligence/dungeons-and-dragons-as-a-dialog-challenge-for-artificial-intelligence-figure-1.png
         label: Figure 1
         caption: Example of 3 turns in the D&D Beyond play-by-post forum.
-
   title: Unsupervised Entity Linking with Guided Summarization and Multiple-Choice Selection
   authors: Jeffrey Young-Min Cho, Harry Li Zhang, Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2022
   url: https://www.cis.upenn.edu/~ccb/publications/unsupervised-entity-linking-with-guided-summarization.pdf
   page_count: 9
   id: unsupervised-entity-linking-with-guided-summarization
   data: 
   code: 
   abstract: |
      Entity linking, the task of linking potentially ambiguous mentions in texts to corresponding knowledge-base entities, is an important component for language understanding. We address two challenge in entity linking: how to leverage wider contexts surrounding a mention, and how to deal with limited training data. We propose a fully unsupervised model called SumMC that first generates a guided summary of the contexts conditioning on the mention, and then casts the task to a multiple-choice problem where the model chooses an entity from a list of candidates. In addition to evaluating our model on existing datasets that focus on named entities, we create a new dataset that links noun phrases from WikiHow to Wikidata. We show that our SumMC model achieves state-of-the-art unsupervised performance on our new dataset and on exiting datasets. 
   bibtex: |
      @inproceedings{cho-2022,
        title={Unsupervised Entity Linking with Guided Summarization and Multiple-Choice Selection},
        author={Young-Min Cho and Li Zhang and Chris Callison-Burch},
        booktitle={The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)},
        address={Abu Dhabi, UAE}
        year={2022}
      }
   figures:
      -
         img: figures/unsupervised-entity-linking-with-guided-summarization/unsupervised-entity-linking-with-guided-summarization-figure-1.png
         label: Figure 1
         caption: Example of an Entity Linking problem.
-
   title: Visualizing the Obvious&colon; A Concreteness-based Ensemble Model for Noun Property Prediction
   authors: Yue Yang, Artemis Panagopoulou, Marianna Apidianaki, Mark Yatskar and Chris Callison-Burch
   venue: Findings of EMNLP
   type: conference
   year: 2022
   url: https://www.cis.upenn.edu/~ccb/publications/visualizing-the-obvious.pdf
   page_count: 17
   id: visualizing-the-obvious
   data: 
   code: 
   abstract: |
      Neural language models encode rich knowledge about entities and their relationships which can be extracted from their representations using probing. Common properties of nouns (e.g., red strawberries, small ant) are, however, more challenging to extract compared to other types of knowledge because they are rarely explicitly stated in texts. We hypothesize this to mainly be the case for perceptual properties which are obvious to the participants in the communication. We propose to extract these properties from images and use them in an ensemble model, in order to complement the information that is extracted from language models. We consider perceptual properties to be more concrete than abstract properties (e.g., interesting, flawless). We propose to use the adjectives’ concreteness score as a lever to calibrate the contribution of each source (text vs. images). We evaluate our ensemble model in a ranking task where the actual properties of a noun need to be ranked higher than other non-relevant properties. Our results show that the proposed combination of text and images greatly improves noun property prediction compared to powerful text-based language models.
   bibtex: |
      @inproceedings{yang-2022,
        title={Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun Property Prediction},
        author={Yue Yang and Artemis Panagopoulou and Marianna Apidianaki and Mark Yatskar and Chris Callison-Burch},
        booktitle={Findings of The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)},
        address={Abu Dhabi, UAE}
        year={2022}
      }
   figures:
      -
         img: figures/visualizing-the-obvious/visualizing-the-obvious-figure-1.png
         label: Figure 1
         caption: Our task is to retrieve relevant properties of nouns from a set of candidates. We tackle the task using (a) Cloze-task probing; (b) CLIP to compute the similarity between the properties and images of the noun; (c) a Concreteness Ensemble Model (CEM) to ensemble language and CLIP predictions which relies on properties’ concreteness ratings.
-
   title: Empathic Conversations&colon; A Multi-level Dataset of Contextualized Conversations
   authors: Damilola Omitaomu, Shabnam Tafreshi, Tingting Liu, Sven Buechel, Chris Callison-Burch, Johannes Eichstaedt, Lyle Ungar, João Sedoc
   venue: arXiv
   type: preprint
   year: 2022
   url: https://arxiv.org/abs/2205.12698
   page_count: 21
   id: empathic-conversations
   data: 
   code: 
   abstract: Empathy is a cognitive and emotional reaction to an observed situation of others. Empathy has recently attracted interest because it has numerous applications in psychology and AI, but it is unclear how different forms of empathy (e.g., self-report vs counterpart other-report, concern vs. distress) interact with other affective phenomena or demographics like gender and age. To better understand this, we created the {\it Empathic Conversations} dataset of annotated negative, empathy-eliciting dialogues in which pairs of participants converse about news articles. People differ in their perception of the empathy of others. These differences are associated with certain characteristics such as personality and demographics. Hence, we collected detailed characterization of the participants' traits, their self-reported empathetic response to news articles, their conversational partner other-report, and turn-by-turn third-party assessments of the level of self-disclosure, emotion, and empathy expressed. This dataset is the first to present empathy in multiple forms along with personal distress, emotion, personality characteristics, and person-level demographic information. We present baseline models for predicting some of these features from conversations.
   bibtex: |
      @misc{https://doi.org/10.48550/arxiv.2205.12698,
        doi = {10.48550/ARXIV.2205.12698},
        url = {https://arxiv.org/abs/2205.12698},
        author = {Omitaomu, Damilola and Tafreshi, Shabnam and Liu, Tingting and Buechel, Sven and Callison-Burch, Chris and Eichstaedt, Johannes and Ungar, Lyle and Sedoc, João},
        keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
        title = {Empathic Conversations: A Multi-level Dataset of Contextualized Conversations},
        publisher = {arXiv},
        year = {2022},
        copyright = {Creative Commons Attribution 4.0 International}
      }
-
   title: The Case for a Single Model that can Both Generate Continuations and Fill-in-the-Blank
   authors: Daphne Ippolito, Liam Dugan, Emily Reif, Ann Yuan, Andy Coenen, Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2022
   url: https://www.cis.upenn.edu/~ccb/publications/fill-in-the-blank-LMs.pdf
   page_count: 12
   id: fill-in-the-blank-LMs
   data: 
   code: 
   abstract: The task of inserting text into a specified position in a passage, known as fill in the blank (FitB), is useful for a variety of applications where writers interact with a natural language generation (NLG) system to craft text. While previous work has tackled this problem with models trained specifically to do fill in the blank, a more useful model is one that can effectively perform both FitB and continuation tasks. In this work, we evaluate the feasibility of using a single model to do both tasks. We show that models pre-trained with a FitB-style objective are capable of both tasks, while models pre-trained for continuation are not. Finally, we show how these models can be easily finetuned to allow for fine-grained control over the length and word choice of the generation.
   bibtex: |
      @inproceedings{Ippolito-2022-fill-in-the-blank,
        title={The Case for a Single Model that can Both Generate Continuations and Fill in the Blank},
        author={Daphne Ippolito and Liam Dugan and Emily Reif and Ann Yuan and Andy Coenen and Chris Callison-Burch},
        booktitle={Findings of the 2022 Annual Conference of the North American Chapter of the Association for Computational Linguistic (NAACL 2022)},
        address={Seattle, Washington}
        year={2022}
      }
-
   title: Is “My Favorite New Movie” My Favorite Movie? Probing the Understanding of Recursive Noun Phrases
   authors: Qing Lyu, Hua Zheng, Daoxin Li, Li Zhang, Marianna Apidianaki, Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2022
   url: https://www.cis.upenn.edu/~ccb/publications/recursive-noun-phrases.pdf
   page_count: 12
   id: recursive-noun-phrases
   data: https://github.com/veronica320/Recursive-NPs
   code: https://github.com/veronica320/Recursive-NPs
   abstract: Recursive noun phrases (NPs) have interesting semantic properties. For example, my favorite new movie is not necessarily my favorite movie, whereas my new favorite movie is. This is common sense to humans, yet it is unknown whether language models have such knowledge. We introduce the Recursive Noun Phrase Challenge (RNPC), a dataset of three textual inference tasks involving textual entailment and event plausibility comparison, precisely targeting the understanding of recursive NPs. When evaluated on RNPC, state-of-the-art Transformer models only perform around chance. Still, we show that such knowledge is learnable with appropriate data. We further probe the models for relevant linguistic features that can be learned from our tasks, including modifier semantic category and modifier scope. Finally, models trained on RNPC achieve strong zero-shot performance on an extrinsic Harm Detection evaluation task, showing the usefulness of the understanding of recursive NPs in downstream applications.
   bibtex: |
      @inproceedings{Lyu2022-NPs,
        title={Is “My Favorite New Movie” My Favorite Movie? Probing the Understanding of Recursive Noun Phrases},
        author={Qing Lyu and Hua Zheng and Daoxin Li and Li Zhang and Marianna Apidianaki and Chris Callison-Burch},
        booktitle={Proceedings of the 2022 Annual Conference of the North American Chapter of the Association for Computational Linguistic (NAACL 2022)},
        address={Seattle, Washington}
        year={2022}
      }
-
   title: A Recipe For Arbitrary Text Style Transfer with Large Language Models
   authors: Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, Jason Wei
   venue: ACL
   type: conference
   year: 2022
   website: https://bit.ly/3fLDuci
   video: https://aclanthology.org/2022.acl-short.94.mp4
   url: https://www.cis.upenn.edu/~ccb/publications/zero-shot-arbitrary-text-style-transfer.pdf
   page_count: 12
   id: zero-shot-arbitrary-text-style-transfer
   data: 
   abstract: In this paper, we leverage large language models (LMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as "make this melodramatic" or "insert a metaphor."
   bibtex: |
      @inproceedings{Reif2022-style-transfer,
        title={A Recipe For Arbitrary Text Style Transfer with Large Language Models},
        author={Emily Reif and Daphne Ippolito and Ann Yuan and Andy Coenen and Chris Callison-Burch and Jason Wei},
        booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022)},
        address={Dublin, Ireland}
        year={2022}
      }
-
   title: Deduplicating Training Data Makes Language Models Better
   authors: Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini 
   venue: ACL
   type: conference
   year: 2022
   url: https://www.cis.upenn.edu/~ccb/publications/deduplicating-training-data-makes-lms-better.pdf
   page_count: 22
   highly_cited: 172
   id: deduplicating-training-data
   code: https://github.com/google-research/deduplicate-text-datasets
   abstract: We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets
   bibtex: |
      @inproceedings{lee2022deduplicating,
        title={Deduplicating Training Data Makes Language Models Better},
        author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
        booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022)},
        address={Dublin, Ireland}
        year={2022}
      }
-
   title: Show Me More Details&colon; Discovering Hierarchies of Procedures from Semi-structured Web Data
   authors: Shuyan Zhou, Li Zhang, Yue Yang, Qing Lyu, Pengcheng Yin, Chris Callison-Burch, Graham Neubig
   venue: ACL
   type: conference
   year: 2022
   website: https://wikihow-hierarchy.github.io/
   url: https://www.cis.upenn.edu/~ccb/publications/wikihow-hierarchies.pdf
   page_count: 14
   id: wikihow-hierarchies
   data: https://github.com/shuyanzhou/wikihow_hierarchy
   code: https://github.com/shuyanzhou/wikihow_hierarchy
   abstract: Procedures are inherently hierarchical. To host a party, one may need to clean the house, which in turn may require putting away the clothes. While such hierarchical knowledge is critical for reasoning about complex procedures, most existing works treat procedures as shallow structures without modeling the hierarchical dependency between them. In this work, we attempt to construct an open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a website containing more than 110k instructional articles, each documenting the steps to accomplish a complex procedure. To this end, we develop a simple and efficient method that links  steps (e.g. clean the house) in an article to other articles with similar intents (e.g. how to deep clean your house), which proceeds recursively to form the KB. Our method significantly outperforms several strong baselines according to automatic evaluation, human judgment, and application to downstream tasks such as instructional video retrieval.
   bibtex: |
      @inproceedings{Zhou-Zhang-et-al-2022-wikihow-hierarchies,
        title={Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data},
        author={Shuyan Zhou and Li Zhang and Yue Yang and Qing Lyu and Pengcheng Yin and Chris Callison-Burch and Graham Neubig},
        booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022)},
        address={Dublin, Ireland}
        year={2022}
      }
-
   title: A Feasibility Study of Answer-Agnostic Question Generation for Education
   authors: Liam Dugan, Eleni Miltsakaki, Etan Ginsberg, Shriyash Upadhyay, Hannah Gonzalez, Dahyeon Choi, Chuning Yuan, Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2022
   website:
   url: https://www.cis.upenn.edu/~ccb/publications/smart-textbook-feasibility-study.pdf
   page_count: 8
   id: smart-textbook-feasibility-study
   data: 
   abstract: We conduct a feasibility study into the applicability of  answer-agnostic question generation models to textbook passages. We show that a significant portion of errors in such systems arise from asking irrelevant or uninterpretable questions and that such errors can be ameliorated by providing summarized input. We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% to 83%) as determined by expert annotators. We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.
   bibtex: |
      @inproceedings{Dugan-et-al-2022-feasibility-study,
        title={A Feasibility Study of Answer-Unaware Question Generation for Education},
        author={Liam Dugan and Eleni Miltsakaki and Etan Ginsberg and Shriyash Upadhyay and Hannah Gonzalez and Dahyeon Choi and Chuning Yuan and Chris Callison-Burch},
        booktitle={Findings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022)},
        address={Dublin, Ireland}
        year={2022}
      }
-
   title: QuakerBot&colon; A Household Dialog System Powered by Large Language Models
   authors: Artemis Panagopoulou, Manni Arora, Li Zhang, Dimitri Cugini, Weiqiu You, Yue Yang, Liyang Zhou, Yuxuan Wang, Zhaoyi Hou, Alyssa Hwang, Lara Martin, Sherry Shi, Chris Callison-Burch, Mark Yatskar
   venue: Amazon Alexa Competition
   type: preprint
   year: 2022
   url: https://assets.amazon.science/d2/af/3db9f05046e386108b76ce01e06d/quakerbot-a-household-dialog-system-powered-by-large-language-models.pdf
   page_count: 12
   id: taskbot
   abstract: We describe QuakerBot, a dialog system that helps users with household tasks and a participant in the Alexa Prize TaskBot Challenge. QuakerBot can process a variety of user requests, search for instructions from web resources such as wikiHow or Whole Foods Market recipes, answer related questions, and so on. Its components simultaneously consist of large language models with an impressive few-shot performance, and rule-based models with robust service.
   bibtex: |
      @misc{quakerbot,
        url = {https://assets.amazon.science/d2/af/3db9f05046e386108b76ce01e06d/quakerbot-a-household-dialog-system-powered-by-large-language-models.pdf},
        author = {Artemis Panagopoulou and Manni Arora and Li Zhang and Dimitri Cugini and Weiqiu You and Yue Yang and Liyang Zhou and Yuxuan Wang and Zhaoyi Hou and Alyssa Hwang and Lara Martin and Sherry Shi and Chris Callison-Burch and Mark Yatskar},
        title = {QuakerBot&colon A Household Dialog System Powered by Large Language Models},
        year = {2022},
      }
-
   title: Is "my favorite new movie" my favorite movie? Probing the Understanding of Recursive Noun Phrases
   authors: Qing Lyu, Hua Zheng, Daoxin Li, Li Zhang, Marianna Apidianaki, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2021
   url: https://arxiv.org/abs/2112.08326
   page_count: 12
   id: recursive-noun-phrases
   data: https://github.com/veronica320/Recursive-NPs
   abstract: Recursive noun phrases (NPs) have interesting semantic properties. For example, "my favorite new movie" is not necessarily "my favorite movie", whereas "my new favorite movie" is. This is common sense to humans, yet it is unknown whether pre-trained language models have such knowledge. We introduce the Recursive Noun Phrase Challenge (RNPC), a challenge set targeting the understanding of recursive NPs. When evaluated on our dataset, state-of-the-art Transformer models only achieve around chance performance. Still, we show that such knowledge is learnable with appropriate data. We further probe the models for relevant linguistic features that can be learned from our tasks, including modifier semantic category and modifier scope. Finally, models trained on RNPC achieve strong zero-shot performance on an extrinsic Harm Detection task, showing the usefulness of the understanding of recursive NPs in downstream applications.
   bibtex: |
      @article{Lyu2021-NPs,
        title={Is "my favorite new movie" my favorite movie? Probing the Understanding of Recursive Noun Phrases},
        author={Qing Lyu and Hua Zheng and Daoxin Li and Li Zhang and Marianna Apidianaki and Chris Callison-Burch},
        journal={arXiv preprint arXiv:2112.08326},
        year={2021}
      }
-
   title: SynthBio&colon; A Case Study in Human-AI Collaborative Curation of Text Datasets
   authors: Ann Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris Callison-Burch, Andy Coenen, and Sebastian Gehrmann
   venue: NeurIPS
   type: conference
   year: 2021
   url: https://www.cis.upenn.edu/~ccb/publications/synthbio.pdf
   page_count: 24
   id: synthbio
   data: https://storage.googleapis.com/gem-benchmark/SynthBio.json
   abstract: NLP researchers need more, higher-quality text datasets. Human-labeled datasets are expensive to collect, while datasets collected via automatic retrieval from the web such as WikiBio are noisy and can include undesired biases. Moreover, data sourced from the web is often included in datasets used to pretrain models, leading to inadvertent cross-contamination of training and test sets. In this work we introduce a novel method for efficient dataset curation&colon; we use a large language model to provide seed generations to human raters, thereby changing dataset authoring from a writing task to an editing task. We use our method to curate SynthBio - a new evaluation set for WikiBio - composed of structured attribute lists describing fictional individuals, mapped to natural language biographies. We show that our dataset of fictional biographies is less noisy than WikiBio, and also more balanced with respect to gender and nationality.
   bibtex: |
      @inproceedings{Yuan2021SynthBio,
        title={{SynthBio}: A Case Study in Human-{AI} Collaborative Curation of Text Datasets},
        author={Ann Yuan and Daphne Ippolito and Vitaly Nikolaev and Chris Callison-Burch and Andy Coenen and Sebastian Gehrmann},
        booktitle={35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.},
        year={2021}
      }
-
   title: Identifying and Responding to Health Misinformation on Reddit Dermatology Forums With Artificially Intelligent Bots Using Natural Language Processing&colon; Design and Evaluation Study
   authors: Monique A Sager, Aditya M Kashyap, Mila Tamminga, Sadhana Ravoori, Chris Callison-Burch and Jules B Lipoff 
   venue: JMIR
   type: journal
   year: 2021
   url: http://dx.doi.org/10.2196/20975 
   page_count: 7
   id: reddit-health-misinformation 
   abstract: |
      Background
      Reddit, the fifth most popular website in the United States, boasts a large and engaged user base on its dermatology forums where users crowdsource free medical opinions. Unfortunately, much of the advice provided is unvalidated and could lead to the provision of inappropriate care. Initial testing has revealed that artificially intelligent bots can detect misinformation regarding tanning and essential oils on Reddit dermatology forums and may be able to produce responses to posts containing misinformation.

      Objective
      To analyze the ability of bots to find and respond to tanning and essential oil–related health misinformation on Reddit’s dermatology forums in a controlled test environment.

      Methods
      Using natural language processing techniques, we trained bots to target misinformation, using relevant keywords and to post prefabricated responses. By evaluating different model architectures across a held-out test set, we compared performances.

      Results
      Our models yielded data test accuracies ranging 95%-100%, with a Bidirectional Encoder Representations from Transformers (BERT) fine-tuned model resulting in the highest level of test accuracy. Bots were then able to post corrective prefabricated responses to misinformation in a test environment.

      Conclusions
      Using a limited data set, bots accurately detected examples of health misinformation within Reddit dermatology forums. Given that these bots can then post prefabricated responses, this technique may allow for interception of misinformation. Providing correct information does not mean that users will be receptive or find such interventions persuasive. Further studies should investigate this strategy’s effectiveness to inform future deployment of bots as a technique in combating health misinformation.
   bibtex: |
      @Article{info:doi/10.2196/20975,
         author="Sager, Monique A and Kashyap, Aditya M and Tamminga, Mila and Ravoori, Sadhana and Callison-Burch, Chris and Lipoff, Jules B",
         title="Identifying and Responding to Health Misinformation on Reddit Dermatology Forums With Artificially Intelligent Bots Using Natural Language Processing: Design and Evaluation Study",
         journal="JMIR Dermatol",
         year="2021",
         month="Sep",
         day="30",
         volume="4",
         number="2",
         pages="e20975",
         keywords="bots; natural language processing; artificial intelligence; Reddit, medical misinformation; health misinformation; detecting misinformation; dermatology; misinformation",
         abstract="Background: Reddit, the fifth most popular website in the United States, boasts a large and engaged user base on its dermatology forums where users crowdsource free medical opinions. Unfortunately, much of the advice provided is unvalidated and could lead to the provision of inappropriate care. Initial testing has revealed that artificially intelligent bots can detect misinformation regarding tanning and essential oils on Reddit dermatology forums and may be able to produce responses to posts containing misinformation. Objective: To analyze the ability of bots to find and respond to tanning and essential oil--related health misinformation on Reddit's dermatology forums in a controlled test environment. Methods: Using natural language processing techniques, we trained bots to target misinformation, using relevant keywords and to post prefabricated responses. By evaluating different model architectures across a held-out test set, we compared performances. Results: Our models yielded data test accuracies ranging 95{\%}-100{\%}, with a Bidirectional Encoder Representations from Transformers            (BERT) fine-tuned model resulting in the highest level of test accuracy. Bots were then able to post corrective prefabricated responses to misinformation in a test environment. Conclusions: Using a limited data set, bots accurately detected examples of health misinformation within Reddit dermatology forums. Given that these bots can then post prefabricated responses, this technique may allow for interception of misinformation. Providing correct information does not mean that users will be receptive or find such interventions persuasive. Further studies should investigate this strategy's effectiveness to inform future deployment of bots as a technique in combating health misinformation. ",
         issn="2562-0959",
         doi="10.2196/20975",
         url="https://derma.jmir.org/2021/2/e20975",
         url="https://doi.org/10.2196/20975"
      }
-
   title: BiSECT&colon; Learning to Split and Rephrase Sentences with Bitexts
   authors: Joongwon Kim, Mounica Maddela, Reno Kriz, Wei Xu, and Chris Callison-Burch
   venue: EMNLP
   type: paper
   year: 2021
   id: bisect
   url: https://www.cis.upenn.edu/~ccb/publications/bisect.pdf
   page_count: 17
   award: Rose Undergraduate Research Award
   note: The Rose Foundation generously funds the Rose Undergraduate Research Award recognizing outstanding undergraduate research projects completed by graduating seniors at the University of Pennsylvania under the supervision of a Penn faculty member. The Rose Fund is administered by the Center for Undergraduate Research and Fellowships, and awards are determined each year on a competitive basis.
   code: https://github.com/mounicam/BiSECT
   data: https://github.com/mounicam/BiSECT/tree/main/bisect
   abstract:  An important task in NLP applications such as sentence simplification is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary.  We introduce a novel dataset and a new model for this `split and rephrase' task. Our BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using machine translation to convert both sides of the corpus into the same language. BiSECT contains higher quality training examples than previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our corpus, and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on BiSECT can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.
   bibtex: |
      @inproceedings{kim-maddela-et-al-2021,
        title={{BiSECT}: Learning to Split and Rephrase Sentences with Bitexts,
      author={Joongwon Kim and Mounica Maddela and Reno Kriz and Wei Xu and Chris Callison-Burch},
        booktitle={Proceedings of The 2021 Conference on Empirical Methods In Natural Language Proceedings (EMNLP)},
        year={2021},
        url={http://www.cis.upenn.edu/~ccb/publications/bisect.pdf}
      }
-
   title: GooAQ 🥑&colon; Open Question Answering with Diverse Answer Types
   authors: Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh Hajishirzi, Chris Callison-Burch
   venue: Findings of EMNLP
   type: paper
   year: 2021
   url: https://www.cis.upenn.edu/~ccb/publications/GooAQ.pdf
   page_count: 13
   id: GooAQ
   data: https://github.com/allenai/gooaq
   abstract: While day-to-day questions come with a variety of answer types, the current question-answering (QA) literature has failed to adequately address the answer diversity of questions. To this end, we present GooAQ, a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. GooAQ questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic questions of practical interest that are nonetheless short and expressed using simple language. GooAQ answers are mined from Google's responses to our collected questions, specifically from the answer boxes in the search results. This yields a rich space of answer types, containing both textual answers (short and long) as well as more structured ones such as collections. We benchmarkT5 models on GooAQ and observe that (a) in line with recent work, LM's strong performance on GooAQ's short-answer questions heavily benefit from annotated data; however, (b) their quality in generating coherent and accurate responses for questions requiring long responses (such as 'how' and 'why' questions) is less reliant on observing annotated data and mainly supported by their pre-training. We release GooAQ to facilitate further research on improving QA with diverse response types
   bibtex: |
      @inproceedings{khashabi2021gooaq,
        title={GooAQ: Open Question Answering with Diverse Answer Types},
        author={Khashabi, Daniel and Ng, Amos and Khot, Tushar and Sabharwal, Ashish and Hajishirzi, Hannaneh and Callison-Burch, Chris},
        booktitle={Proceedings of The 2021 Conference on Empirical Methods In Natural Language Proceedings (EMNLP)},
        year={2021},
        url={http://www.cis.upenn.edu/~ccb/publications/GooAQ.pdf}
      }
-
   title: Wikily Supervised Neural Translation Tailored to Cross-Lingual Tasks
   authors: Mohammad Sadegh Rasooli, Chris Callison-Burch, Derry Wijaya
   venue: EMNLP
   type: paper
   year: 2021
   url: https://www.cis.upenn.edu/~ccb/publications/wikily-supervised-translation.pdf
   page_count: 16
   id: wikily-supervised-translation
   code: https://github.com/rasoolims/ImageTranslate
   abstract:  We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from Wikipedia. Our final model achieves high BLEU scores that are close to or sometimes higher than strong supervised baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh. Moreover, we tailor our wikily translation models to unsupervised image captioning and cross-lingual dependency parser transfer. In image captioning, we train a multi-tasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a wikily translation of the English captioning data. Our captioning results in Arabic are slightly better than that of its supervised model. In dependency parsing, we translate a large amount of monolingual text, and use it as an artificial training data in an annotation projection framework. We show that our model outperforms recent work on cross-lingual transfer of dependency parsers.
   bibtex: |
      @inproceedings{rasooli2021wikily,
        title={``Wikily'' Supervised Neural Translation Tailored to Cross-Lingual Tasks},
        author={Rasooli, Mohammad Sadegh and Callison-Burch, Chris and Wijaya, Derry Tanti},
        booktitle={Proceedings of The 2021 Conference on Empirical Methods In Natural Language Proceedings (EMNLP)},
        year={2021},
        url={http://www.cis.upenn.edu/~ccb/publications/wikily-supervised-translation.pdf}
      }
-
   title: Visual Goal-Step Inference using wikiHow
   authors: Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, Chris Callison-Burch
   venue: EMNLP
   type: paper
   year: 2021
   data: https://github.com/YueYANG1996/wikiHow-VGSI
   code: https://github.com/YueYANG1996/wikiHow-VGSI
   url: https://www.cis.upenn.edu/~ccb/publications/visual-goal-step-inference-using-wikihow.pdf
   page_count: 13
   id: visual-goal-step-inference-using-wikihow
   abstract:  Procedural events can often be thought of as a high level goal composed of a sequence of steps. Inferring the sub-sequence of steps of a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task where a model is given a textual goal and must choose a plausible step towards that goal from among four candidate images. Our task is challenging for state-of-the-art muitimodal models. We introduce a novel dataset harvested from wikiHow that consists of 772,294 images representing human actions. We show that the knowledge learned from our data can effectively transfer to other datasets like HowTo100M, increasing the multiple-choice accuracy by 15% to 20%. Our task will facilitate multi-modal reasoning about procedural events.
   bibtex: |
      @inproceedings{yang2021visual,
        title={Visual Goal-Step Inference using {wikiHow},
        author={Yang, Yue and Panagopoulou, Artemis and Lyu, Qing and Zhang, Li and Yatskar, Mark and Callison-Burch, Chris},
        booktitle={Proceedings of The 2021 Conference on Empirical Methods In Natural Language Proceedings (EMNLP)},
        year={2021},
        url={http://www.cis.upenn.edu/~ccb/publications/visual-goal-step-inference-using-wikihow.pdf}
      }
-
   title: Goal-Oriented Script Construction
   authors: Qing Lyu and Li Zhang and Chris Callison-Burch
   venue: INGL
   type: conference
   year: 2021
   url: https://www.cis.upenn.edu/~ccb/publications/goal-oriented-script-construction.pdf
   page_count: 17
   id: goal-oriented-script-construction
   data: https://github.com/veronica320/wikihow-GOSC
   abstract: The knowledge of scripts, common chains of events in stereotypical scenarios, is a valuable asset for task-oriented natural language understanding systems. We propose the GoalOriented Script Construction task, where a model produces a sequence of steps to accomplish a given goal. We pilot our task on the first multilingual script learning dataset supporting 18 languages collected from wikiHow, a website containing half a million how-to articles. For baselines, we consider both a generation0based approach using a language model and a retrieval-based approach by first retrieving the relevant steps from a large candidate pool and then ordering them. We show that our task is practical, feasible but challenging for state-of-the-art Transformer models, and that our methods can be readily deployed for various other datasets and domains with decent zero-shot performance.
   bibtex: |
      @inproceedings{Lyu-et-al:2021,
       author = {Qing Lyu and Li Zhang and Chris Callison-Burch},
       title = {Goal-Oriented Script Construction},
       booktitle = {Proceedings of the 14th International Conference on Natural Language Generation (INLG 2021)},
       year = {2021},
       url = {http://www.cis.upenn.edu/~ccb/publications/goal-oriented-script-construction.pdf}
      }
-
   title: TopGuNN&colon; Fast NLP Training Data Augmentation using Large Corpora
   authors: Rebecca Iglesias-Flores, Megha Mishra, Ajay Patel, Akanksha Malhotra, Reno Kriz, Martha Palmer and Chris Callison-Burch
   venue: Workshop on Data Science with Human in the Loop 
   type: workshop
   year: 2021
   url: https://www.cis.upenn.edu/~ccb/publications/TopGuNN-system.pdf
   page_count: 15
   id: TopGuNN-system
   code: https://github.com/Penn-TopGuNN/TopGuNN
   abstract: Acquiring training data for natural language processing systems can be expensive and time-consuming. Given a few training examples crafted by experts, large corpora can be mined for thousands of semantically similar examples that provide useful variability to improve model generalization. We present TopGuNN, a fast contextualized k-NN retrieval system that can efficiently index and search over contextual embeddings generated from large corpora to easily retrieve new diverse training examples. TopGuNN is demonstrated for a semantic role labeling training data augmentation use case over the Gigaword corpus. Using approximate k-NN and an efficient architecture, TopGuNN performs queries over an embedding space of 4.63TB (approximately 1.5B embeddings) in less than a day.
   bibtex: |
      @inproceedings{TopGuNN-system:2021,
       author = {Rebecca Iglesias-Flores and Megha Mishra and Ajay Patel and Akanksha Malhotra and Reno Kriz and Martha Palmer and Chris Callison-Burch},
       title = {{TopGuNN}: Fast {NLP} Training Data Augmentation using Large Corpora},
       booktitle = {Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances},
       year = {2021},
       url = {http://www.cis.upenn.edu/~ccb/publications/TopGuNN-system.pdf}
      }
-
   title: RESIN&colon; A Dockerlized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System
   authors: Haoyang Wen, Ying Lin, Tuan Lai, Xiaoman Pan, Sha Li, Xudong Lin, Ben Zhou, Manling Li, Haoyu Wang, Hongming Zhang, Xiaodong Yu, Alexander Dong, Zhenhailong Wang, Yi Fung, Piyush Mishra, Qing Lyu, Dídac Surís, Brian Chen, Susan Windisch Brown, Martha Palmer, Chris Callison-Burch, Carl Vondrick, Jiawei Han, Dan Roth, Shih-Fu Chang, and Heng Ji
   venue: NAACL
   type: demo
   year: 2021
   url: https://www.cis.upenn.edu/~ccb/publications/kairos-resin-system.pdf
   code: https://github.com/RESIN-KAIROS/RESIN-pipeline-public
   page_count: 10
   id: kairos-resin-system
   abstract: We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects&colon; (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerized system publicly available for research purpose at GitHub, with a demo video.
   bibtex: |
      @inproceedings{kairos-resin-system:2021,
       author = {Haoyang Wen and Ying Lin and Tuan Lai and Xiaoman Pan and Sha Li and Xudong Lin and Ben Zhou and Manling Li and Haoyu Wang and Hongming Zhang and Xiaodong Yu and Alexander Dong and Zhenhailong Wang and Yi Fung and Piyush Mishra and Qing Lyu and Dídac Surís and Brian Chen and Susan Windisch Brown and Martha Palmer and Chris Callison-Burch and Carl Vondrick and Jiawei Han and Dan Roth and Shih-Fu Chang and Heng Ji},
       title = {{RESIN}: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System},
       booktitle = {Proceedings of The 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
       year = {2021},
       url = {http://www.cis.upenn.edu/~ccb/publications/kairos-resin-system.pdf}
      }
-
   title: Cultural and Geographical Influences on Image Translatability of Words across Languages
   authors: Nikzad Khani, Isidora Chara Tourni, Mohammad Sadegh Rasooli, Chris Callison-Burch and Derry Tanti Wijaya
   venue: NAACL
   type: conference
   year: 2021
   url: https://www.cis.upenn.edu/~ccb/publications/cultural-and-geographical-influences-on-image-translatability-of-words-across-languages.pdf
   code: https://github.com/nikzadkhani/MMID-CNN-Analysis
   page_count: 12
   id: cultural-and-geographical-influences-on-image-translatability-of-words-across-languages
   abstract: Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, e.g., *horse* may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has been shown to improve upon text-only models only marginally. To better understand *when* images are useful for translation, we study *image translatability* of words, which we define as the translatability of words via images, by measuring intra- and inter-cluster similarities of image representations of words that are translations of each other. We find that images of words are not always invariant across languages, and that similarities of image representations of words that are translations of each other. We find that a language pairs with shared culture, meaning having either a common language family, ethnicity or religion, have improved image translatability (i.e., have more similar images for similar words) compared to its converse regardless of their geographic proximity. In addition, in line with previous works that show images help more in translating concrete words, we found that concrete words have improved image translatability compared to abstract ones.
   bibtex: |
      @inproceedings{khani-cultural-and-geographical-influences:2021,
       author = {Nikzad Khani and Isidora Chara Tourni and Mohammad Sadegh Rasooli and Chris Callison-Burch and Derry Tanti Wijaya},
       title = {Cultural and Geographical Influences on Image Translatability of Words across Languages},
       booktitle = {Proceedings of The 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
       year = {2021},
       url = {http://www.cis.upenn.edu/~ccb/publications/cultural-and-geographical-influences-on-image-translatability-of-words-across-languages.pdf}
      }
-
   title: Simple-QE&colon; Better Automatic Quality Estimation for Text Simplification
   authors: Reno Kriz, Marianna Apidianaki, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2020
   url: https://arxiv.org/abs/2012.12382
   page_count: 6
   id: simple-qe
   abstract: Text simplification systems generate versions of texts that are easier to understand for a broader audience. The quality of simplified texts is generally estimated using metrics that compare to human references, which can be difficult to obtain. We propose Simple-QE, a BERT-based quality estimation (QE) model adapted from prior summarization QE work, and show that it correlates well with human quality judgments. Simple-QE does not require human references, which makes the model useful in a practical setting where users would need to be informed about the quality of generated simplifications. We also show that we can adapt this approach to accurately predict the complexity of human-written texts.
   bibtex: |
      @article{kriz2020simple,
        title={Simple-QE: Better Automatic Quality Estimation for Text Simplification},
        author={Kriz, Reno and Apidianaki, Marianna and Callison-Burch, Chris},
        journal={arXiv preprint arXiv:2012.12382},
        year={2020}
      }
-
   title: Automatic Standardization of Colloquial Persian
   authors: Mohammad Sadegh Rasooli, Farzane Bakhtyari, Fatemeh Shafiei, Mahsa Ravanbakhsh, Chris Callison-Burch
   venue: arXiv
   type: preprint
   year: 2020
   url: https://arxiv.org/abs/2012.05879
   data: https://github.com/rasoolims/Shekasteh
   code: https://github.com/rasoolims/PBreak
   page_count: 6
   id: standardization-of-persian
   abstract: The Iranian Persian language has two varieties&colon; standard and colloquial. Most natural language processing tools for Persian assume that the text is in standard form&colon; this assumption is wrong in many real applications especially web content. This paper describes a simple and effective standardization approach based on sequence-to-sequence translation. We design an algorithm for generating artificial parallel colloquial-to-standard data for learning a sequence-to-sequence model. Moreover, we annotate a publicly available evaluation data consisting of 1912 sentences from a diverse set of domains. Our intrinsic evaluation shows a higher BLEU score of 62.8 versus 61.7 compared to an off-the-shelf rule-based standardization model in which the original text has a BLEU score of 46.4. We also show that our model improves English-to-Persian machine translation in scenarios for which the training data is from colloquial Persian with 1.4 absolute BLEU score difference in the development data, and 0.8 in the test data.
   bibtex: |
      @article{rasooli2020automatic,
        title={Automatic Standardization of Colloquial Persian},
        author={Rasooli, Mohammad Sadegh and Bakhtyari, Farzane and Shafiei, Fatemeh and Ravanbakhsh, Mahsa and Callison-Burch, Chris},
        journal={arXiv preprint arXiv:2012.05879},
        year={2020}
      }
-
   title: Artificial Intelligence in mental health and the biases of language based models
   authors: Isabel Straw and Chris Callison-Burch
   venue: PLOS One
   type: journal
   year: 2020
   url: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0240376
   page_count: 19
   id: AI-and-mental-health
   abstract: The rapid integration of Artificial Intelligence (AI) into the healthcare field has occurred with little communication between computer scientists and doctors. The impact of AI on health outcomes and inequalities calls for health professionals and data scientists to make a collaborative effort to ensure historic health disparities are not encoded into the future. We present a study that evaluates bias in existing Natural Language Processing (NLP) models used in psychiatry and discuss how these biases may widen health inequalities. Our approach systematically evaluates each stage of model development to explore how biases arise from a clinical, data science and linguistic perspective.   A literature review of the uses of NLP in mental health was carried out across multiple disciplinary databases with defined Mesh terms and keywords. Our primary analysis evaluated biases within ‘GloVe’ and ‘Word2Vec’ word embeddings. Euclidean distances were measured to assess relationships between psychiatric terms and demographic labels, and vector similarity functions were used to solve analogy questions relating to mental health.  Our primary analysis of mental health terminology in GloVe and Word2Vec embeddings demonstrated significant biases with respect to religion, race, gender, nationality, sexuality and age. Our literature review returned 52 papers, of which none addressed all the areas of possible bias that we identify in model development. In addition, only one article existed on more than one research database, demonstrating the isolation of research within disciplinary silos and inhibiting cross-disciplinary collaboration or communication.  Our findings are relevant to professionals who wish to minimize the health inequalities that may arise as a result of AI and data-driven algorithms. We offer primary research identifying biases within these technologies and provide recommendations for avoiding these harms in the future.
   bibtex: |
    @article{straw2020,
      title={Artificial Intelligence in mental health and the biases of language based models},
      author={Straw, Isabel and Callison-Burch, Chris},
      journal={PloS one},
      volume={15},
      number={12},
      year={2020},
      publisher={Public Library of Science}
    }
-
   title: Reasoning about Goals, Steps, and Temporal Ordering with WikiHow
   authors: Qing Lyu*, Li Zhang*, Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2020
   url: https://www.cis.upenn.edu/~ccb/publications/reasoning-about-goals-with-wikihow.pdf
   data: https://github.com/zharry29/wikihow-goal-step
   code: https://colab.research.google.com/drive/16Qz4fp6eFHd0OGTITTHvw7sZo1Qe_VOS?usp=sharing
   page_count: 11
   id: reasoning-about-goals-with-wikihow
   abstract: We propose a suite of reasoning tasks on two types of relations between procedural events&colon; GOAL-STEP relations ("learn poses" is a step in the larger goal of "doing yoga") and STEP-STEP TEMPORAL relations ("buy a yoga mat" typically precedes "learn poses"). We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles. Our human-validated test set serves as a reliable benchmark for commonsense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance. Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and Story Cloze Test in zero- and few-shot settings.
   bibtex: |
      @inproceedings{lyu-zhang-wikihow:2020,
       author = {Qing Lyu and Li Zhang and Chris Callison-Burch},
       title = {Reasoning about Goals, Steps, and Temporal Ordering with WikiHow},
       booktitle = {Proceedings of The 2020 Conference on Empirical Methods In Natural Language Proceedings (EMNLP)},
       year = {2020},
       url = {http://www.cis.upenn.edu/~ccb/publications/reasoning-about-goals-with-wikihow.pdf}
      } 
-
   title: Intent Detection with WikiHow
   authors: Li Zhang, Qing Lyu, Chris Callison-Burch
   venue: AACL-IJCNLP
   type: conference
   year: 2020
   url: https://www.cis.upenn.edu/~ccb/publications/intent-detection-with-wikihow.pdf
   data: https://github.com/zharry29/wikihow-intent
   page_count: 6
   id: intent-detection-with-wikihow
   abstract: Modern task-oriented dialog systems need to reliably understand users’ intents. Intent detection is most challenging when moving to new domains or new languages, since there is little annotated data. To address this challenge, we present a suite of pretrained intent detection models. Our models are able to predict a broad range of intended goals from many actions because they are trained on wikiHow, a comprehensive instructional website. Our models achieve state-of-the-art results on the Snips dataset, the Schema-Guided Dialogue dataset, and all 3 languages of the Facebook multilingual dialog datasets. Our models also demonstrate strong zero- and few-shot performance, reaching over 75% accuracy using only 100 training examples in all datasets.
   bibtex: |
      @inproceedings{zhang-et-al-wikihow-intent-detection:2020,
       author = {Qing Lyu and Li Zhang and Chris Callison-Burch},
       title = {Intent Detection with WikiHow},
       booktitle = {Proceedings of The 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
       year = {2020},
       url = {http://www.cis.upenn.edu/~ccb/publications/intent-detection-with-wikihow.pdf}
      } 
-
   title: RoFT&colon; A Tool for Evaluating Human Detection of Machine-Generated Text
   authors: Liam Dugan*, Daphne Ippolito*, Arun Kirubarajan*, Chris Callison-Burch
   venue: EMNLP
   type: demo
   year: 2020
   url: https://www.cis.upenn.edu/~ccb/publications/roft-demo.pdf
   code: https://github.com/kirubarajan/roft
   website: https://roft.io
   page_count: 7
   id: roft-demo
   abstract: In recent years, large neural networks for natural language generation (NLG) have made leaps and bounds in their ability to generate fluent text. However, the tasks of evaluating quality differences between NLG systems and understanding how humans perceive the generated text remain both crucial and difficult. In this system demonstration, we present Real or Fake Text (RoFT), a website that tackles both of these challenges by inviting users to try their hand at detecting machine-generated text in a variety of domains. We introduce a novel evaluation task based on detecting the boundary at which a text passage that starts off human-written transitions to being machine-generated. We show preliminary results of using RoFT to evaluate detection of machine-generated news articles.
   bibtex: |
      @inproceedings{roft-demo:2020,
       author = {Liam Dugan and Daphne Ippolito and Arun Kirubarajan and Chris Callison-Burch},
       title = {RoFT: A Tool for Evaluating \\Human Detection of Machine-Generated Text},
       booktitle = {Proceedings of The 2020 Conference on Empirical Methods In Natural Language Proceedings (EMNLP) - Demo Track},
       year = {2020},
       url = {http://www.cis.upenn.edu/~ccb/publications/roft-demo.pdf}
      }
-
   title: Turkish Judge&colon; A Peer Evaluation Framework for Crowd Work Appeals
   authors: Edward Cohen, Mukund Venkateswaran, Nivedita Sankar, Chris Callison-Burch
   venue: HCOMP
   type: demo
   year: 2020
   url: https://www.cis.upenn.edu/~ccb/publications/turkish-judge-demo.pdf
   page_count: 9
   id: turkish-judge-demo
   abstract: We present a crowd-driven adjudication system for rejected work on Amazon Mechanical Turk. The Mechanical Turk crowdsourcing platform allows Requesters to approve or reject assignments submitted by Workers. If the work is rejected, then Workers aren’t paid, and their reputation suffers. Currently, there is no built-in mechanism for Workers to appeal rejections, other than contacting Requesters directly. The time it takes Requesters to review potentially incorrectly rejected tasks means that their costs are substantially higher than the payment amount that is in dispute. As a solution to this issue, we present an automated appeals system called Turkish Judge which employs crowd workers as judges to adjudicate whether work was fairly rejected when their peers initiate an appeal. We describe our system, analyze the added cost to Requesters, and discuss the advantages of such a system to the Mechanical Turk marketplace and other similar microtasking platforms.
   bibtex: |
      @inproceedings{turkish-judge-demo:2020,
       author = {Edward Cohen and Mukund Venkateswaran and Nivedita Sankar and Chris Callison-Burch},
       title = {Turkish Judge: A Peer Evaluation Framework for Crowd Work Appeals},
       booktitle = {Proceedings of the Eighth AAAI Conference on Human Computation and Crowdsourcing (HCOMP)},
       year = {2020},
       url = {http://www.cis.upenn.edu/~ccb/publications/turkish-judge-demo.pdf}
      } 
-
   title: Automatic Detection of Generated Text is Easiest when Humans are Fooled
   authors: Daphne Ippolito*, Daniel Duckworth*, Chris Callison-Burch and Douglas Eck
   venue: ACL
   type: conference
   year: 2020
   url: https://www.cis.upenn.edu/~ccb/publications/automatic-detection-of-generated-text-is-easiest-when-humans-are-fooled.pdf
   page_count: 15
   highly_cited: 158
   id: automatic-detection-of-generated-text-is-easiest-when-humans-are-fooled
   abstract: Recent advancements in neural language modeling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies like top-k, nucleus sampling, and untruncated random sampling—and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.
   bibtex: |
      @inproceedings{Ippolito-Duckworth-et-al:2020,
       author = {Daphne Ippolito and Daniel Duckworth and Douglas Eck and Chris Callison-Burch},
       title = {Automatic Detection of Generated Text is Easiest when Humans are Fooled},
       booktitle = {Proceedings of The 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
       year = {2020},
       url = {http://www.cis.upenn.edu/~ccb/publications/automatic-detection-of-generated-text-is-easiest-when-humans-are-fooled.pdf}
      } 
-
   title: Toward Better Storylines with Sentence-Level Language Models
   authors: Daphne Ippolito, David Grangier, Douglas Eck and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2020
   url: https://www.cis.upenn.edu/~ccb/publications/toward-better-storylines-with-sentence-level-language-models.pdf
   code: https://github.com/google-research/google-research/tree/master/better_storylines
   page_count: 7
   id: toward-better-storylines-with-sentence-level-language-models
   abstract: We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.
   bibtex: |
      @inproceedings{Ippolito-et-al:2020,
       author = {Daphne Ippolito and David Grangier and Douglas Eck and Chris Callison-Burch},
       title = {Toward Better Storylines with Sentence-Level Language Models},
       booktitle = {Proceedings of The 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
       year = {2020},
       url = {http://www.cis.upenn.edu/~ccb/publications/toward-better-storylines-with-sentence-level-language-models.pdf}
      } 
-
   title: Resolving Pronouns in Twitter Streams&colon; Context Can Help
   authors: Anietie Andy, Chris Callison-Burch and Derry Wijaya
   venue: Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC)
   type: workshop
   year: 2020
   url: https://www.cis.upenn.edu/~ccb/publications/resolving-pronouns-in-twitter-streams.pdf
   page_count: 6
   id: resolving-pronouns-in-twitter-streams
   abstract: Many people live-tweet televised events like Presidential debates and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people/characters. We propose an algorithm for resolving personal pronouns that make reference to people involved in an event, in tweet streams collected during the event.
   bibtex: |
      @inproceedings{Andy-et-al:2020,
       author = {Anietie Andy and Chris Callison-Burch and Derry Wijaya},
       title = {Resolving Pronouns in Twitter Streams&colon; Context Can Help},
       booktitle = {Proceedings of Third Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2020)},
       year = {2020},
       url = {http://www.cis.upenn.edu/~ccb/publications/resolving-pronouns-in-twitter-streams.pdf}
      } 
-
   title: The CLASSE GATOR (CLinical Acronym SenSE disambiGuATOR)&colon; A Method for Predicting Acronym Sense from Neonatal Clinical Notes
   authors: Aditya Kashyap, Heather Burris, Chris Callison-Burch, Mary Regina Boland
   venue: International Journal of Medical Informatics
   type: journal
   year: 2020
   url: https://www.sciencedirect.com/science/article/pii/S1386505619312122?via%3Dihub
   page_count: 
   id: CLASSE-GATOR
   abstract: |
      Objective
      To develop an algorithm for identifying acronym ‘sense’ from clinical notes without requiring a clinically annotated training set.

      Materials and Methods
      Our algorithm is called CLASSE GATOR: Clinical Acronym SenSE disambiGuATOR. CLASSE GATOR extracts acronyms and definitions from PubMed Central (PMC). A logistic regression model is trained using words associated with specific acronym-definition pairs from PMC. CLASSE GATOR uses this library of acronym-definitions and their corresponding word feature vectors to predict the acronym ‘sense’ from Beth Israel Deaconess (MIMIC-III) neonatal notes.

      Results
      We identified 1,257 acronyms and 8,287 definitions including a random definition from 31,764 PMC articles on prenatal exposures and 2,227,674 PMC open access articles. The average number of senses (definitions) per acronym was 6.6 (min = 2, max = 50). The average internal 5-fold cross validation was 87.9 % (on PMC). We found 727 unique acronyms (57.29 %) from PMC were present in 105,044 neonatal notes (MIMIC-III). We evaluated the performance of acronym prediction using 245 manually annotated clinical notes with 9 distinct acronyms. CLASSE GATOR achieved an overall accuracy of 63.04 % and outperformed random for 8/9 acronyms (88.89 %) when applied to clinical notes. We also compared our algorithm with UMN's acronym set, and found that CLASSE GATOR outperformed random for 63.46 % of 52 acronyms when using logistic regression, 75.00 % when using Bert and 76.92 % when using BioBert as the prediction algorithm within CLASSE GATOR.

      Conclusions
      CLASSE GATOR is the first automated acronym sense disambiguation method for clinical notes. Importantly, CLASSE GATOR does not require an expensive manually annotated acronym-definition corpus for training.
   bibtex: |
     @article{KASHYAP2020104101,
        title = "The {CLASSE GATOR (CLinical Acronym SenSE disambiGuATOR)}: A Method for predicting acronym sense from neonatal clinical notes",
        journal = "International Journal of Medical Informatics",
        volume = "137",
        year = "2020",
        doi = "https://doi.org/10.1016/j.ijmedinf.2020.104101",
        url = "http://www.sciencedirect.com/science/article/pii/S1386505619312122",
        author = "Aditya Kashyap and Heather Burris and Chris Callison-Burch and Mary Regina Boland",
        keywords = "Electronic health records, Natural language processing, Secondary reuse, Transfer learning"
      }
-
   title: SNAP judgments into the digital age&colon; Reporting on food stamps varies significantly with time, publication type, and political leaning
   authors: Benjamin Chrisinger, Eliza Kinsey, Ellie Pavlick, Chris Callison-Burch
   venue: PLOS One
   type: journal
   year: 2020
   url: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0229180
   page_count: 19
   id: SNAP-judgments
   abstract: The Supplemental Nutrition Assistance Program (SNAP) is the second-largest and most contentious public assistance program administered by the United States government. The media forums where SNAP discourse occurs have changed with the advent of social and web-based media. We used machine learning techniques to characterize media coverage of SNAP over time (1990–2017), between outlets with national readership and those with narrower scopes, and, for a subset of web-based media, by the outlet’s political leaning. We applied structural topic models, a machine learning methodology that categorizes and summarizes large bodies of text that have document-level covariates or metadata, to a corpus of print media retrieved via LexisNexis (n = 76,634). For comparison, we complied a separate corpus via web-scrape algorithm of the Google News API (2012–2017), and assigned political alignment metadata to a subset documents according to a recent study of partisanship on social media. A similar procedure was used on a subset of the print media documents that could be matched to the same alignment index. Using linear regression models, we found some, but not all, topics to vary significantly with time, between large and small media outlets, and by political leaning. Our findings offer insights into the polarized and partisan nature of a major social welfare program in the United States, and the possible effects of new media environments on the state of this discourse.
   bibtex: |
    @article{chrisinger2020snap,
      title={SNAP judgments into the digital age: Reporting on food stamps varies significantly with time, publication type, and political leaning},
      author={Chrisinger, Benjamin W and Kinsey, Eliza W and Pavlick, Ellie and Callison-Burch, Chris},
      journal={PloS one},
      volume={15},
      number={2},
      year={2020},
      publisher={Public Library of Science}
    }
-
   title: Paraphrase-Sense-Tagged Sentences
   authors: Anne Cocos and Chris Callison-Burch
   venue: TACL
   type: journal
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/paraphrase-sense-tagged-sentences.pdf
   website: http://www.psts.io
   data: https://github.com/acocos/paraphrase-sense-tagged-sentences
   page_count: 15
   id: paraphrase-sense-tagged-sentences
   abstract: Many natural language processing tasks require discriminating the particular meaning of a word in context, but building corpora for developing sense-aware models can be a challenge. We present a large resource of example usages for words having a particular meaning, called Paraphrase-Sense Tagged Sentences (PSTS). Built upon the premise that a word's paraphrases instantiate its fine-grained meanings – i.e. bug has different meanings corresponding to its paraphrases fly and microbe – the resource contains up to 10,000 sentences for each of 3 million target-paraphrase pairs where the target word takes on the meaning of the paraphrase. We describe an automatic method based on bilingual pivoting used to enumerate sentences for PSTS, and present two models for ranking PSTS sentences based on their quality. Finally, we demonstrate the utility of PSTS by using it to build a dataset for the task of hypernym prediction in context. Training a model on this automatically-generated dataset produces accuracy that is competitive with a model trained on smaller datasets crafted with some manual effort.
   bibtex: |
      @article{Cocos-Callison-Burch:2019:TACL,
         author = {Anne Cocos and Chris Callison-Burch},
         title = {Paraphrase-Sense-Tagged Sentences},
         journal = {Transactions of the Association for Computational Linguistics},
         volume = {},
         year = {2019},
         url = {http://www.cis.upenn.edu/~ccb/publications/paraphrase-sense-tagged-sentences.pdf},
         pages = {}
       }
-
   title: PerspectroScope&colon; A Window to the World of Diverse Perspectives
   authors: Sihao Chen, Daniel Khashabi, Chris Callison-Burch and Dan Roth
   venue: ACL
   type: demo
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/perspectroscope-demo.pdf
   code: https://github.com/CogComp/perspectroscope
   page_count: 6
   id: perspectroscope-demo
   abstract: This work presents PERSPECTROSCOPE, a web-based system which lets users query a discussion-worthy natural language claim, and extract and visualize various perspectives in support or against the claim, along with evidence supporting each perspective. The system thus lets users explore various perspectives that could touch upon aspects of the issue at hand. The system is built as a combination of retrieval engines and learned textualentailment-like classifiers built using a few recent developments in natural language understanding. To make the system more adaptive, expand its coverage, and improve its decisions over time, our platform employs various mechanisms to get corrections from the users. PERSPECTROSCOPE is available at github.com/CogComp/perspectroscope.  A brief video of the system is available at youtube.com/watch?v=MXBTR1Sp3Bs.
   bibtex: |
      @inproceedings{Chen-Khashabi-et-al:2019,
       author = {Sihao Chen and Daniel Khashabi and Chris Callison-Burch and Dan Roth},
       title = {PerspectroScope&colon; A Window to the World of Diverse Perspectives},
       booktitle = {Proceedings of The 57th Annual Meeting of the Association for Computational Linguistics (ACL) demo session},
       year = {2019},
       address = {Florence, Italy},
       url = {http://www.cis.upenn.edu/~ccb/publications/comparison-of-diverse-decoding-methods-from-conditional-language-models.pdf}
      } 
-
   title: Comparison of Diverse Decoding Methods from Conditional Language Models
   authors: Daphne Ippolito*, Reno Kriz*, João Sedoc, Maria Kustikova and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/comparison-of-diverse-decoding-methods-from-conditional-language-models.pdf
   code: https://github.com/rekriz11/DeDiv
   page_count: 11
   id: comparison-of-diverse-decoding-methods-from-conditional-language-models
   abstract: While conditional language models have greatly improved in their ability to output high-quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that re-rank and combine candidate outputs. Standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. In this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from conditional language models. We also show how diversity can be improved without sacrificing quality by oversampling additional candidates, then filtering to the desired number.
   bibtex: |
      @inproceedings{Ippolito-Kriz-et-al:2019,
       author = {Daphne Ippolito and Reno Kriz and Joao Sedoc and Maria Kustikova and Chris Callison-Burch},
       title = {Comparison of Diverse Decoding Methods from Conditional Language Models},
       booktitle = {Proceedings of The 57th Annual Meeting of the Association for Computational Linguistics (ACL) },
       year = {2019},
       address = {Florence, Italy},
       url = {http://www.cis.upenn.edu/~ccb/publications/comparison-of-diverse-decoding-methods-from-conditional-language-models.pdf}
      } 
-
   title: Winter is here&colon; Summarizing Twitter Streams related to Pre-Scheduled Events
   authors: Anietie Andy, Derry Wijaya and Chris Callison-Burch
   venue: Proceedings of the Second Workshop on Storytelling
   type: workshop
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/winter-is-here.pdf
   page_count: 5
   id: winter-is-here
   abstract: Pre-scheduled events, such as TV shows and sports games, usually garner considerable attention from the public. Twitter captures large volumes of discussions and messages related to these events, in real-time. Twitter streams related to pre-scheduled events are characterized by the following&colon; (1) spikes in the volume of published tweets reflect the highlights of the event and (2) some of the published tweets make reference to the characters involved in the event, in the context in which they are currently portrayed in a subevent. In this paper, we take advantage of these characteristics to identify the highlights of pre-scheduled events from tweet streams and we demonstrate a method to summarize these highlights. We evaluate our algorithm on tweets collected around 2 episodes of a popular TV show, Game of Thrones, Season 7.
   bibtex: |
      @inproceedings{Andy-Wijaya-et-al:2019,
       author = {Anietie Andy and Derry Wijaya and Chris Callison-Burch},
       title = {Winter is here: Summarizing Twitter Streams related to Pre-Scheduled Events},
       booktitle = {Proceedings of the Second Workshop on Storytelling},
       year = {2019},
       address = {Florence, Italy},
       url = {http://www.cis.upenn.edu/~ccb/publications/winter-is-here.pdf}
       } 
-
   title: A Comparison of Context-sensitive Models for Lexical Substitution
   authors: Aina Garí Soler, Anne Cocos, Marianna Apidianaki, Chris Callison-Burch
   venue: 13th International Conference on Computational Semantics (IWCS)
   type: workshop
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/comparison-of-context-sensitive-models-for-lexical-substitution.pdf
   page_count: 12
   id: comparison-of-context-sensitive-models-for-lexical-substitution
   abstract: Word embedding representations provide good estimates of word meaning and give state-of-the art performance in semantic tasks. Embedding approaches differ as to whether and how they account for the context surrounding a word. We present a comparison of different word and context representations on the task of proposing substitutes for a target word in context (lexical substitution). We also experiment with tuning contextualized word embeddings on a dataset of sense-specific instances for each target word. We show that powerful contextualized word representations, which give high performance in several semantics-related tasks, deal less well with the subtle in-context similarity relationships needed for substitution. This is better handled by models trained with this objective in mind, where the inter-dependence between word and context representations is explicitly modeled during training.
   bibtex: |
      @inproceedings{Gar-Soler:2019,
       author = {Aina Gar\'{i} Soler and Anne Cocos and Marianna Apidianaki and Chris Callison-Burch},
       title = {A Comparison of Context-sensitive Models for Lexical  Substitution},
       booktitle = {Proceedings of the 13th International Conference on Computational Semantics (IWCS)},
       year = {2019},
       address = {Gothenburg, Sweden},
       url = {http://www.cis.upenn.edu/~ccb/publications/comparison-of-context-sensitive-models-for-lexical-substitution.pdf}
       } 
-
   title: Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification
   authors: Reno Kriz, João Sedoc, Marianna Apidianaki, Carolina Zheng, Gaurav Kumar, Eleni Miltsakaki,  and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/complexity-weighted-loss-for-sentence-simplification.pdf
   page_count: 10
   id: complexity-weighted-loss-for-sentence-simplification
   abstract: Sentence simplification is the task of rewriting texts so they are easier to understand. Recent research has applied sequence-to-sequence (Seq2Seq) models to this task, focusing largely on training-time improvements via reinforcement learning and memory augmentation. One of the main problems with applying generic Seq2Seq models for simplification is that these models tend to copy directly from the original sentence, resulting in outputs that are relatively long and complex. We aim to alleviate this issue through the use of two main techniques. First, we incorporate content word complexities, as predicted with a leveled word complexity model, into our loss function during training. Second, we generate a large set of diverse candidate simplifications at test time, and rerank these to promote fluency, adequacy, and simplicity. Here, we measure simplicity through a novel sentence complexity model. These extensions allow our models to perform competitively with state-of-the-art systems while generating simpler sentences. We report standard automatic and human evaluation metrics.
   bibtex: |
      @inproceedings{Kriz-et-al:2019:NAACL,
       author = {Reno Kriz and Joao Sedoc and Marianna Apidianaki and Carolina Zheng and Gaurav Kumar and Eleni Miltsakaki and Chris Callison-Burch},
       title = {Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification},
       booktitle = {The 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2019)},
       month = {June},
       year = {2019},
       address = {Minneapolis, Minnesota},
       url = {http://www.cis.upenn.edu/~ccb/publications/complexity-weighted-loss-for-sentence-simplification.pdf}
       } 
-
   title: Seeing Things from a Different Angle&colon; Discovering Diverse Perspectives about Claims
   authors: Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch and Dan Roth
   venue: NAACL
   type: conference
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/discovering-diverse-perspectives.pdf
   data: https://github.com/CogComp/perspectrum
   page_count: 16
   id: discovering-diverse-perspectives
   abstract: One key consequence of the information revolution is a significant increase and a contamination of our information supply. The practice of fact checking won’t suffice to eliminate the biases in text data we observe, as the degree of factuality alone does not determine whether biases exist in the spectrum of opinions visible to us. To better understand controversial issues, one needs to view them from a diverse yet comprehensive set of perspectives. For example, there are many ways to respond to a claim such as “animals should have lawful rights”, and these responses form a spectrum of perspectives, each with a stance relative to this claim and, ideally, with evidence supporting it. Inherently, this is a natural language understanding task, and we propose to address it as such. Specifically, we propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. We construct PERSPECTRUM, a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify our dataset. We use crowdsourcing to filter out the noise and ensure high-quality data. Our dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively. We provide a thorough analysis of the dataset to highlight key underlying language understanding challenges, and show that human baselines across multiple subtasks far outperform machine baselines built upon state-of-the-art NLP techniques. This poses a challenge and opportunity for the NLP community to address.
   bibtex: |
      @inproceedings{Chen-et-al:2019:NAACL,
       author = {Sihao Chen and Daniel Khashabi and Wenpeng Yin and Chris Callison-Burch and Dan Roth},
       title = {Seeing Things from a Different Angle&colon; Discovering Diverse Perspectives about Claims},
       booktitle = {The 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2019)},
       month = {June},
       year = {2019},
       address = {Minneapolis, Minnesota},
       url = {http://www.cis.upenn.edu/~ccb/publications/discovering-diverse-perspectives.pdf}
       } 
-
   title: ChatEval&colon; A Tool for the Systematic Evaluation of Chatbots
   authors: João Sedoc*, Daphne Ippolito*, Arun Kirubarajan, Jai Thirani, Lyle Ungar, and Chris Callison-Burch
   venue: NAACL
   type: demo
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/chateval-demo.pdf
   website: https://chateval.org
   page_count: 6
   id: chateval-demo
   abstract: Open-domain dialog systems (i.e. chatbots) are difficult to evaluate. The current best practice for analyzing and comparing these dialog systems is the use of human judgments. However, the lack of standardization in evaluation procedures, and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments. We introduce a unified framework for human evaluation of chatbots that augments existing tools and provides a web-based hub for researchers to share and compare their dialog systems. Researchers can submit their trained models to the ChatEval web interface and obtain comparisons with baselines and prior work. The evaluation code is open-source to ensure standardization and transparency. In addition, we introduce open-source baseline models and evaluation datasets. ChatEval can be found at chateval.org.
   bibtex: |
      @inproceedings{Sedoc:2018:ChatEval,
       author = {Joao Sedoc and Daphne Ippolito and Arun Kirubarajan and Jai Thirani and Lyle Ungar and Chris Callison-Burch},
       title = {ChatEval&colon; A Tool for the Systematic Evaluation of Chatbots},
       booktitle = {The 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2019) Demonstrations},
       month = {June},
       year = {2019},
       address = {Minneapolis, Minnesota},
       url = {http://www.cis.upenn.edu/~ccb/publications/chateval-demo.pdf}
       } 
-
   title: Unsupervised Hierarchical Story Infilling
   authors:  Daphne Ippolito, David Grangier, Chris Callison-Burch and Douglas Eck
   venue: 13th International Conference on Computational Semantics (IWCS)
   type: workshop
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/story-infilling.pdf
   page_count: 7
   id: story-infilling
   abstract: Story infilling involves predicting words to go into a missing span from a story. This challenging task has the potential to transform interactive tools for creative writing. However, state-of-the-art conditional language models have trouble balancing fluency and coherence with novelty and diversity. We address this limitation with a hierarchical model which first selects a set of rare words and then generates text conditioned on that set. By relegating the high entropy task of picking rare words to a word-sampling model, the second-stage model conditioned on those words can achieve high fluency and coherence by searching for likely sentences, without sacrificing diversity.
   bibtex: |
      @inproceedings{Ippolito-et-al:2019,
       author = {Daphne Ippolito and David Grangier and Chris Callison-Burch and Douglas Eck},
       title = {A Comparison of Context-sensitive Models for Lexical  Substitution},
       booktitle = {Proceedings of the First Workshop on Narrative Understanding},
       year = {2019},
       address = {Minneapolis, Minnesota},
       url = {http://www.cis.upenn.edu/~ccb/publications/story-infilling.pdf}
       } 
-
   title: Anonymization of Sensitive Information in Medical Health Records
   authors:  Bhavna Saluja, Gaurav Kumar, João Sedoc, and Chris Callison-Burch
   venue: Iberian Languages Evaluation Forum
   type: workshop
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/meddocan-shared-task-submission.pdf
   page_count: 7
   id: meddocan-shared-task-submission
   abstract: Due to privacy constraints, clinical records with protected health information (PHI) cannot be directly shared. De-identification, i.e., the exhaustive removal, or replacement, of all mentioned PHI phrases has to be performed before making the clinical records available outside of hospitals. We have tried to identify PHI on medical records written in Spanish language. We applied two approaches for the anonymization of medical records in this paper. In the first approach, we gathered various token-level features and built a LinearSVC model which gave us F1 score of 0.861 on test data. In the other approach, we built a neural network involving an LSTM-CRF model which gave us a higher F1 score of 0.935 which is an improvement over the first approach.
   bibtex: |
      @inproceedings{Saluja-Kumar-et-al:2019,
       author = {Bhavna Saluja and Gaurav Kumar and Joao Sedoc and Chris Callison-Burch},
       title = {Anonymization of Sensitive Information in Medical Health Records},
       booktitle = {Iberian Languages Evaluation Forum},
       year = {2019},
       address = {Bilbao, Spain},
       url = {http://www.cis.upenn.edu/~ccb/publications/meddocan-shared-task-submission.pdf}
       } 
-
   title: Natural Language Processing of Reddit Data to Evaluate Dermatology Patient Experiences and Therapeutics
   authors: Edidiong Okon, Vishnutheja Rachakonda, Hyo Jung Hong,  Chris Callison-Burch and Jules Lipoff
   venue: Journal of the American Academy of Dermatology
   type: journal
   year: 2019
   url: https://www.sciencedirect.com/science/article/pii/S0190962219323710
   page_count: 20
   id: reddit-dermatology
   abstract: Background - There is a lack of research studying patient-generated data on Reddit, one of the world’s most popular forums with active users interested in dermatology. Techniques within natural language processing, a field of artificial intelligence, can analyze large amounts of text information and extract insights. Objective - To apply natural language processing to Reddit comments about dermatology topics to assess for feasibility and potential for insights and engagement. Methods - A software pipeline preprocessed Reddit comments from 2005 to 2017 from seven popular dermatology-related subforums on Reddit, applied Latent Dirichlet allocation (LDA), and used spectral clustering to establish cohesive themes and the frequency of word representation and grouped terms within these topics. Results - We created a corpus of 176K comments and identified trends in patient engagement in spaces such as eczema, acne, etc., with a focus on homeopathic treatments and Accutane. Limitations - LDA is an unsupervised model, meaning there is no ground truth to which the model output can be compared. However, as these forums are anonymous, there seems little incentive for patients to be dishonest. Conclusions -  Reddit data has viability and utility for dermatologic research and engagement with the public, especially for common dermatology topics such as tanning, acne, and psoriasis.
   bibtex: |
      @article{Okon-EtAl:2016:JAAD,
         author = {Edidiong Okon and Vishnutheja Rachakonda and Hyo Jung Hong and  Chris Callison-Burch and Jules Lipoff},
         title = {Natural Language Processing of Reddit Data to Evaluate Dermatology Patient Experiences and Therapeutics},
         journal = {Journal of the American Academy of Dermatology},
         volume = {},
         number = {},
         year = {2017},
         url = {https://www.sciencedirect.com/science/article/pii/S0190962219323710}
       }
-
   title: Worker Demographics and Earnings on Amazon Mechanical Turk&colon; An Exploratory Analysis
   authors: Kotaro Hara, Abigail Adams, Kristy Milland, Saiph Savage, Benjamin V. Hanrahan, Jeffrey P. Bigham and Chris Callison-Burch
   venue: CHI Late Breaking Work
   type: conference
   year: 2019
   url: https://www.cis.upenn.edu/~ccb/publications/crowd-workers-demographics.pdf
   page_count: 5
   id: crowd-workers-demographics
   abstract: Prior research reported that workers on Amazon Mechanical Turk (AMT) are underpaid, earning about $2/h. But the prior research did not investigate the difference in wage due to worker characteristics (e.g., country of residence). We present the first data-driven analysis on wage gap on AMT. Using work log data and demographic data collected via online survey, we analyse the gap in wage due to different factors. We show that there is indeed wage gap; for example, workers in the U.S. earn $3.01/h while those in India earn $1.41/h.
   bibtex: |
      @inproceedings{Hara-et-al:2019:CHI-LBW,
       author = {Kotaro Hara and Abigail Adams and Kristy Milland and Saiph Savage and Benjamin V. Hanrahan and Jeffrey P. Bigham and Chris Callison-Burch},
       title = {Worker Demographics and Earnings on Amazon Mechanical Turk&colon; An Exploratory Analysis},
       booktitle = {CHI'19 Late Breaking Work},
       month = {May},
       year = {2019},
       address = {Glasgow, Scotland, UK},
       url = {http://www.cis.upenn.edu/~ccb/publications/crowd-workers-demographics.pdf}
       } 
-
   title: Learning Scalar Adjective Intensity from Paraphrases
   authors: Anne Cocos, Skyler Wharton, Ellie Pavlick, Marianna Apidianaki and Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2018
   url: https://www.cis.upenn.edu/~ccb/publications/learning-scalar-adjective-intensity-from-paraphrases.pdf
   page_count: 11 
   id: learning-scalar-adjective-intensity-from-paraphrases
   abstract: Adjectives like *warm*, *hot*, and *scalding* all describe temperature but differ in intensity. Understanding these differences between adjectives is a necessary part of reasoning about natural language. We propose a new paraphrase-based method to automatically learn the relative intensity relation that holds between a pair of scalar adjectives. Our approach analyzes over 36k adjectival pairs from the Paraphrase Database under the assumption that, for example, paraphrase pair *really hot* <-> *scalding* suggests that *hot* < *scalding*. We show that combining this paraphrase evidence with existing, complementary pattern- and lexicon-based approaches improves the quality of systems for automatically ordering sets of scalar adjectives and inferring the polarity of indirect answers to yes/no questions.
   bibtex: |
      @inproceedings{Cocos-et-al:2018:ACL,
       author = {Anne Cocos and Skyler Wharton and Ellie Pavlick and Marianna Apidianaki and Chris Callison-Burch},
       title = {Learning Scalar Adjective Intensity from Paraphrases},
       booktitle = {2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018)},
       month = {November},
       year = {2018},
       address = {Brussels, Belgium},
       url = {http://www.cis.upenn.edu/~ccb/publications/learning-scalar-adjective-intensity-from-paraphrases.pdf}
       } 
-
   title: Magnitude&colon; A Fast, Efficient Universal Vector Embedding Utility Package
   authors: Ajay Patel, Alex Sands, Marianna Apidianaki and Chris Callison-Burch
   venue: EMNLP
   type: demo
   year: 2018
   url: https://www.cis.upenn.edu/~ccb/publications/magnitude-fast-efficient-vector-embeddings-in-python.pdf
   page_count: 7 
   id: magnitude-fast-efficient-vector-embeddings-in-python
   abstract: Vector space embedding models like word2vec, GloVe, and fastText are extremely popular representations in natural language processing (NLP) applications.  We present Magnitude, a fast, lightweight tool for utilizing and processing embeddings.  Magnitude is an open source Python package with a compact vector storage file format that allows for efficient manipulation of huge numbers of embeddings.  Magnitude performs common operations up to 60 to 6,000 times faster than Gensim. Magnitude introduces several novel features for improved robustness like out-of-vocabulary lookups.
   bibtex: |
      @inproceedings{Cocos-et-al:2018:ACL,
       author = {Ajay Patel and Alex Sands and Marianna Apidianaki and Chris Callison-Burch},
       title = {Magnitude&colon; A Fast, Efficient Universal Vector Embedding Utility Package},
       booktitle = {2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018) - Demos},
       month = {November},
       year = {2018},
       address = {Brussels, Belgium},
       url = {http://www.cis.upenn.edu/~ccb/magnitude-fast-efficient-vector-embeddings-in-python.pdf}
       } 
-
   title: Learning Translations via Images with a Massively Multilingual Image Dataset
   authors: John Hewitt*, Daphne Ippolito*, Brendan Callahan, Reno Kriz, Derry Wijaya and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2018
   url: https://www.cis.upenn.edu/~ccb/publications/learning-translations-via-images.pdf
   page_count: 12 
   id: learning-translations-via-images
   abstract: We conduct the most comprehensive study to date into translating words via images.  To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents.  Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings.  In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages.  We run experiments on a dozen high resource languages and 20 low resources languages, demonstrating the effect of word concreteness and part-of-speech on translation quality.  To improve image-based translation, we introduce a novel method of predicting word concreteness from images, which improves upon previously state-of-the-art unsupervised techniques. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and dataset will be made available.
   bibtex: |
      @inproceedings{Hewitt-et-al:2018:ACL,
       author = {John Hewitt and Daphne Ippolito and Brendan Callahan and Reno Kriz and Derry Wijaya and Chris Callison-Burch},
       title = {Learning Translations via Images with a Massively Multilingual Image Dataset},
       booktitle = {The 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)},
       month = {July},
       year = {2018},
       address = {Melbourne, Australia},
       url = {http://www.cis.upenn.edu/~ccb/publications/learning-translations-via-images.pdf}
       } 
-
   title: Simplification Using Paraphrases and Context-based Lexical Substitution
   authors: Reno Kriz, Eleni Miltsakaki, Marianna Apidianaki and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2018
   url: https://www.cis.upenn.edu/~ccb/publications/simpliciation-using-paraphrases-and-lexical-substitution.pdf
   page_count: 12 
   id: simpliciation-using-paraphrases-and-lexical-substitution
   abstract: Lexical simplification involves, among other steps, identifying complex words or phrases that need simplification and recommending simpler words or phrases that can be more easily understood. In this paper, we improve the task of identifying English complex words by proposing a model that exploits both lexical and contextual features for identifying words in a text that need to be simplified. We improve the lexical substitution task by using a word embedding-based lexical substitution model, and replacing the detected complex words with simpler paraphrases that preserve the meaning of the original segments. We compare our lexical simplification system to several baselines, and evaluate the best performing system using human judgments for 2,351 tokens. 
   bibtex: |
      @inproceedings{Kriz-et-al:2018:NAACL,
       author = {Reno Kriz and Eleni Miltsakaki and Marianna Apidianaki and Chris Callison-Burch},
       title = {Simplification Using Paraphrases and Context-based Lexical Substitution},
       booktitle = {The 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2018)},
       month = {June},
       year = {2018},
       address = {New Orleans, Louisiana },
       url = {http://www.cis.upenn.edu/~ccb/publications/simpliciation-using-paraphrases-and-lexical-substitution.pdf}
       } 
-
   title: Automated Paraphrase Lattice Creation for HyTER Machine Translation Evaluation
   authors: Marianna Apidianaki, Guillaume Wisniewski, Anne Cocos and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2018
   url: https://www.cis.upenn.edu/~ccb/publications/hyter-paraphrase-lattices.pdf
   page_count: 6 
   id: hyter-paraphrase-lattices
   abstract: Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references. The HyTER metric relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks encode the set of all correct translations for a sentence, and HyTER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations
   bibtex: |
      @inproceedings{Apidianaki-et-al:2018:NAACL,
       author = {Marianna Apidianaki and Guillaume Wisniewski and Anne Cocos and Chris Callison-Burch},
       title = {Automated Paraphrase Lattice Creation for {HyTER} Machine Translation Evaluation},
       booktitle = {The 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2018)},
       month = {June},
       year = {2018},
       address = {New Orleans, Louisiana },
       url = {http://www.cis.upenn.edu/~ccb/publications/hyter-paraphrase-lattices.pdf}
       } 
-
   title: Comparing Constraints for Taxonomic Organization
   authors: Anne Cocos, Marianna Apidianaki and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2018
   url: https://www.cis.upenn.edu/~ccb/publications/comparing-constraints-for-taxonomic-organization.pdf
   page_count: 12 
   id: comparing-constraints-for-taxonomic-organization
   abstract: Building a lexical taxonomy from the ground up involves several sub-tasks&colon; selecting terms to include, predicting relations between terms, and selecting the best subset of relations to keep, given constraints on the taxonomy graph. Methods for the final taxonomic organization step vary in terms of the constraints they impose, and whether they enable discovery of synonymous terms. But it is hard to isolate the impact of these factors on the quality of the resulting taxonomy because organization methods are rarely compared directly. In this paper, we present a head-to-head comparison of six taxonomic organization algorithms that vary with respect to their structural and transitivity constraints, and treatment of synonymy. We find that while transitive algorithms out-perform their non-transitive counterparts, the top-performing transitive algorithm is prohibitively slow for taxonomies with as few as 50 entities. We propose a simple modification to a non-transitive maximum spanning tree algorithm to explicitly incorporate synonymy, resulting in a method that is orders of magnitude faster than the top-performer while giving comparable performance.
   bibtex: |
      @inproceedings{Cocos-et-al:2018:NAACL,
       author = {Anne Cocos and Marianna Apidianaki and Chris Callison-Burch},
       title = {Comparing Constraints for Taxonomic Organization},
       booktitle = {The 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2018)},
       month = {June},
       year = {2018},
       address = {New Orleans, Louisiana },
       url = {http://www.cis.upenn.edu/~ccb/publications/comparing-constraints-for-taxonomic-organization.pdf}
       } 
-
   title: A Data-Driven Analysis of Workers’ Earnings on Amazon Mechanical Turk
   authors: Kotaro Hara, Abigail Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, Jeffrey P. Bigham
   venue: CHI
   type: conference
   award: Honorable Mention Award
   note: Honorable Mention Award for CHI 2018, which means that it is ranked among the top 5% of all submissions to the SIGCHI 2018 conference. The conference received over 2500 submissions and 25 were chosen for Best Paper (101 were chosen for Honourable Mention.)
   highly_cited: 590
   press: 
    - 
      title: Newly Unemployed, and Labeling Photos for Pennies&colon; People who've lost jobs and are stuck indoors are turning to crowd work—filling out online surveys and transcribing audio for less than the minimum wage.
      source: Wired Magazine
      date: April 23, 2020
      url: https://www.wired.com/story/newly-unemployed-labeling-photos-pennies/
    - 
      title: America’s Gig-Based Economy Gets Zero 
      source: Full Frontal with Samantha Bee
      date: March 11, 2020
      url: https://www.youtube.com/watch?v=u11DK9kMp5Y&t=96
    - 
      title: I Found Work on an Amazon Website. I Made 97 Cents an Hour.
      source: New York Times
      date: November 15, 2019
      url: https://www.nytimes.com/interactive/2019/11/15/nyregion/amazon-mechanical-turk.html
   year: 2018
   url: https://www.cis.upenn.edu/~ccb/publications/data-driven-analysis-of-workers-earnings-on-amazon-mechanical-turk.pdf
   page_count: 12 
   id: data-driven-analysis-of-workers-earnings-on-amazon-mechanical-turk
   abstract: A growing number of people are working as part of on-line crowd work. Crowd work is often thought to be low wage work. However, we know little about the wage distribution in practice and what causes low/high earnings in this setting. We recorded 2,676 workers performing 3.8 million tasks on Amazon Mechanical Turk. Our task-level analysis revealed that workers earned a median hourly wage of only ~$2/h, and only 4% earned more than $7.25/h. While the average requester pays more than $11/h, lower-paying requesters post much more work. Our wage calculations are influenced by how unpaid work is accounted for, e.g., time spent searching for tasks, working on tasks that are rejected, and working on tasks that are ultimately not submitted. We further explore the characteristics of tasks and working patterns that yield higher hourly wages. Our analysis informs platform design and worker tools to create a more positive future for crowd work.
   bibtex: |
      @inproceedings{Hara-et-al:2018:CHI,
       author = {Kotaro Hara and Abi Adams and Kristy Milland and Saiph Savage and Chris Callison-Burch and Jeffrey P. Bigham},
       title = {A Data-Driven Analysis of Workers’ Earnings on Amazon Mechanical Turk},
       booktitle = {CHI 2018},
       month = {April},
       year = {2018},
       address = {Montreal, QC, Canada},
       url = {http://www.cis.upenn.edu/~ccb/publications/data-driven-analysis-of-workers-earnings-on-amazon-mechanical-turk.pdf}
       } 
-
   title: ChatEval&colon; A Tool for the Systematic Evaluation of Chatbots
   authors: Joao Sedoc*, Daphne Ippolito*, Arun Kirubarajan, Jai Thirani, Lyle Ungar, and Chris Callison-Burch
   venue: Workshop on Intelligent Interactive Systems and Language Generation
   type: workshop
   year: 2018
   url: https://www.cis.upenn.edu/~ccb/publications/chateval.pdf
   page_count: 4 
   id: chateval
   abstract: Open-domain dialog systems are difficult to evaluate. The current best practice for analyzing and comparing these dialog systems is the use of human judgments. However, the lack of standardization in evaluation procedures, and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments. We introduce a unified framework for human evaluation of chatbots that augments existing chatbot tools, and provides a web-based hub for researchers to share and compare their dialog systems. Researchers can submit their trained models to the ChatEval web interface and obtain comparisons with baselines and prior work. The evaluation code is open-source to ensure evaluation is performed in a standardized and transparent way. In addition, we introduce open-source baseline models and evaluation datasets. ChatEval can be found at https://chateval.org/
   bibtex: |
      @inproceedings{Sedoc:2018:ChatEval,
       author = {Joao Sedoc and Daphne Ippolito and Arun Kirubarajan and Jai Thirani and Lyle Ungar and Chris Callison-Burch},
       title = {ChatEval&colon; A Tool for the Systematic Evaluation of Chatbots},
       booktitle = {Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation},
       year = {2018},
       address = {Tilburg, The Netherlands},
       url = {http://www.cis.upenn.edu/~ccb/publications/chateval.pdf}
       } 
-
   title: Learning Translations via Matrix Completion
   authors: Derry Wijaya, Brendan Callahan, John Hewitt, Jie Gao, Xiao Ling, Marianna Apidianaki and Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2017
   url: https://www.cis.upenn.edu/~ccb/publications/learning-translations-via-matrix-completion.pdf
   page_count: 12 
   id: learning-translations-via-matrix-completion
   abstract: Bilingual Lexicon Induction is the task of learning word translations without bilingual parallel corpora. We model this task as a matrix completion problem, and present an effective and extendable framework for completing the matrix. This method harnesses diverse bilingual and monolingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages.
   bibtex: |
      @inproceedings{Wijaya-et-al:2017:EMNLP,
       author = {Derry Wijaya and Brendan Callahan and John Hewitt and Jie Gao and Xiao Ling and Marianna Apidianaki and Chris Callison-Burch},
       title = {Learning Translations via Matrix Completion},
       booktitle = {Conference on Empirical Methods in Natural Language Processing},
       month = {September},
       year = {2017},
       address = {Copenhagen, Denmark},
       url = {http://www.cis.upenn.edu/~ccb/publications/learning-translations-via-matrix-completion.pdf}
       } 
-
   title: KnowYourNyms? A Game of Semantic Relationships
   authors: Ross Mechanic, Dean Fulgoni, Hannah Cutler, Sneha Rajana, Zheyuan Liu, Bradley Jackson, Anne Cocos, Chris Callison-Burch and Marianna Apidianaki
   venue: EMNLP
   type: demo
   year: 2017
   url: https://www.cis.upenn.edu/~ccb/publications/know-your-nyms.pdf
   code: https://github.com/rossmechanic/know_your_nyms/
   website: https://www.know-your-nyms.com
   page_count: 6 
   id: know-your-nyms
   abstract: Semantic relation knowledge is crucial for natural language understanding. We introduce KnowYourNyms?, a web-based game for learning semantic relations. While providing users with an engaging experience, the application collects large amounts of data that can be used to improve semantic relation classifiers. The data also broadly informs us of how people perceive the relationships between words, providing useful insights for research in psychology and linguistics.
   bibtex: |
      @inproceedings{Mechanic-et-al:2017:EMNLP,
       author = {Ross Mechanic and Dean Fulgoni and Hannah Cutler and Sneha Rajana and Zheyuan Liu and Bradley Jackson and Anne Cocos and Chris Callison-Burch and Marianna Apidianaki},
       title = {KnowYourNyms? A Game of Semantic Relationships},
       booktitle = {Conference on Empirical Methods in Natural Language Processing},
       month = {September},
       year = {2017},
       address = {Copenhagen, Denmark},
       url = {http://www.cis.upenn.edu/~ccb/publications/know-your-nyms.pdf}
       } 
-
   title: Constructing an Alias List for Named Entities During an Event
   authors: Anietie Andy, Mark Dredze, Mugizi Rwebangira, and Chris Callison-Burch
   venue: Workshop on Noisy User-generated Text 
   type: workshop
   year: 2017
   url: https://www.cis.upenn.edu/~ccb/publications/constructing-an-alias-list.pdf
   page_count: 5 
   id: constructing-an-alias-list
   abstract: In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent realtime knowledge related to an event, it is important to identify the named entities and their corresponding aliases related to the event. The problem of identifying aliases of named entities that spike has remained unexplored. In this paper, we introduce an algorithm, EntitySpike, that identifies entities that spike in popularity in tweets from a given time period, and constructs an alias list for these spiked entities. EntitySpike uses a temporal heuristic to identify named entities with similar context that occur in the same time period (within minutes) during an event. Each entity is encoded as a vector using this temporal heuristic.We show how these entityvectors can be used to create a named entity alias list. We evaluated our algorithm on a dataset of temporally ordered tweets from a single event, the 2013 Grammy Awards show. We carried out various experiments on tweets that were published in the same time period and show that our algorithm identifies most entity name aliases and outperforms a competitive baseline.
   bibtex: |
    @inproceedings{andy2017constructing,
     title={Constructing an Alias List for Named Entities during an Event},
     author={Andy, Anietie and Dredze, Mark and Rwebangira, Mugizi and Callison-Burch, Chris},
     booktitle={Proceedings of the 3rd Workshop on Noisy User-generated Text},
     pages={40--44},
     year={2017}
    }
-
   title: Systematically Adapting Machine Translation for Grammatical Error Correction
   authors: Courtney Napoles and Chris Callison-Burch
   venue: 12th Workshop on Innovative Use of NLP for Building Educational Applications (BEA12)
   type: workshop
   year: 2017
   url: https://www.cis.upenn.edu/~ccb/publications/adapting-machine-translation-for-grammatical-error-correction.pdf
   page_count: 12 
   id: adapting-machine-translation-for-grammatical-error-correction
   abstract: In this work we adapt machine translation (MT) to grammatical error correction, identifying how components of the statistical MT pipeline can be modified for this task and analyzing how each modification impacts system performance. We evaluate the contribution of each of these components with standard evaluation metrics and automatically characterize the morphological and lexical transformations made in system output. Our model rivals the current state of the art using a fraction of the training data.
   bibtex: |
    @InProceedings{napoles-callisonburch:2017:BEA,
      author    = {Napoles, Courtney  and  Callison-Burch, Chris},
      title     = {Systematically Adapting Machine Translation for Grammatical Error Correction},
      booktitle = {Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications},
      month     = {September},
      year      = {2017},
      address   = {Copenhagen, Denmark},
      publisher = {Association for Computational Linguistics},
      pages     = {345--356},
      url       = {http://www.aclweb.org/anthology/W17-5039}
    }
-
   title: Mapping the Paraphrase Database to WordNet
   authors: Anne Cocos, Marianna Apidianaki and Chris Callison-Burch
   venue: STARSEM
   type: conference
   year: 2017
   url: https://www.cis.upenn.edu/~ccb/publications/mapping-ppdb-to-wordnet.pdf
   page_count: 
   id: mapping-ppdb-to-wordnet
   abstract: WordNet has facilitated important research in natural language processing but its usefulness is somewhat limited by its relatively small coverage. The Paraphrase Database (PPDB) covers 650 times more words, but lacks the semantic structure of WordNet that would make it more directly useful for downstream tasks. We present a method for mapping words from PPDB to WordNet synsets with 89% accuracy. The mapping also lays important groundwork for incorporating WordNet's  relations into PPDB so as to increase its utility for semantic  reasoning in applications. 
   bibtex: |
      @inproceedings{Cocos-et-al:2017:STARSEM,
       author = {Anne Cocos and Marianna Apidianaki and Chris Callison-Burch},
       title = {Mapping the Paraphrase Database to WordNet},
       booktitle = {*SEM 2017: The Sixth Joint Conference on Lexical and Computational Semantics},
       month = {August},
       year = {2017},
       address = {Vancouver, Canada},
       url = {http://www.cis.upenn.edu/~ccb/publications/mapping-ppdb-to-wordnet.pdf}
       } 
-
   title: Learning Antonyms with Paraphrases and a Morphology-aware Neural Network
   authors: Sneha Rajana, Chris Callison-Burch, Marianna Apidianaki and Vered Shwartz
   venue: STARSEM
   type: conference
   year: 2017
   url: https://www.cis.upenn.edu/~ccb/publications/learning-antonyms.pdf
   page_count: 10
   id: learning-antonyms
   abstract: Recognizing antonymy is a key task for improving the performance of NLP systems. In this paper, we propose a novel method for deriving antonym pairs from paraphrase pairs containing negation markers. We further integrate morphological features indicative of antonymy into a path-based relation detection algorithm. Our novel neural network model, AntNET, outperforms state-of-the-art models in distinguishing antonyms from other semantic relations and is capable of efficiently handling multi-word expressions.
   bibtex: |
      @inproceedings{Rajana-et-al:2017:STARSEM,
       author = {Sneha Rajana and Chris Callison-Burch and Marianna Apidianaki and Vered Shwartz},
       title = {Learning Antonyms with Paraphrases and a Morphology-aware Neural Network},
       booktitle = {*SEM 2017: The Sixth Joint Conference on Lexical and Computational Semantics},
       month = {August},
       year = {2017},
       address = {Vancouver, Canada},
       url = {http://www.cis.upenn.edu/~ccb/publications/learning-antonyms.pdf}
       } 
-
   title: Word Sense Filtering Improves Embedding-Based Lexical Substitution
   authors: Anne Cocos, Marianna Apidianaki and Chris Callison-Burch
   venue: Workshop on Sense, Concept and Entity Representations and their Applications
   type: workshop
   award: Best Paper Award
   year: 2017
   url: https://www.cis.upenn.edu/~ccb/publications/word-sense-filtering-improves-lexical-substitution.pdf
   page_count: 9
   id: word-sense-filtering-improves-lexical-substitution
   abstract: The role of word sense disambiguation in lexical substitution has been questioned due to the high performance of vector space models which propose good substitutes without explicitly accounting for sense. We show that a filtering mechanism based on a sense inventory optimized for substitutability can improve the results of these models. Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution annotations in a development set. The results show that lexical substitution can still benefit from senses which can improve the output of vector space paraphrase ranking models. 
   bibtex: |
      @inproceedings{Cocos-Apidianaki-Callison-Burch:2017:SENSE-WS,
        author    = {Anne Cocos and Marianna Apidianaki  and  Chris Callison-Burch},
        title     = {Word Sense Filtering Improves Embedding-Based Lexical Substitution},
        booktitle = {Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications},
        month     = {April},
        year      = {2017},
        address   = {Valencia, Spain},
        publisher = {Association for Computational Linguistics},
        pages     = {110--119},
        url       = {http://www.aclweb.org/anthology/E17-2016}
      }
-
   title: The Language of Place&colon; Semantic Value from Geospatial Context
   authors: Ann Cocos and Chris Callison-Burch
   venue: EACL
   type: conference
   year: 2017
   url: https://www.cis.upenn.edu/~ccb/publications/language-of-place.pdf
   page_count: 5
   id: language-of-place
   abstract: There is a relationship between what we say and where we say it. Word embeddings are usually trained assuming that semantically-similar words occur within the same textual contexts. We investigate the extent to which semantically-similar words occur within the same geospatial contexts. We enrich a corpus of geolocated Twitter posts with physical data derived from Google Places and OpenStreetMap, and train word embeddings using the resulting geospatial contexts. Intrinsic evaluation of the resulting vectors shows that geographic context alone does provide useful information about semantic relatedness.
   bibtex: |
    @InProceedings{cocos-callisonburch:2017:EACLshort,
      author    = {Cocos, Anne  and  Callison-Burch, Chris},
      title     = {The Language of Place: Semantic Value from Geospatial Context},
      booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
      month     = {April},
      year      = {2017},
      address   = {Valencia, Spain},
      publisher = {Association for Computational Linguistics},
      pages     = {99--104},
      url       = {http://www.aclweb.org/anthology/E17-2016}
    }
-
   title: Crowd Control&colon; Effectively Utilizing Unscreened Crowd Workers for Biomedical Data Annotation
   authors: Anne Cocos, Ting Qiana, Chris Callison-Burch, and Aaron J. Masino
   venue: Journal of Biomedical Informatics
   type: journal
   year: 2017
   url: http://www.sciencedirect.com/science/article/pii/S1532046417300746
   page_count: 22
   id: optimizing-machine-translation-for-text-simplifciation
   abstract: Annotating unstructured texts in Electronic Health Records data is usually a necessary step for conducting machine learning research on such datasets. Manual annotation by domain experts provides data of the best quality, but has become increasingly impractical given the rapid increase in the volume of EHR data. In this article, we examine the effectiveness of crowdsourcing with unscreened online workers as an alternative for transforming unstructured texts in EHRs into annotated data that are directly usable in supervised learning models. We find the crowdsourced annotation data to be just as effective as expert data in training a sentence classification model to detect the mentioning of abnormal ear anatomy in radiology reports of audiology. Furthermore, we have discovered that enabling workers to self-report a confidence level associated with each annotation can help researchers pinpoint less-accurate annotations requiring expert scrutiny. Our findings suggest that even crowd workers without specific domain knowledge can contribute effectively to the task of annotating unstructured EHR datasets.
   bibtex: |
      @article{Cocos-EtAl:2017:Biomedical-Informatics,
         author = {Anne Cocos and Ting Qiana and Chris Callison-Burch and Aaron Masino},
         title = {Crowd Control: Effectively Utilizing Unscreened Crowd Workers for Biomedical Data Annotation},
         journal = {Journal of Biomedical Informatics},
         volume = {},
         number = {},
         year = {2017},
         url = {http://www.sciencedirect.com/science/article/pii/S1532046417300746}
       }
-
   title: The Gun Violence Database
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: Bloomberg Data for Good Exchange
   type: workshop
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/gvdb-d4gx.pdf
   page_count: 6
   id: gvdb-d4gx
   abstract: We describe the Gun Violence Database (GVDB), a large and growing database of gun violence incidents in the United States. The GVDB is built from the detailed information found in local news reports about gun violence, and is constructed via a large-scale crowdsourced annotation effort through our web site, http://gun-violence.org/. We argue that centralized and publicly available data about gun violence can facilitate scientific, fact-based discussion about a topic that is often dominated by politics and emotion. We describe our efforts to automate the construction of the database using state-of-the-art natural language processing (NLP) technologies, eventually enabling a fully-automated, highly-scalable resource for research on this important public health problem.
-
   title: The Gun Violence Database&colon; A new task and data set for NLP
   authors: Ellie Pavlick, Heng Ji, Xiaoman Pan and Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/gun-violence-database.pdf
   page_count: 6
   id: gun-violence-database
   abstract: We argue that NLP researchers are especially well-positioned to contribute to the national discussion about gun violence. Reasoning about the causes and outcomes of gun violence is typically dominated by politics and emotion, and data-driven research on the topic is stymied by a shortage of data and a lack of federal funding. However, data abounds in the form of unstructured text from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem.
   bibtex: |
      @inproceedings{Pavlick-EtAl:2016:EMNLP,
       author = {Ellie Pavlick and Heng Ji and Xiaoman Pan and Chris Callison-Burch},
       title = {The Gun Violence Database: A new task and data set for {NLP}},
       booktitle = {Proceedings of The 2016 Conference on Empirical Methods on Natural Language Processing (EMNLP)},
       month = {November},
       year = {2016},
       address = {Austin, TX},
       url = {http://www.cis.upenn.edu/~ccb/publications/gun-violence-database.pdf}
       } 
-
   title: Tense Manages to Predict Implicative Behavior in Verbs
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/tense-predicts-implicative-verbs.pdf
   page_count: 5
   id: tense-predicts-implicative-verbs
   abstract: Implicative verbs (e.g. manage) entail their compliment clauses, while non-implicative verbs (e.g. want) do not. For example, while managing to solve the problem entails solving the problem, no such inference follows from wanting to solve the problem. Differentiating between implicative and non-implicative verbs is therefore an essential component of natural language understanding, relevant to applications such as textual entailment and summarization. We present a simple method for predicting implicativeness which exploits known constraints on the tense of implicative verbs and their compliments. We show that this yields an effective, data-driven way of capturing this nuanced property in verbs.
   bibtex: |
      @inproceedings{Pavlick-Callison-Burch:2016:EMNLP,
       author = {Ellie Pavlick and Chris Callison-Burch},
       title = {Tense Manages to Predict Implicative Behavior in Verbs},
       booktitle = {Proceedings of The 2016 Conference on Empirical Methods on Natural Language Processing (EMNLP)},
       month = {November},
       year = {2016},
       address = {Austin, TX},
       url = {http://www.cis.upenn.edu/~ccb/publications/tense-predicts-implicative-verbs.pdf}
       } 
-
   title: So-Called Non-Subsective Adjectives
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: STARSEM
   type: conference
   award: Best Paper Award
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/non-subsective-adjectives.pdf
   page_count: 6
   id: non-subsective-adjectives
   abstract: The interpretation of adjective-noun pairs plays a crucial role in tasks such as recognizing textual entailment. Formal semantics often places adjectives into a taxonomy which should dictate adjectives’ entailment behavior when placed in adjective-noun compounds. However, we show experimentally that the behavior of subsective adjectives (e.g. red) versus non-subsective adjectives (e.g. fake) is not as cut and dry as often assumed. For example, inferences are not always symmetric&colon; while ID is generally considered to be mutually exclusive with fake ID, fake ID is considered to entail ID. We discuss the implications of these findings for automated natural language understanding.
   bibtex: |
      @inproceedings{Pavlick-Callison-Burch:2016:ACL,
       author = {Ellie Pavlick and Chris Callison-Burch},
       title = {So-Called Non-Subsective Adjectives},
       booktitle = {*SEM 2016: The Fifth Joint Conference on Lexical and Computational Semantics},
       month = {August},
       year = {2016},
       address = {Berlin, Germany},
       url = {http://www.cis.upenn.edu/~ccb/publications/non-subsective-adjectives.pdf}
       } 
-
   title: Most babies are little and most problems are huge&colon; Compositional Entailment in Adjective-Nouns
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/compositional-entailment-in-adjective-nouns.pdf
   data: http://www.seas.upenn.edu/~nlp/resources/AN-composition.tgz
   page_count: 11
   id: compositional-entailment-in-adjective-nouns
   abstract: We examine adjective-noun (AN) composition in the task of recognizing textual entailment (RTE). We analyze behavior of ANs in large corpora and show that, despite conventional wisdom, adjectives do not always restrict the denotation of the nouns they modify. We use natural logic to characterize the variety of entailment relations that can result from AN composition. Predicting these relations depends on context and on common-sense knowledge, making AN composition especially challenging for current RTE systems. We demonstrate the inability of current state-of-the-art systems to handle AN composition in a simplified RTE task which involves the insertion of only a single word.
   bibtex: |
      @inproceedings{Pavlick-Callison-Burch:2016:ACL,
       author = {Ellie Pavlick and Chris Callison-Burch},
       title = {Most babies are little and most problems are huge&colon; Compositional Entailment in Adjective-Nouns},
       booktitle = {The 54th Annual Meeting of the Association for Computational Linguistics (ACL)},
       month = {August},
       year = {2016},
       address = {Berlin, Germany},
       url = {http://www.cis.upenn.edu/~ccb/publications/compositional-entailment-in-adjective-nouns.pdf}
       } 
-
   title: Simple PPDB&colon; A Paraphrase Database for Simplification
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/simple-ppdb.pdf
   data: http://www.seas.upenn.edu/~nlp/resources/simple-ppdb.tgz
   page_count: 6
   highly_cited: 108
   id: simple-ppdb
   abstract: We release the Simple Paraphrase Database, a subset of of the Paraphrase Database (PPDB) adapted for the task of text simplification. We train a supervised model to associate simplification scores with each phrase pair, producing rankings competitive with state-of-the-art lexical simplification models. Our new simplification database contains 4.4 million paraphrase rules, making it the largest available resource for lexical simplification.
   bibtex: |
      @inproceedings{Pavlick-Callison-Burch:2016:ACL,
       author = {Ellie Pavlick and Chris Callison-Burch},
       title = {Simple {PPDB}&colon; A Paraphrase Database for Simplification},
       booktitle = {The 54th Annual Meeting of the Association for Computational Linguistics (ACL)},
       month = {August},
       year = {2016},
       address = {Berlin, Germany},
       url = {http://www.cis.upenn.edu/~ccb/publications/simple-ppdb.pdf}
       } 
-
   title: Clustering Paraphrases by Word Sense
   authors: Anne Cocos and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/clustering-paraphrases-by-word-sense.pdf
   page_count: 10
   id: clustering-paraphrases-by-word-sense
   abstract: Automatically generated databases of English paraphrases have the drawback that they return a single list of paraphrases for an input word or phrase. This means that all senses of polysemous words are grouped together, unlike WordNet which partitions different senses into separate synsets. We present a new method for clustering paraphrases by word sense, and apply it to the Paraphrase Database (PPDB). We investigate the performance of hierarchical and spectral clustering algorithms, and systematically explore different ways of defining the similarity matrix that they use as input. Our method produces sense clusters that are qualitatively and quantitatively good, and that represent a substantial improvement to the PPDB resource.
   bibtex: |
      @inproceedings{Cocos-Callison-Burch:2016:NAACL,
       author = {Anne Cocos and Chris Callison-Burch},
       title = {Clustering Paraphrases by Word Sense},
       booktitle = {The 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)},
       month = {June},
       year = {2016},
       address = {San Diego, California},
       url = {http://www.cis.upenn.edu/~ccb/publications/clustering-paraphrases-by-word-sense.pdf}
       } 
-
   title: Sentential Paraphrasing as Black-Box Machine Translation
   authors: Courtney Napoles, Chris Callison-Burch, and Matt Post
   venue: NAACL
   type: conference
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/sentential-paraphrasing-demo-paper.pdf
   page_count: 5
   id: sentential-paraphrasing-demo-paper
   abstract: We present a simple, prepackaged solution to generating paraphrases of English sentences. We use the Paraphrase Database (PPDB) for monolingual sentence rewriting and provide machine translation language packs&colon; prepackaged, tuned models that can be downloaded and used to generate paraphrases on a standard Unix environment. The language packs can be treated as a black box or customized to specific tasks. In this demonstration, we will explain how to use the included interactive web-based tool to generate sentential paraphrases.
   bibtex: |
      @inproceedings{Napoles-et-al:2016:NAACL-demos,
       author = {Courtney Napoles and Chris Callison-Burch and Matt Post},
       title = {Sentential Paraphrasing as Black-Box Machine Translation},
       booktitle = {The 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)},
       month = {June},
       year = {2016},
       address = {San Diego, California},
       url = {http://www.cis.upenn.edu/~ccb/publications/sentential-paraphrasing-demo-paper.pdf}
       } 
-
   title: Optimizing Statistical Machine Translation for Text Simplification
   authors: Wei Xu, Courtney Napoles, Ellie Pavlick, Jim Chen, and Chris Callison-Burch
   venue: TACL
   type: journal
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/optimizing-machine-translation-for-text-simplifciation.pdf
   page_count: 15
   highly_cited: 520
   id: optimizing-machine-translation-for-text-simplifciation
   abstract: Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and  evaluating simplification systems, which will facilitate iterative development for this task.
   bibtex: |
      @article{Xu-EtAl:2016:TACL,
         author = {Wei Xu and Courtney Napoles and Ellie Pavlick and Quanze Chen and Chris Callison-Burch},
         title = {Optimizing Statistical Machine Translation for Text Simplification},
         journal = {Transactions of the Association for Computational Linguistics},
         volume = {4},
         year = {2016},
         url = {http://www.cis.upenn.edu/~ccb/publications/optimizing-machine-translation-for-text-simplifciation.pdf},
         pages = {401--415}
       }
-
   title: A Comprehensive Analysis of Bilingual Lexicon Induction
   authors: Ann Irvine and Chris Callison-Burch
   venue: Computational Linguistics
   type: journal
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/discriminative-bilingual-lexicon-induction.pdf
   page_count: 38
   id: discriminative-bilingual-lexicon-induction
   abstract: Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages.  In this paper we present the most comprehensive analysis of bilingual lexicon induction to date.  We present experiments on a wide range of languages and data sizes.  We examine translation into English from 25 foreign languages -- Albanian, Azeri, Bengali, Bosnian, Bulgarian, Cebuano, Gujarati, Hindi, Hungarian, Indonesian, Latvian, Nepali, Romanian, Serbian, Slovak, Somali, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Uzbek, Vietnamese and Welsh.  We analyze the behavior of bilingual lexicon induction on low frequency words, rather than testing solely on high frequency words, as previous research has done.  Low frequency words are more relevant to statistical machine translation, where systems typically lack translations of rare words that fall outside of their training data.  We systematically explore a wide range of features and phenomena that affect the quality of the translations discovered by bilingual lexicon induction. We give illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity.  We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora.  Additionally, we introduce a novel discriminative approach to bilingual lexicon induction.  Our discriminative model is capable of combining a wide variety of features, which individually provide only weak indications of translation equivalence.  When feature weights are discriminatively set, these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion (e.g. using minimum reciprocal rank).  We also directly compare our model's performance against a sophisticated generative approach, the matching canonical correlation analysis (MCCA) algorithm used by Haghighi et al (2008).  Our algorithm achieves an accuracy of 42% versus MCCA's 15%.
-
   title: The Shield of Heroic Memories (mp3)
   authors: Chris Callison-Burch
   venue: The Adventure Zone podcast
   type: unpublished
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/shield-of-heroic-memories.mp3
   id: shield-of-heroic-memories
   abstract: I designed an item for The Adventure Zone, a comedy podcast about three brothers playing D&D with their dad.  The McElroy brothers were incredibly enthusiastic about my submission.  I want all of my paper reviews to say what they said, "That's already radical and then my boy Chris Callison-Burch kicked it up a notch. It's brilliant."       
-
   title: Adding Semantics to Data-Driven Paraphrasing
   authors: Ellie Pavlick, Johan Bos, Malvina Nissim, Charley Beller, Benjamin Van Durme, and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf
   data: http://cs.brown.edu/people/epavlick/resources/natlog-labeled-rte-pairs.gz
   page_count: 10
   id: adding-semantics-to-data-driven-paraphrasing
   figures:
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-1.jpg
         label: Figure 1
         caption: An example sentence pair for the RTE task. In order for a system to conclude that the premise (top) does not entail the hypothesis (bottom), it should recognize that sparked implies caused but that in Denmark precludes in Jordan. These phrase-level entailment relationships are modeled by natural logic.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-1.jpg
         label: Table 1
         caption: Examples of different types of entailment relations appearing in PPDB.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-2.jpg
         label: Figure 2
         caption: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent).
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-3.jpg
         label: Figure 3
         caption: Summary of features extracted for each phrase pair ⟨p1,p2⟩. Full descriptions of the features used are given in the supplementary material.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-2.jpg
         label: Table 2
         caption: Column 1 gives the semantics of each label under MacCartney’s Natural Logic. Column 2 gives the notation we use throughout the remainder of this paper. Column 3 gives the description that was shown to Turkers.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-3.jpg
         label: Table 3
         caption: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-4.jpg
         label: Table 4
         caption: Top paths associated with the ¬ class.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-5.jpg
         label: Table 5
         caption: F1 measure (×100) achieved by entailment classifier using 10-fold cross validation on the training data.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-4.jpg
         label: Figure 4
         caption: Confusion matrices for classifier trained using only monolingual features (distributional and path) versus bilingual features (paraphrase and translation). True labels are shown along rows, predicted along columns. The matrix is normalized along rows, so that the predictions for each (true) class sum to 100%.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-7.jpg
         label: Table 7
         caption: F1 measure (×100) achieved by entailment classifier on the held out phrase pairs from the sentences in SICK test.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-6.jpg
         label: Table 6
         caption: Example misclassifications from some of the most frequent and most interesting error categories.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-5.jpg
         label: Figure 5
         caption: ENTAILMENT
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-6.jpg
         label: Figure 6
         caption: CONTRADICTION
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-7.jpg
         label: Figure 7
         caption: NEUTRAL
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-8.jpg
         label: Figure 8
         caption: F1 measures achieved by Nutcracker on SICK test data when using various KBs. Baselines are in gray, this work in blue, human references in gold. PPDB-XL refers to a run in which every pair which appears in PPDB is assumed to be equivalent. PPDB-H refers to a run in which manual labels were used to generate axioms. PPDB+ refers to runs in which the automatic classifications were used to generate axioms. In some cases, better proof coverage causes NC to find incorrect proofs, illustrated by the decreased performance on CONTRADICTION when using PPDB-H. For example, using PPDB-H, NC finds an inconsistency for the pair Someone is not playing piano./A person is playing a keyboard. Using the PPDB+, in which piano/keyboard is falsely classified as #, NC fails to find a proof and so correctly guesses NEUTRAL.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-8.jpg
         label: Table 8
         caption: Nutcracker’s overall system accuracy and proof coverage when using different sources of axioms. Coverage is measured as the percent of sentence pairs for which NC’s theorem prover or model builder is able to find a complete logical proof of either entailment or contradiction. When NC fails to find either type of proof, it guesses the most frequent class, NEUTRAL. NC alone uses no axioms. PPDB+ refers to the axioms generated automatically using the classifier described in this paper. PPDB-H refers axioms generated using the human labels on which the classifier was trained.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-9.jpg
         label: Table 9
         caption: Examples of T/H pairs for which the system’s prediction differed when using PPDB+ vs. WN.
   abstract: We add an interpretable semantics to the paraphrase database (PPDB). To date, the relationship between the phrase pairs in the database has been weakly defined as approximately equivalent. We show that in fact these pairs represent a variety of relations, including directed entailment (little girl/girl) and exclusion (nobody/someone). We automatically assign semantic entailment relations to entries in PPDB using features derived from past work on discovering inference rules from text and semantic taxonomy induction. We demonstrate that our model assigns these entailment relations with high accuracy. In a downstream RTE task, our labels rival relations from WordNet and improve the coverage of a proof-based RTE system by 17%. 
   bibtex: |
      @inproceedings{Pavlick-EtAl:2015:ACL,
       author = {Ellie Pavlick and Johan Bos and Malvina Nissim and Charley Beller and Benjamin Van Durme and Chris Callison-Burch},
       title = {Adding Semantics to Data-Driven Paraphrasing},
       booktitle = {The 53rd Annual Meeting of the Association for Computational
       Linguistics (ACL 2015)},
       month = {July},
       year = {2015},
       address = {Beijing, China},
       url = {http://www.cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf}
       }       
-
   title: PPDB 2.0&colon; Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification
   authors: Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevich, Ben Van Durme, Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/ppdb-reranking.pdf
   data: http://www.seas.upenn.edu/~nlp/resources/ppdb-2.0-human-labels.tgz
   website: http://paraphrase.org/
   page_count: 6
   id: ppdb-reranking
   highly_cited: 363
   abstract: We present a new release of the Paraphrase Database. PPDB 2.0 includes a discriminatively re-ranked set of paraphrases that achieve a higher correlation with human judgments than PPDB 1.0’s heuristic rankings. Each paraphrase pair in the database now also includes fine-grained entailment relations, word embedding similarities, and style annotations.
   bibtex: |
    @InProceedings{PavlickEtAl-2015:ACL:Semantics,
      author =  {Ellie Pavlick and Pushpendre Rastogi and Juri Ganitkevich and Ben Van Durme, Chris Callison-Burch},
      title =   {{PPDB} 2.0&colon; Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification}
      booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)},
      month     = {July},
      year      = {2015},
      address   = {Beijing, China},
      publisher = {Association for Computational Linguistics},
    }
-
   title: Domain-Specific Paraphrase Extraction
   authors: Ellie Pavlick, Juri Ganitkevich, Tsz Ping Chan, Xuchen Yao, Ben Van Durme, Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/domain-specific-paraphrases.pdf
   page_count: 6
   id: domain-specific-paraphrases
   abstract: The validity of applying paraphrase rules depends on the domain of the text that they are being applied to. We develop a novel method for extracting domain-specific paraphrases. We adapt the bilingual pivoting paraphrase method to bias the training data to be more like our target domain of biology. Our best model results in higher precision while retaining complete recall, giving a 10% relative improvement in AUC.
   bibtex: |
    @InProceedings{PavlickEtAl-2015:ACL:Domain,
      author =  {Ellie Pavlick and Juri Ganitkevich and Tsz Ping Chan and Xuchen Yao and Ben Van Durme, Chris Callison-Burch},
      title =   {Domain-Specific Paraphrase Extraction},
      booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)},
      month     = {July},
      year      = {2015},
      address   = {Beijing, China},
      publisher = {Association for Computational Linguistics},
    }
-
   title: FrameNet+&colon; Fast Paraphrastic Tripling of FrameNet
   authors: Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi, Chris Callison-Burch, Mark Drezde, Ben Van Durme
   venue: ACL
   type: conference
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/FrameNetPlus.pdf
   data: http://www.seas.upenn.edu/~nlp/resources/FN+.zip
   page_count: 6
   id: FrameNetPlus
   abstract: We increase the lexical coverage of FrameNet through automatic paraphrasing. We use crowdsourcing to manually filter out bad paraphrases in order to ensure a high-precision resource. Our expanded FrameNet contains an additional 22K lexical units, a 3-fold increase over the current FrameNet, and achieves 40% better coverage when evaluated in a practical setting on New York Times data.
   bibtex: |
    @InProceedings{PavlickEtAl-2015:ACL:FNPlus,
      author =  {Ellie Pavlick and Travis Wolfe and Pushpendre Rastogi and Chris Callison-Burch and Mark Drezde and Benjamin Van Durme},
      title =   {FrameNet+: Fast Paraphrastic Tripling of FrameNet},
      booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)},
      month     = {July},
      year      = {2015},
      address   = {Beijing, China},
      publisher = {Association for Computational Linguistics},
    }
-
   title: Problems in Current Text Simplification Research&colon; New Data Can Help
   authors: Wei Xu, Chris Callison-Burch, and Courtney Napoles
   venue: TACL
   type: journal
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf
   page_count: 16
   highly_cited: 420
   id: new-data-for-text-simplification
   figures:
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-1.jpg
         label: Table 1
         caption: Example sentence pairs (NORM-SIMP) aligned between English Wikipedia and Simple English Wikipedia. The breakdown in percentages is obtained through manual examination of 200 randomly sampled sentence pairs in the Parallel Wikipedia Simplification (PWKP) corpus.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-2.jpg
         label: Table 2
         caption: The vocabulary size of the Parallel Wikipedia Simplification (PWKP) corpus and the vocabulary difference between its normal and simple sides (as a 2×2 matrix). Only words consisting of the 26 English letters are counted.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-3.jpg
         label: Table 3
         caption: Example of sentences written at multiple levels of text complexity from the Newsela data set. The Lexile readability score and grade level apply to the whole article rather than individual sentences, so the same sentences may receive different scores, e.g. the above sentences for the 6th and 7th grades. The bold font highlights the parts of sentence that are different from the adjacent version(s).
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-figure-1.jpg
         label: Figure 1
         caption: Manual classification of aligned sentence pairs from the Newsela corpus. We categorize randomly sampled 50 sentence pairs drawn from the Original-Simp2 and 50 sentences from the Original-Simp4.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-4.jpg
         label: Table 4
         caption: Basic statistics of the Newsela Simplification corpus vs. the Parallel Wikipedia Simplification (PWKP) corpus. The Newsela corpus consists of 1130 articles with original and 4 simplified versions each. Simp-1 is of the least simplified level, while Simp-4 is the most simplified. The numbers marked by * are slightly different from previously reported, because of the use of different tokenizers.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-5.jpg
         label: Table 5
         caption: This table shows the vocabulary changes between different levels of simplification in the Newsela corpus (as a 5×5 matrix). Each cell shows the number of unique word types that appear in the corpus listed in the column but do not appear in the corpus listed in the row. We also list the average frequency of those vocabulary items. For example, in the cell marked *, the Simp-4 version contains 583 unique words that do not appear in the Original version. By comparing the cells marked **, we see about half of the words (19,197 out of 39,046) in the Original version are not in the Simp-4 version. Most of the vocabulary that is removed consists of low-frequency words (with an average frequency of 2.6 in the Original).
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-6.jpg
         label: Table 6
         caption: Top 50 tokens associated with the complex text, computed using the Monroe et al. (2008) method. Bold words are shared by the complex version of Newsela and the complex version of Wikipedia.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-7.jpg
         label: Table 7
         caption: Top 50 tokens associated with the simplified text.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-8.jpg
         label: Table 8
         caption: Frequency of example words from Table 6. These complex words are reduced at a much greater rate in the simplified Newsela than they are in the Simple English Wikipedia. A smaller odds ratio indicates greater reduction.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-9.jpg
         label: Table 9
         caption: Top 30 syntax patterns associated with the complex text (left) and simplified text (right). Bold patterns are the top patterns shared by Newsela and Wikipedia.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-figure-2.jpg
         label: Figure 2
         caption: Distribution of document-level compression ratio, displayed as a histogram smoothed by kernel density estimation. The Newsela corpus is more normally distributed, suggesting more consistent quality.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-figure-3.jpg
         label: Figure 3
         caption: A radar chart that visualizes the odds ratio (radius axis) of discourse connectives in simple side vs. complex side. An odds ratio larger than 1 indicates the word is more likely to occur in the simplified text than in the complex text, and vice versa. Simple cue words (in the shaded region), except “hence”, are more likely to be added during Newsela’s simplification process than in Wikipedia’s. Complex conjunction connectives (in the unshaded region) are more likely to be retained in Wikipedia’s simplifications than in Newsela’s.
   abstract: Simple Wikipedia has dominated simplification research in the past 5 years. In this opinion paper, we argue that focusing on Wikipedia limits simplification research. We back up our arguments with corpus analysis and by highlighting statements that other researchers have made in the simplification literature. We introduce a new simplification dataset that is a significant improvement over Simple Wikipedia, and present a novel quantitative-comparative approach to study the quality of simplification data resources. 
   bibtex: |
      @article{Xu-EtAl:2015:TACL,
         author = {Wei Xu and Chris Callison-Burch and Courtney Napoles},
         title = {Problems in Current Text Simplification Research: New Data Can
       Help},
         journal = {Transactions of the Association for Computational Linguistics},
         volume = {3},
         year = {2015},
         url = {http://www.cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf},
         pages = {283--297}
       }
-
   title: Cost Optimization for Crowdsourcing Translation
   authors: Mingkun Gao, Wei Xu, and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf
   page_count: 9
   id: cost-optimization-for-crowdsourcing-translation
   figures:
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-figure-1.jpg
         label: Figure 1
         caption: Example bilingual features for two crowdsourced translations of an Urdu sentence. The numbers are alignment probabilities for each aligned word. The bilingual feature is the average of these probabilities, thus 0.240 for the good translation and 0.043 for the bad translation. Some words are not aligned if potential word pairs don’t exist in bilingual training corpus.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-table-1.jpg
         label: Table 1
         caption: The relationship between _ (the allowable deviation from the expected upper bound on BLEU score), the BLEU score for translations selected by models from partial sets and the average number of translation candidates set for each source sentence (# Trans).
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-figure-2.jpg
         label: Figure 2
         caption: A time-series plot of all of the translations produced by Turkers (identified by their WorkerID serial number). Turkers are sorted with the best translator at the top of the y-axis. Each tick represent a single translation and black means better than average quality.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-table-2.jpg
         label: Table 2
         caption: Pearson Correlations for calibration data in different proportion. The percentage column shows what proportion of the whole data set is used for calibration.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-figure-3.jpg
         label: Figure 3
         caption: Correlation between gold standard ranking and ranking computed using the first 20 sentences as calibration. Each bubble represents a worker. The radius of each bubble shows the relative volume of translations completed by the worker. The weighted correlation is 0.94.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-figure-4.jpg
         label: Figure 4
         caption: Correlation between gold standard ranking and our model’s ranking. The corresponding weighted correlation is 0.95.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-table-3.jpg
         label: Table 3
         caption: Correlation (ρ) and translation quality for the various features used by our model. Translation quality is computed by selecting best translations based on model-predicted ranking for workers (rank) and model-predicted scores for translations (score). Here we do not filter out bad workers when selecting the best translation.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-table-4.jpg
         label: Table 4
         caption: A comparison of the translation quality when we retain the top translators under different rankings. The rankings shown are random, the model’s ranking (using all features from Table 3) and the gold ranking. ∆ is the difference between the BLEU scores for the gold ranking and the model ranking. # Trans is the average number of translations needed for each source sentence.
   abstract: Crowdsourcing makes it possible to create translations at much lower cost than hiring professional translators. However, it is still expensive to obtain the millions of translations that are needed to train statistical machine translation systems. We propose two mechanisms to reduce the cost of crowdsourcing while maintaining high translation quality. First, we develop a method to reduce redundant translations. We train a linear model to evaluate the translation quality on a sentence-by-sentence basis, and fit a threshold between acceptable and unacceptable translations. Unlike past work, which always paid for a fixed number of translations for each source sentence and then chose the best from them, we can stop earlier and pay less when we receive a translation that is good enough. Second, we introduce a method to reduce the pool of translators by quickly identifying bad translators after they have translated only a few sentences. This also allows us to rank translators, so that we re-hire only good translators to reduce cost. 
   bibtex: |
      @inproceedings{Gao-EtAl:2015:NAACL,
       author = {Mingkun Gao and Wei Xu and Chris Callison-Burch},
       title = {Cost Optimization in Crowdsourcing Translation},
       booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2015)},
       month = {June},
       year = {2015},
       address = {Denver, Colorado},
       url = {http://www.cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf}
       }
-
   title: SemEval-2015 Task 1&colon; Paraphrase and Semantic Similarity in Twitter
   authors: Wei Xu, Chris Callison-Burch, and Bill Dolan
   venue: SemEval
   type: workshop
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/paraphrase-and-semantic-similarity-in-twitter.pdf
   page_count: 11
   highly_cited: 172
   id: paraphrase-and-semantic-similarity-in-twitter
   figures:
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-table-1.jpg
         label: Table 1
         caption: Representative examples from PIT-2015 Twitter Paraphrase Corpus
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-table-2.jpg
         label: Table 2
         caption: Statistics of PIT-2015 Twitter Paraphrase Corpus. Debatable cases are those received a medium-score from annotators. The percentage of paraphrases is lower in the test set because it was constructed without topic selection.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-figure-1.jpg
         label: Figure 1
         caption: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are non-paraphrases; 4,5 are paraphrases. Medium-scored cases (2 for crowdsourcing; 3 for expert annotation) are discarded in the system evaluation of the PI sub-task.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-figure-2.jpg
         label: Figure 2
         caption: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-figure-3.jpg
         label: Figure 3
         caption: Numbers of paraphrases collected by different methods. The annotation efficiency (3,4,5 are regarded as paraphrases) is significantly improved by the sentence filtering and Multi-Armed Bandits (MAB) based topic selection.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-figure-4.jpg
         label: Figure 4
         caption: PINC scores of paraphrases collected. The higher the PINC, the more significant the rewording. Our proposed annotation strategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-table-3.jpg
         label: Table 3
         caption: Evaluation results. The first column presents the rank of each team in the two tasks based on each team’s best system. The superscripts are the ranks of systems, ordered by F1 for Paraphrase Identification (PI) task and Pearson for Semantic Similarity (SS) task. ⇧ indicates unsupervised or semi-supervised system. In total, 19 teams participated in the PI task, of which 14 teams also participated in the SS task. Note that although the two sub-tasks share the same test set of 972 sentence pairs, the PI task ignores 134 debatable cases (received a medium-score from expert annotator) and uses only 838 pairs (663 paraphrases and 175 non-paraphrases) in evaluation, while SS task uses all 972 pairs. This causes that the F1-score in the PI task can be higher than the maximum F1-score in the SS task. Also note that the F1-scores of the baselines in the PI task are higher than reported in the Table 2 of (Xu et al., 2014), because the later reported maximum F1-scores on the PI task, ignoring the debatable cases.
   abstract: In this shared task, we present evaluations on two related tasks Paraphrase Identification (PI) and Semantic Textual Similarity (SS) systems for the Twitter data. Given a pair of sentences, participants are asked to produce a binary yes/no judgement or a graded score to measure their semantic equivalence. The task features a newly constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs. A total of 19 teams participated, submitting 36 runs to the PI task and 26 runs to the SS task. The evaluation shows encouraging results and open challenges for future research. The best systems scored a F1-measure of 0.674 for the PI task and a Pearson correlation of 0.619 for the SS task respectively, comparing to a strong baseline using logistic regression model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach >0.80 Pearson on well-formed text. This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together. We make all the data, baseline systems and evaluation scripts publicly available. 
   bibtex: |
      @inproceedings{Xu-EtAl:2015:semeval,
         author    = {Wei Xu and Chris Callison-Burch and William B. Dolan},
         title     = {{SemEval-2015 Task} 1: Paraphrase and Semantic Similarity in {Twitter} ({PIT})},
         booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval)},
         year      = {2015},
         url = 
       }
       
-
   title: Effectively Crowdsourcing Radiology Report Annotations
   authors: Anne Cocos, Aaron J. Masino, Ting Qian, Ellie Pavlick, and Chris Callison-Burch
   venue: Sixth International Workshop on Health Text Mining and Information Analysis
   type: workshop
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/crowdsourcing-radiology.pdf
   page_count: 6
   id: crowdsourcing-radiology
   abstract: Crowdsourcing platforms are a popular choice for researchers to gather text annotations quickly at scale. We investigate whether crowdsourced annotations are useful when the labeling task requires medical domain knowledge. Comparing a sentence classification model trained with expert-annotated sentences to the same model trained on crowd-labeled sentences, we find the crowdsourced training data to be just as effective as the manually produced dataset. We can improve the accuracy of the crowd-fueled model without collecting further labels by filtering out worker labels applied with low confidence.
   bibtex: |
    @InProceedings{CocosEtAl-2015:LOUHI:Radiology,
      author =  {Anne Cocos and Aaron J. Masino and Ting Qian and Ellie Pavlick and Chris Callison-Burch},
      title =   {Effectively Crowdsourcing Radiology Report Annotations}
      booktitle = {Sixth International Workshop on Health Text Mining and Information Analysis},
      month     = {November},
      year      = {2015},
      address   = {Lisbon, Portugal},
    }
-
   title: Automatically Scoring Freshman Writing&colon; A Preliminary Investigation
   authors: Courtney Napoles and Chris Callison-Burch
   venue: Workshop on Innovative Use of NLP for Building Educational Applications
   type: workshop
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/automatically-scoring-freshman-writing.pdf
   page_count: 10
   id: automatically-scoring-freshman-writing
   abstract: In this work, we explore applications of automatic essay scoring (AES) to a corpus of essays written by college freshmen and discuss the challenges we faced. While most AES systems evaluate highly constrained writing, we developed a system that handles open-ended, long-form writing. We present a novel corpus for this task, containing more than 3,000 essays and drafts written for a freshman writing course. We describe statistical analysis of the corpus and identify problems with automatically scoring this type of data. Finally, we demonstrate how to overcome grader bias by using a multi-task setup, and predict scores as well as human graders on a different dataset. Finally, we discuss how AES can help teachers assign more uniform grades.
   bibtex: |
    @InProceedings{napoles-callisonburch:2015:bea,
      author    = {Napoles, Courtney  and  Callison-Burch, Chris},
      title     = {Automatically Scoring Freshman Writing: A Preliminary Investigation},
      booktitle = {Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications},
      month     = {June},
      year      = {2015},
      address   = {Denver, Colorado},
      publisher = {Association for Computational Linguistics},
      pages     = {254--263}
    }

-
   title: Extracting Structured Information via Automatic + Human Computation
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: HCOMP
   type: workshop
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/gun-violence-db.pdf
   page_count: 2
   id: gun-violence-db
   abstract: We present a system for extracting structured information from unstructured text using a combination of information retrieval, natural language processing, machine learning, and crowdsourcing. We test our pipeline by building a structured database of gun violence incidents in the United States. The results of our pilot study demonstrate that the proposed methodology is a viable way of collecting large-scale, up-to-date data for public health, public policy, and social science research.
   bibtex: |
    @InProceedings{PavlickAndCallisonBurch-2015:HCOMP:GVDB,
      author =  {Ellie Pavlick and Chris Callison-Burch},
      title =   {Extracting Structured Information via Automatic + Human Computation}
      booktitle = {HCOMP},
      month     = {November},
      year      = {2015},
      address   = {San Diego, California},
    }
-
   title: Ideological Perspective Detection Using Semantic Features
   authors: Heba Elfardy, Mona Diab and Chris Callison-Burch
   venue: STARSEM
   type: conference
   year: 2015
   url: https://www.cis.upenn.edu/~ccb/publications/ideological-perspective-detection.pdf
   page_count: 10
   id: ideological-perspective-detection
   abstract: In this paper, we propose the use of word sense disambiguation and latent semantic features to automatically identify a person’s perspective from his/her written text. We run an Amazon Mechanical Turk experiment where we ask Turkers to answer a set of constrained and open-ended political questions drawn from the American National Election Studies (ANES). We then extract the proposed features from the answers to the open-ended questions and use them to predict the answer to one of the constrained questions, namely, their preferred Presidential Candidate. In addition to this newly created dataset, we also evaluate our proposed approach on a second standard dataset of "Ideological-Debates". This latter dataset contains topics from four domains&colon; Abortion, Creationism, Gun Rights and Gay Rights. Experimental results show that using word sense disambiguation and latent semantics, whether separately or combined, beats the majority and random baselines on the cross-validation and held-out-test sets for both the ANES and the four domains of the "Ideological Debates" datasets. Moreover combining both feature sets outperforms a stronger unigram-only classification system.
   bibtex: |
    @InProceedings{elfardy-diab-callisonburch:2015:*SEM2015,
      author    = {Elfardy, Heba  and  Diab, Mona  and  Callison-Burch, Chris},
      title     = {Ideological Perspective Detection Using Semantic Features},
      booktitle = {Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics},
      month     = {June},
      year      = {2015},
      address   = {Denver, Colorado},
      publisher = {Association for Computational Linguistics},
      pages     = {137--146},
      url       = {http://www.aclweb.org/anthology/S15-1015}
    }
-
   title: End-to-End Statistical Machine Translation with Zero or Small Parallel Texts
   authors: Ann Irvine and Chris Callison-Burch
   venue: Journal of Natural Language Engineering
   pages: 34
   type: journal
   year: 2016
   url: https://www.cis.upenn.edu/~ccb/publications/end-to-end-smt-with-zero-or-small-bitexts.pdf
   page_count: 34
   id: end-to-end-smt-with-zero-or-small-bitexts
   abstract: We use bilingual lexicon induction techniques, which learn translations from monolingual texts in two languages, to build an end-to-end statistical machine translation (SMT) system without the use of any bilingual sentence-aligned parallel corpora. We present detailed analysis of the accuracy of bilingual lexicon induction, and show how a discriminative model can be used to combine various signals of translation equivalence (like contextual similarity, temporal similarity, orthographic similarity and topic similarity). Our discriminative model produces higher accuracy translations than previous bilingual lexicon induction techniques. We reuse these signals of translation equivalence as features on a phrase-based SMT system. These monolingually-estimated features enhance low resource SMT systems in addition to allowing end-to-end machine translation without parallel corpora.
   bibtex: |
      @article{Irvine-Callison-Burch:2015:JNLE,
         author = {Ann Irvine and Chris Callison-Burch},
         title = {End-to-End Statistical Machine Translation with Zero or Small Parallel Texts},
         journal = {Journal of Natural Language Engineering},
         volume = {22},
         issue = {4},
         year = {2016},
         url = {http://www.cis.upenn.edu/~ccb/publications/end-to-end-smt-with-zero-or-small-bitexts.pdf},
         pages = {517-548}
       }
-
   title: Arabic Dialect Identification
   authors: Omar Zaidan and Chris Callison-Burch
   venue: Computational Linguistics
   type: journal
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/arabic-dialect-id.pdf
   page_count: 36
   id: arabic-dialect-id
   figures:
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-1.jpg
         label: Figure 1
         caption: One possible breakdown of spoken Arabic into dialect groups&colon; Maghrebi, Egyptian, Levantine, Gulf, and Iraqi. Habash (2010) and Versteegh (2001) give a breakdown along mostly the same lines. Note that this is a relatively coarse breakdown, and further division of the dialect groups is possible, especially in large regions such as the Maghreb.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-1.jpg
         label: Table 1
         caption: A few examples illustrating similarities and differences across MSA and three Arabic dialects&colon; Levantine, Gulf, and Egyptian. Even when a word is spelled the same across two or more varieties, the pronunciation might differ due to differences in short vowels (which are not spelled out). Also, due to the lack of orthography standardization, and variance in pronunciation even within a single dialect, some dialectal words could have more than one spelling (e.g. Egyptian “I drink” could be bAšrb, Levantine “He drinks” could be byšrb). (We use the Habash-Soudi-Buckwalter transliteration scheme to represent Arabic orthography, which maps each Arabic letter to a single, distinct character. We provide a table with the character mapping in Appendix A.)
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-2.jpg
         label: Figure 2
         caption: Two roughly equivalent Arabic sentences, one in MSA and one in Levantine Arabic, translated by the same MT system (Google Translate) into English. An acceptable translation would be When will we see this group of criminals undergo trial (or tried)?. The MSA variant is handled well, while the dialectal variant is mostly transliterated.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-3.jpg
         label: Figure 3
         caption: Two roughly equivalent Arabic sentences, one in MSA and one in Egyptian Arabic, translated by the same MT system (Google Translate) into English. An acceptable translation would be What is this that is happening? What is this that I’m seeing?. As in Figure 2, the dialectal variant is handles quite poorly.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-4.jpg
         label: Figure 4
         caption: The output of a Spanish-to-English system when given a Portuguese sentence as input, compared to the output of a Portuguese-to-English system, which performs well. The behavior is very similar to that in Figures 2 and 3, namely the failure to translate out-of-vocabulary words when there is a language mismatch.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-2.jpg
         label: Table 2
         caption: A summary of the different components of the AOC dataset. Overall, 1.4M comments were harvested from 86.1K articles, corresponding to 52.1M words.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-5.jpg
         label: Figure 5
         caption: Three sentences that were identified by our annotators as dialectical, even thought they do not contain individually dialectal words. A word-based OOV-detection approach would fail to classify these sentences as being dialectal, since all these words could appear in an MSA corpus. One might argue that a distinction should be drawn between informal uses of MSA versus dialectical sentences, but annotators consistently classify these sentences as dialect.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-6.jpg
         label: Figure 6
         caption: The dialectal sentences of Figure 5, with MSA equivalents.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-7.jpg
         label: Figure 7
         caption: The interface for the dialect identification task. This example, and the full interface, can be viewed at the URL http&colon;//bit.ly/eUtiO3.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-3.jpg
         label: Table 3
         caption: Some statistics over the labels provided by three spammers. Compared to the typical worker (right-most column), all workers perform terribly on the MSA control items, and also usually fail to recognize dialectal content in commentary sentences. Other red flags, such as geographic location and ‘identifying’ unrepresented dialects, are further proof of the spammy behavior.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-8.jpg
         label: Figure 8
         caption: The distribution of labels provided by the workers for the dialect identification task, over all three news sources (a) and over each individual news source (b–d). Al-Ghad is published in Jordan, Al-Riyadh in Saudi Arabia, and Al-Youm Al-Sabe’ in Egypt. Their local readerships are reflected in the higher proportion of corresponding dialects. Note that this is not a breakdown on the sentence level, and does not reflect any kind of majority voting. For example, most of the LEV labels on sentences from the Saudi newspaper are trumped by GLF labels when taking a majority vote, making the proportion of LEV-majority sentences smaller than what might be deduced by looking at the label distribution in (c).
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-4.jpg
         label: Table 4
         caption: The specific-dialect label distribution (given that a dialect label was provided), shown for each speaker group.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-9.jpg
         label: Figure 9
         caption: A bubble chart showing workers’ MSA and dialect recall. Each data point (or ‘bubble’) in the graph represents one annotator, with the bubble size corresponding to the number of Assignments completed by that annotator.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-5.jpg
         label: Table 5
         caption: Two annotators with a General label bias, one who uses the label liberally, and one who is more conservative. Note that in both cases, there is a noticeably smaller percentage of General labels in the Egyptian newspaper than in the Jordanian and Saudi newspapers.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-10.jpg
         label: Figure 10
         caption: Learning curves for the general MSA vs. dialect task, with all three news sources pooled together. Learning curves for the individual news sources can be found in Figure 11. The 83% line has no significance, and is provided to ease comparison with Figure 11.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-11.jpg
         label: Figure 11
         caption: Learning curves for the MSA vs. dialect task, for each of the three news sources. The 83% line has no significance, and is provided to ease comparison across the three components, and with Figure 10.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-6.jpg
         label: Table 6
         caption: Accuracy rates (%) on several 2-way classification tasks (MSA vs. dialect) for various models. Models in the top part of the table do not utilize the dialect-annotated data, while models in the bottom part do. (For the latter kind of models, the accuracy rates reported are based on a training set size of 90% of the available data.)
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-7.jpg
         label: Table 7
         caption: Confusion matrix in the 4-way classification setup. Rows correspond to actual labels, and columns correspond to predicted labels. For instance, 6.7% of MSA sentences were given a GLF label (first row, third column). Note that entries within a single row sum to 100%.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-8.jpg
         label: Table 8
         caption: Predicted label breakdown for the crawled data, over the four varieties of Arabic. All varieties were given equal priors.
   abstract: The written form of the Arabic language, Modern Standard Arabic (MSA), differs in a non-trivial manner from the various spoken regional dialects of Arabic – the true “native languages” of Arabic speakers. Those dialects, in turn, differ quite a bit from each other. However, due to MSA’s prevalence in written form, almost all Arabic datasets have predominantly MSA content. In this article, we describe the creation of a novel Arabic resource with dialect annotations. We have created a large monolingual dataset rich in dialectal Arabic content, called the Arabic Online Commentary Dataset (Zaidan and Callison-Burch 2011). We describe our annotation effort to identify the dialect level (and dialect itself) in each of more than 100,000 sentences from the dataset by crowdsourcing the annotation task, and delve into interesting annotator behaviors (like over-identification of one’s own dialect). Using this new annotated dataset, we consider the task of Arabic dialect identification&colon; given the word sequence forming an Arabic sentence, determine the variety of Arabic in which it is written. We use the data to train and evaluate automatic classifiers for dialect identification, and establish that classifiers using dialectal data significantly and dramatically outperform baselines that use MSA-only data, achieving near-human classification accuracy. Finally, we apply our classifiers to discover dialectical data from a large web crawl consisting of 3.5 million pages mined from online Arabic newspapers. 
   bibtex: |
      @article{zaidan-callisonburch:CL:2013,
         author    = {Omar F. Zaidan and Chris Callison-Burch},
         title =   {Arabic Dialect Identification},
         journal = {Computational Linguistics},
         year =    {2014},
         volume = {40},
         number = {1},
         pages = {171-202}
       }

-
   title: Extracting Lexically Divergent Paraphrases from Twitter
   authors: Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan and Yangfeng Ji
   venue: TACL
   type: journal
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf
   page_count: 14
   highly_cited: 130
   id: extracting-paraphrases-from-twitter
   figures:
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-1.jpg
         label: Figure 1
         caption: (a) a plate representation of the MULTIP model (b) an example instantiation of MULTIP for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30, are shown here.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-table-1.jpg
         label: Table 1
         caption: Representative examples from paraphrase corpora. The average sentence length is 11.9 words in Twitter vs. 18.6 in the news corpus.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-2.jpg
         label: Figure 2
         caption: MULTIP Learning Algorithm
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-table-2.jpg
         label: Table 2
         caption: Performance of different paraphrase identification approaches on Twitter version that uses additional 1.6 million sentences from Twitter. ** Reimplementation of a strong baseline used by Das and Smith (2009).
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-table-3.jpg
         label: Table 3
         caption: Feature ablation by removing each individual feature group from the full set.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-table-4.jpg
         label: Table 4
         caption: Example system outputs; rank is the position in the list of all candidate paraphrase pairs in the test set ordered by model score. MULTIP discovers lexically divergent paraphrases while LEXLATENT prefers more overall sentence similarity. Underline marks the word pair(s) with highest estimated probability as paraphrastic anchor(s) for each sentence pair.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-3.jpg
         label: Figure 3
         caption: Precision and recall curves. Our MULTIP model alone achieves competitive performance with the LEXLATENT system that combines latent space model and feature-based supervised classifier. The two approaches have complementary strengths, and achieves significant improvement when combined together (MULTIP-PE).
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-4.jpg
         label: Figure 4
         caption: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are non-paraphrases; 4,5 are paraphrases. Medium-scored cases are discarded in training and testing in our experiments.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-5.jpg
         label: Figure 5
         caption: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-6.jpg
         label: Figure 6
         caption: Numbers of paraphrases collected by different methods. The annotation efficiency (3,4,5 are regarded as paraphrases) is significantly improved by the sentence filtering and Multi-Armed Bandits (MAB) based topic selection.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-7.jpg
         label: Figure 7
         caption: PINC scores of paraphrases collected. The higher the PINC, the more significant the rewording. Our proposed annotation strategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC.
   abstract: We present MULTIP (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent variable model alone, we achieve the performance competitive with a state-of-the-art method which combines a latent space model with a feature-based supervised classifier. Our model also captures lexically divergent paraphrases that differ from yet complement previous methods; combining our model with previous work significantly outperforms the state-of-the-art. In addition, we present a novel annotation methodology that has allowed us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community. 
   bibtex: |
      @article{Xu-EtAl-2014:TACL,
         author =  {Wei Xu and Alan Ritter and Chris Callison-Burch and William B. Dolan and Yangfeng Ji},
         title =   {Extracting Lexically Divergent Paraphrases from {Twitter}},
         journal = {Transactions of the Association for Computational Linguistics},
         volume =  {2},
         number =  {},
         year =    {2014},
         pages = {435--448},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf}
       }
       
-
   title: Translations of the CALLHOME Egyptian Arabic corpus for conversational speech translation
   authors: Gaurav Kumar, Yuan Cao, Ryan Cotterell, Chris Callison-Burch, Daniel Povey, and Sanjeev Khudanpur
   venue: IWSLT
   type: workshop
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/callhome-egyptian-arabic-speech-translations.pdf
   page_count: 5
   id: callhome-egyptian-arabic-speech-translations
   figures:
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-1.jpg
         label: Table 1
         caption: Sizes (in # conversations) of the Callhome Egyptian Arabic corpus, supplements and evaluation datasets. The conversations last between 5-30 minutes.
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-2.jpg
         label: Table 2
         caption: Partition statistics for the Callhome Egyptian Arabic corpus, supplements and evaluation datasets. Column 2,3 and 4 represent number of utterances, numbers of words and average number of words per utterance respectively.
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-3.jpg
         label: Table 3
         caption: A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion.
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-4.jpg
         label: Table 4
         caption: The results of the translation task described in section 4. Each utterance in the original partitions has about four redundant translations. The number of utterances in column 2 has hence effectively been multiplied by 4. The last column represents the number of words per utterance in the translations.
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-5.jpg
         label: Table 5
         caption: A sample of the translations obtained using the translation task described in section 4. The translations are lower-cased, tokenized and punctuation has been normalized.
   abstract: Translation of the output of automatic speech recognition (ASR) systems, also known as speech translation, has received a lot of research interest recently. This is especially true for programs such as DARPA BOLT which focus on improving spontaneous human-human conversation across languages. However, this research is hindered by the dearth of datasets developed for this explicit purpose. For Egyptian Arabic-English, in particular, no parallel speech-transcription-translation dataset exists in the same domain. In order to support research in speech translation, we introduce the Callhome Egyptian Arabic-English Speech Translation Corpus. This supplements the existing LDC corpus with four reference translations for each utterance in the transcripts. The result is a three-way parallel dataset of Egyptian Arabic Speech, transcriptions and English translations. 
   bibtex: |
      @InProceedings{kumar-EtAl:2014:IWSLT,
         author    = {Matt Post and Gaurav Kumar and Adam Lopez and Damianos Karakos and Chris Callison-Burch and Sanjeev Khudanpur},
         title     = {Translations of the {CALLHOME} {Egyptian} {Arabic} corpus for conversational speech translation},
         booktitle = {Proceedings of the International Workshop on Spoken Language Translation (IWSLT)}
         month     = {December},
         year      = {2014},
         address   = {Lake Tahoe, USA},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/callhome-egyptian-arabic-speech-translations.pdf}
       }
       
-
   title: Poetry of the Crowd&colon; A Human Computation Algorithm to Convert Prose into Rhyming Verse
   authors: Quanze Chen, Chenyang Lei, Wei Xu, Ellie Pavlick and Chris Callison-Burch
   venue: HCOMP Poster
   type: workshop
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/poetry-generation-with-crowdsourcing.pdf
   page_count: 3
   id: poetry-generation-with-crowdsourcing
   figures:
      -
         img: figures/poetry-generation-with-crowdsourcing/poetry-generation-with-crowdsourcing-figure-1.jpg
         label: Figure 1
         caption: We combine the creative powers and language skills of people with computational algorithms for expanding paraphrase, identifying rhymes and calculating meter
      -
         img: figures/poetry-generation-with-crowdsourcing/poetry-generation-with-crowdsourcing-figure-2.jpg
         label: Figure 2
         caption: Appropriate lexical substitutions (in shaded boxes) selected by crowdsourcing workers regarding to the context.
      -
         img: figures/poetry-generation-with-crowdsourcing/poetry-generation-with-crowdsourcing-figure-3.jpg
         label: Figure 3
         caption: Figure 3&colon; A crowd worker composes a sentence path with the “- + - + - + - + - +” stress pattern (iambic pentameter). The ending word stalls rhymes with other words that terminate sentences constructed in another worker’s task.
   abstract: Poetry composition is a very complex task that requires a poet to satisfy multiple constraints concurrently. We believe that the task can be augmented by combining the creative abilities of humans with computational algorithms that efficiently constrain and permute available choices. We present a hybrid method for generating poetry from prose that combines crowdsourcing with natural language processing (NLP) machinery. We test the ability of crowd workers to accomplish the technically challenging and creative task of composing poems. 
   bibtex: |
      @InProceedings{Chen-et-al:HCOMP:2014,
         author    = {Quanze Chen and Chenyang Lei and Wei Xu and Ellie Pavlick and Chris Callison-Burch},
         title     = {Poetry of the Crowd: A Human Computation Algorithm to Convert Prose into Rhyming Verse},
         booktitle = {The Second AAAI Conference on Human Computation and Crowdsourcing (HCOMP-2014)},
         month     = {November},
         year      = {2014},
         url       = {http://cis.upenn.edu/~ccb/publications/poetry-generation-with-crowdsourcing.pdf}
       }
       
-
   title: Crowd-Workers&colon; Aggregating Information Across Turkers To Help Them Find Higher Paying Work
   authors: Chris Callison-Burch
   venue: HCOMP Poster
   type: workshop
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/crowd-workers.pdf
   page_count: 2
   id: crowd-workers
   figures:
      -
         img: figures/crowd-workers/crowd-workers-figure-1.jpg
         label: Figure 1
         caption: The crowd-workers.com web service allows users to sort HITs based on hourly rate. The hourly rate is estimated by tracking how long it took other workers to complete the task (using our browser extension), along with the reward amount (which MTurk makes available).
      -
         img: figures/crowd-workers/crowd-workers-figure-2.jpg
         label: Figure 2
         caption: The hourly earnings of the 65 Turkers in our pilot study who completed more than 100 HITs using the Crowd-Workers browser plugin.
   abstract: The Mechanical Turk crowdsourcing platform currently fails to provide the most basic piece of information to enable workers to make informed decisions about which tasks to undertake&colon; what is the expected hourly pay? Mechanical Turk advertises a reward amount per assignment, but does not give any indication of how long each assignment will take. We have developed a browser plugin that tracks the length of time it takes to complete a task, and a web service that aggregates the information across many workers. Crowd-Workers. com allows workers to discovery higher paying work by sorting tasks by estimated hourly rate. 
   bibtex: |
      @InProceedings{Chen-et-al:HCOMP:2014,
         author    = {Chris Callison-Burch},
         title     = {Crowd-Workers: Aggregating Information Across Turkers To Help Them Find Higher Paying Work},
         booktitle = {The Second AAAI Conference on Human Computation and Crowdsourcing (HCOMP-2014)},
         month     = {November},
         year      = {2014},
         url       = {http://cis.upenn.edu/~ccb/publications/crowd-workers.pdf}
       }
       
-
   title: The Language Demographics of  Amazon Mechanical Turk
   authors: Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev, and Chris Callison-Burch
   venue: TACL
   type: journal
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf
   data: http://www.seas.upenn.edu/~nlp/resources/TACL-data-release/dictionaries.tar.gz
   code: http://www.seas.upenn.edu/~nlp/resources/TACL-data-release/mturk-data.tar.gz
   page_count: 13
   id: language-demographics-of-mechanical-turk
   figures:
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-1.jpg
         label: Figure 1
         caption: The number of workers per country. This map was generated based on geolocating the IP address of 4,983 workers in our study. Omitted are 60 workers who were located in more than one country during the study, and 238 workers who could not be geolocated. The size of the circles represents the number of workers from each country. The two largest are India (1,998 workers) and the United States (866). To calibrate the sizes&colon; the Philippines has 142 workers, Egypt has 25, Russia has 10, and Sri Lanka has 4.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-1.jpg
         label: Table 1
         caption: Self-reported native language of 3,216 bilingual Turkers. Not shown are 49 languages with 20 speakers. We omit 1,801 Turkers who did not report their native language, 243 who reported 2 native languages, and 83 with
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-2.jpg
         label: Table 2
         caption: A list of the languages that were used in our study, grouped by the number of Wikipedia articles in the language. Each language’s code is given in parentheses. These language codes are used in other figures throughout this paper.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-3.jpg
         label: Figure 3
         caption: An example of the Turkers’ translations of a Hindi sentence. The translations are unedited and contain fixable spelling, capitalization and grammatical errors.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-2.jpg
         label: Figure 2
         caption: Days to complete the translation HITs for 40 of the languages. Tick marks represent the completion of individual assignments.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-4.jpg
         label: Figure 4
         caption: Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the proportion of translations which exactly matched gold standard translations, and light blue indicate translations which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-5.jpg
         label: Figure 5
         caption: (a) Individual workers’ overlap with Google Translate. We removed the 500 workers with the highest overlap (shaded region on the left) from our analyses, as it is reasonable to assume these workers are cheating by submitting translations from Google. Workers with no overlap (shaded region on the right) are also likely to be cheating, e.g. by submitting random text. (b) Cumulative distribution of overlap with Google translate for workers and translations. We see that eliminating all workers with >70% overlap with google translate still preserves 90% of translations and >90% of workers.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-3.jpg
         label: Table 3
         caption: Translation quality when partitioning the translations into two groups, one containing translations submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the other containing translations from Turkers outside those regions. In general, in-region Turkers provide higher quality translations. (**) indicates differences significant at p=0.05, (*) at p=0.10.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-6.jpg
         label: Figure 6
         caption: The total volume of translations (measured in English words) as a function of elapsed days.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-4.jpg
         label: Table 4
         caption: Size of parallel corpora and bilingual dictionaries collected for each language.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-5.jpg
         label: Table 5
         caption: BLEU scores for translating into English using bilingual parallel corpora by themselves, and with the addition of single-word dictionaries. Scores are calculated using four reference translations and represent the mean of three MERT runs.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-6.jpg
         label: Table 6
         caption: The green box shows the best languages to target on MTurk. These languages have many workers who generate high quality results quickly. We defined many workers as 50 or more active in-region workers, high quality as
   abstract: We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers' self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems. 
   bibtex: |
      @article{Pavlick-EtAl-2014:TACL,
         author =  {Ellie Pavlick and Matt Post and Ann Irvine and Dmitry Kachaev and Chris Callison-Burch},
         title =   {The Language Demographics of {Amazon Mechanical Turk}},
         journal = {Transactions of the Association for Computational Linguistics},
         volume =  {2},
         number =  {Feb},
         year =    {2014},
         pages = {79--92},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf}
       }
       
-
   title: Hallucinating Phrase Translations for Low Resource MT
   authors: Ann Irvine and Chris Callison-Burch
   venue: CoNLL
   type: conference
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/hallucinating-phrase-translations.pdf
   page_count: 11
   id: hallucinating-phrase-translations
   figures:
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-figure-1.jpg
         label: Figure 1
         caption: Example of loosely composed translations for the Spanish input in A, la casa linda. In B, we remove the stop word la. Then, in C, we enumerate the cartesian product of all unigram translations in the bilingual dictionary and sort the words within each alphabetically. Finally, we look up each list of words in C in the inverted index, and corresponding target phrases are enumerated in D. The inverted index contains all phrasal combinations and permutations of the word lists in C which also appear monolingually with some frequency and with, optionally, any number of stop words.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-figure-2.jpg
         label: Figure 2
         caption: Example output from motivating experiment&colon; a comparison of the baseline and full oracle translations of Spanish no hab ́ıa nadie en los centros electorales, which translates correctly as there was nobody at the voting offices. The full oracle is augmented with translations composed from the seed model as well as induced unigram translations. The phrase was no one is composeable from hab ́ıa nadie given the seed model. In contrast, the phrase polling stations is composeable from centros electorales using induced translations. For each translation, the phrase segmentations used by the decoder are highlighted.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-table-1.jpg
         label: Table 1
         caption: Motivating Experiment&colon; BLEU results using the baseline SMT model and composeable oracle translations with and without induced unigram translations.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-table-2.jpg
         label: Table 2
         caption: Top five induced translations for several source words. Correct translations are bolded. aceite translates as oil.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-table-3.jpg
         label: Table 3
         caption: Top three compositional translations for several source phrases and their model scores. Correct translations are bolded.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-figure-3.jpg
         label: Figure 3
         caption: Precision Recall curve with BLEU scores for the top-k scored hallucinated translations. k varies from 1 to 200. Baseline model performance is shown with a red triangle.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-table-4.jpg
         label: Table 4
         caption: Experimental results. First, the baseline models are augmented with monolingual phrase table features and then also with the top-5 induced translations for all OOV unigrams. Then, we append the top-k hallucinated phrase translations to the third baseline models. BLEU scores are aver- aged over three tuning runs. We measure the statistical significance of each +Phrase Trans model in comparison with the highest performing (bolded) baseline for each language; * indicates statistical significance with p ă0.01.
   abstract: We demonstrate that “hallucinating” phrasal translations can significantly improve the quality of machine translation in low resource conditions. Our hallucinated phrase tables consist of entries composed from multiple unigram translations drawn from the baseline phrase table and from translations that are induced from monolingual corpora. The hallucinated phrase table is very noisy. Its translations are low precision but high recall. We counter this by introducing 30 new feature functions (including a variety of monolingually-estimated features) and by aggressively pruning the phrase table. Our analysis evaluates the intrinsic quality of our hallucinated phrase pairs as well as their impact in end-to-end Spanish-English and Hindi-English MT. 
   bibtex: |
      @InProceedings{irvine-callisonburch:2014:W14-16,
         author    = {Irvine, Ann  and  Callison-Burch, Chris},
         title     = {Hallucinating Phrase Translations for Low Resource MT},
         booktitle = {Proceedings of the Eighteenth Conference on Computational Natural Language Learning},
         month     = {June},
         year      = {2014},
         pages     = {160--170},
         url       = {http://www.aclweb.org/anthology/W14-1617}
       }
       
-
   title: Using Comparable Corpora to Adapt MT Models to New Domains
   authors: Ann Irvine and Chris Callison-Burch
   venue: WMT
   type: workshop
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/using-comparable-corpora-for-mt-adaptation.pdf
   page_count: 8
   id: using-comparable-corpora-for-mt-adaptation
   figures:
      -
         img: figures/using-comparable-corpora-for-mt-adaptation/using-comparable-corpora-for-mt-adaptation-table-1.jpg
         label: Table 1
         caption: Summary of the size of each corpus of text used in this work in terms of the number of source and target word tokens.
      -
         img: figures/using-comparable-corpora-for-mt-adaptation/using-comparable-corpora-for-mt-adaptation-table-2.jpg
         label: Table 2
         caption: Top 10 Wikipedia articles ranked by their similarity to large new-domain English monolingual corpora.
      -
         img: figures/using-comparable-corpora-for-mt-adaptation/using-comparable-corpora-for-mt-adaptation-table-3.jpg
         label: Table 3
         caption: Comparison between the performance of baseline old-domain translation models and domain-adapted models in translating science and medical domain text. We experiment with two language models&colon; old, trained on the English side of our Hansard old-domain training corpus and new, trained on the English side of the parallel training data in each new domain. We use comparable corpora of 5, 000 (1) random, and (2) the most new-domain-like document pairs to score phrase tables. All results are averaged over three tuning runs, and we perform statistical significance testing comparing each system augmented with additional features with the baseline system that uses the same language model(s). * indicates that the BLEU scores are statistically significant with p † 0.01.
   abstract: In previous work we showed that when using an SMT model trained on old-domain data to translate text in a new-domain, most errors are due to unseen source words, unseen target translations, and inaccurate translation model scores (Irvine et al., 2013a). In this work, we target errors due to inaccurate translation model scores using new-domain comparable corpora, which we mine from Wikipedia. We assume that we have access to a large olddomain parallel training corpus but only enough new-domain parallel data to tune model parameters and do evaluation. We use the new-domain comparable corpora to estimate additional feature scores over the phrase pairs in our baseline models. Augmenting models with the new features improves the quality of machine translations in the medical and science domains by up to 1.3 BLEU points over very strong baselines trained on the 150 million word Canadian Hansard dataset. 
   bibtex: |
      @InProceedings{irvine-callisonburch:2014:W14-33,
         author    = {Irvine, Ann  and  Callison-Burch, Chris},
         title     = {Using Comparable Corpora to Adapt MT Models to New Domains},
         booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2014},
         pages     = {437--444},
         url       = {http://www.aclweb.org/anthology/W14-3357}
       }
       
-
   title: Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors
   authors: Rui Yan, Mingkun Gao, Ellie Pavlick, and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf
   page_count: 11
   id: crowdsourced-translation-via-collaboration-between-translators-and-editors
   figures:
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-table-1.jpg
         label: Table 1
         caption: Different versions of translations.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-1.jpg
         label: Figure 1
         caption: Relationship between editor aggressiveness and effectiveness. Each point represents an editor/translation pair. Aggressiveness (x-axis) is measured as the TER between the pre-edit and post-edit version of the translation, and effective- ness (y-axis) is measured as the average amount by which the editing reduces the translation’s TERgold. While many editors make only a few changes, those who make many changes can bring the translation substantially closer to professional quality.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-2.jpg
         label: Figure 2
         caption: Effect of editing on translations of varying quality. Rows reflect bins of editors, with the worse editors (those whose changes result in increased TERgold) on the top and the most effective editors (those whose changes result in the largest reduction in TERgold) on the bottom. Bars reflect bins of translations, with the highest TERgold translations on the left, and the lowest on the right. We can see from the consistently negative
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-3.jpg
         label: Figure 3
         caption: Three alternative translations (left) and the edited versions of each (right). Each edit on the right was produced by a different editor. Order reflects the TERgold of each translation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-4.jpg
         label: Figure 4
         caption: 2-step collaborative crowdsourcing translation model based on graph ranking framework including three sub-networks. The undirected links between users denotes translation-editing collaboration. The undirected links between candidate translations indicate lexical similarity between candidates. A bipartite graph ties candidate and Turker networks together by authorship (to make the figure clearer, some linkage is omitted). A dashed circle indicates the group of candidate translations for a single source sentence to translate.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-table-2.jpg
         label: Table 2
         caption: Overall BLEU performance for all methods (with and without post-editing). The highlighted result indicates the best performance, which is based on both candidate sentences and Turker information.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-5.jpg
         label: Figure 5
         caption: Effect of candidate-Turker coupling (
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-table-3.jpg
         label: Table 3
         caption: Variations of all component settings.
   abstract: Crowdsourcing is a viable mechanism for creating training data for machine translation. It provides a low cost, fast turn-around way of processing large volumes of data. However, when compared professional translation, naive collection of translations from non-professionals yields low-quality results. Careful quality control is necessary for crowdsourcing to work well. In this paper, we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals. We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits, and improves translation quality closer to professionals. 
   bibtex: |
      @InProceedings{Yan-EtAl-2014:ACL,
         author =  {Rui Yan and Mingkun Gao and Ellie Pavlick and Chris Callison-Burch},
         title =   {Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors},
         booktitle = {The 52nd Annual Meeting of the Association for Computational Linguistics},
         month     = {June},
         year      = {2014},
         address   = {Baltimore, Maryland},
         publisher = {Association for Computional Linguistics},
         url = {http://www.cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf}
       }
       
-
   title: PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching
   authors: Jonathan Weese, Juri Ganitkevitch, and Chris Callison-Burch
   venue: EACL
   type: conference
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf
   page_count: 10
   id: paradigm-paraphrase-evaluation
   figures:
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-figure-1.jpg
         label: Figure 1
         caption: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-figure-2.jpg
         label: Figure 2
         caption: Four examples each of lexical, phrasal, and syntactic paraphrases that can be extracted from the sentence pair in Figure 1.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-figure-3.jpg
         label: Figure 3
         caption: We measure the goodness of paraphrase grammars by determine how often they can be used to synchronously parse gold-standard sentential paraphrases. Note we do not require the synchronous derivation to match a gold-standard parse tree.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-1.jpg
         label: Table 1
         caption: Amount of English–English parallel data. LDC data has 4 parallel translations per sentence. Literature data is from Barzilay and McKeown (2001). MSR data is from Quirk et al. (2004) and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008).
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-2.jpg
         label: Table 2
         caption: Size of various paraphrase grammars.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-3.jpg
         label: Table 3
         caption: Size of strict overlap (number of rules and % of the gold standard) of each grammar with a syntactic grammar derived from ParaMetric. freq. ≥ 2 means we first removed all rules that appeared only once from the ParaMetric grammar. The number in parentheses shows the percentage of ParaMetric rules that are present in the overlap.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-4.jpg
         label: Table 4
         caption: Size of non-strict overlap of each grammar with the syntactic grammar derived from ParaMetric. The number in parentheses shows the percentage of ParaMetric rules that are present in the overlap.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-figure-4.jpg
         label: Figure 4
         caption: Precision lower bound and relative recall when overlapping different sizes of PPDB with the syntactic ParaMetric grammar.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-5.jpg
         label: Table 5
         caption: Number of paraphrases of each type in each grammar’s strict overlap with the syntactic ParaMetric grammar. Numbers in parentheses show the percentage of ParaMetric rules of each type.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-6.jpg
         label: Table 6
         caption: Parse coverage on held-out LDC data. The all column considers every possible sentential paraphrase in the test set. The any column considers a sentence parsed if any of its paraphrases was able to parsed.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-7.jpg
         label: Table 7
         caption: The correlation (Spearman’s ρ) of different automatic evaluation metrics with human judgments of paraphrase quality for the text-to- text generation task of sentence compression.
   abstract: Paraphrase evaluation is typically done either manually or through indirect, task-based evaluation. We introduce an intrinsic evaluation PARADIGM which measures the goodness of paraphrase collections that are represented using synchronous grammars. We formulate two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a monolingual parallel corpus. The first measure calculates how often a paraphrase grammar is able to synchronously parse the sentence pairs in the corpus. The second measure enumerates paraphrase rules from the monolingual parallel corpus and calculates the overlap between this reference paraphrase collection and the paraphrase resource being evaluated. We demonstrate the use of these evaluation metrics on paraphrase collections derived from three different data types&colon; multiple translations of classic French novels, comparable sentence pairs drawn from different newspapers, and bilingual parallel corpora. We show that PARADIGM correlates with human judgments more strongly than BLEU on a task-based evaluation of paraphrase quality. 
   bibtex: |
      @InProceedings{Weese-EtAl:2014:EACL,
         author    = {Jonathan Weese and Juri Ganitkevitch and Chris Callison-Burch},
         title     = {PARADIGM: Paraphrase Diagnostics through Grammar Matching},
         booktitle = {14th Conference of the European Chapter of the Association for Computational Linguistics},
         month     = {April},
         year      = {2014},
         address   = {Gothenburg, Sweden},
         publisher = {Association for Computional Linguistics},
         url       = {http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf}}
       }
       
-
   title: Crowdsourcing for Grammatical Error Correction
   authors: Ellie Pavlick, Rui Yan, and Chris Callison-Burch
   venue: CSCW Poster
   type: conference
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/crowdsourcing-for-grammatical-error-correction.pdf
   page_count: 4
   id: crowdsourcing-for-grammatical-error-correction
   figures:
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-table-1.jpg
         label: Table 1
         caption: Multiple ways of producing a correct sentence for the same input.
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-table-2.jpg
         label: Table 2
         caption: Edit distance may favor lazy workers over workers who make a concientious effort.
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-figure-1.jpg
         label: Figure 1
         caption: F1s (x100) of automated systems in CoNLL 2013 shared task (blue) and of Turkers (red). Turker performance is measured by taking the edits produced by the single highest-scoring Turker for each sentence. Turkers edited a subset of the training data, not the final test data.
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-figure-2.jpg
         label: Figure 2
         caption: F1 scores of each Turker vs. # of sentences corrected. Red lines show F1 scores of best CoNLL systems, black line is the average F1 of CoNLL systems. Omitted are 7 Turkers with >1000 sentences corrected. All had F1<0.15.
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-figure-3.jpg
         label: Figure 3
         caption: Data structure allows us to meausre agreement on a specific edit, even if final versions of the sentence vary considerable. Here, we are able to tell that the two workers agree that ’into’ should be changed to ’in’, even though they each perform the edit on a different version of the sentence.
   abstract: We discuss the problem of grammatical error correction, which has gained attention for its usefulness both in the development of tools for learners of foreign languages and as a component of statistical machine translation systems. We believe the task of suggesting grammar and style corrections in writing is well suited to a crowdsourcing solution but is currently hindered by the difficulty of automatic quality control. In this proposal, we motivate the problem of grammatical error correction and outline the challenges of ensuring quality in a setting where traditional methods of aggregation (e.g. majority vote) fail to produce the desired results. We then propose a design for quality control and present preliminary results indicating the potential of crowd workers to provide a scalable solution. 
   bibtex: |
      @InProceedings{Pavlick-EtAl:2014:CSCW,
         author    = {Ellie Pavlick and Rui Yan and Chris Callison-Burch},
         title     = {Crowdsourcing for Grammatical Error Correction},
         booktitle = {17th ACM Conference on Computer Supported Cooperative Work and Social Computing, Companion Volume},
         month     = {February},
         year      = {2014},
         address   = {Baltimore, Maryland},
         publisher = {Association for Computing Machinery},
         pages     = {209--213},
         url       = {http://cis.upenn.edu/~ccb/publications/crowdsourcing-for-grammatical-error-correction.pdf}}
       }
       
-
   title: The Multilingual Paraphrase Database
   authors: Juri Ganitkevitch and Chris Callison-Burch
   venue: LREC
   type: conference
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/ppdb-multilingual.pdf
   page_count: 8
   id: ppdb-multilingual
   figures:
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-figure-1.jpg
         label: Figure 1
         caption: German paraphrases are extracted by pivoting over a shared English translation.
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-figure-2.jpg
         label: Figure 2
         caption: In addition to extracting lexical and phrasal paraphrases, we also extract syntactic paraphrases. These have nonterminal symbols that act as slots that can be filled by other paraphrases that match that syntactic type. The syntactic labels are drawn from parse trees of the English sen- tences in our bitexts.
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-table-1.jpg
         label: Table 1
         caption: An example paraphrase rule for German. The four fields are the left hand size nonterminal, the phrase, the paraphrase and the features associated with the rule.
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-figure-3.jpg
         label: Figure 3
         caption: An overview of paraphrase collection size per language, measured in millions of paraphrase pairs.
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-table-2.jpg
         label: Table 2
         caption: The sizes of the bilingual training data used to extract each language-specific version of PPDB.
   abstract: We release a massive expansion of the paraphrase database (PPDB) that now includes a collection of paraphrases in 23 different languages. The resource is derived from large volumes of bilingual parallel data. Our collection is extracted and ranked using state of the art methods. The multilingual PPDB has over a billion paraphrase pairs in total, covering the following languages&colon; Arabic, Bulgarian, Chinese, Czech, Dutch, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Latvian, Lithuanian, Polish, Portugese, Romanian, Russian, Slovak, Slovenian, and Swedish. 
   bibtex: |
      @InProceedings{Ganitkevitch-Callison-Burch-2014:LREC,
         author =  {Juri Ganitkevitch and Chris Callison-Burch},
         title =   {The Multilingual Paraphrase Database},
         booktitle = {The 9th edition of the Language Resources and Evaluation Conference},
         month     = {May},
         year      = {2014},
         address   = {Reykjavik, Iceland},
         pages     = {},
         publisher = {European Language Resources Association},
         url = {http://cis.upenn.edu/~ccb/publications/ppdb-multilingual.pdf}
       }
       
-
   title: The American Local News Corpus
   authors: Ann Irvine, Joshua Langfus, and Chris Callison-Burch
   venue: LREC
   type: conference
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/american-local-news-corpus.pdf
   page_count: 4
   id: american-local-news-corpus
   figures:
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-1.jpg
         label: Figure 1
         caption: Millions of words of newspaper data contained in the ALNC, by state.
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-2.jpg
         label: Figure 2
         caption: Relative frequency of words related to different sports across time, for all locations. The beginning of professional sports leagues’ seasons are indicated with dotted vertical lines and the end of seasons with dashed vertical lines. For example, the National Football League’s season began in early September and ended in early February.
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-3.jpg
         label: Figure 3
         caption: Ratio of football words to hockey words, by state. The darkest shade indicates that football words are mentioned 80 times as often as hockey words; the lightest that football words are mentioned only about 20 times as often.
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-4.jpg
         label: Figure 4
         caption: Relative frequency (x10k) of ‘murder,’ by state. The lightest shade indicates that ‘murder’ appears once in every 20, 000 words; the darkest, about four times as often.
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-5.jpg
         label: Figure 5
         caption: Relative frequency (x10k) of ‘murder’ vs. the number of murders per 100, 000 people.
   abstract: We present the American Local News Corpus (ALNC), containing over 4 billion words of text from 2, 652 online newspapers in the United States. Each article in the corpus is associated with a timestamp, state, and city. All 50 U.S. states and 1, 924 cities are represented. We detail our method for taking daily snapshots of thousands of local and national newspapers and present two example corpus analyses. The first explores how different sports are talked about over time and geography. The second compares per capita murder rates with news coverage of murders across the 50 states. The ALNC is about the same size as the Gigaword corpus and is growing continuously. Version 1.0 is available for research use. 
   bibtex: |
      @InProceedings{Irvine-EtAl-2014:LREC,
         author =  {Ann Irvine and Joshua Langfus and Chris Callison-Burch},
         title =   {The {American} Local News Corpus},
         booktitle = {The 9th edition of the Language Resources and Evaluation Conference},
         month     = {May},
         year      = {2014},
         address   = {Reykjavik, Iceland},
         pages     = {},
         publisher = {European Language Resources Association},
         url = {http://cis.upenn.edu/~ccb/publications/american-local-news-corpus.pdf}
       }
       
-
   title: A Multi-Dialect, Multi-Genre Corpus of Informal Written Arabic
   authors: Ryan Cotterell and Chris Callison-Burch
   venue: LREC
   type: conference
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf
   page_count: 5
   highly_cited: 105
   id: arabic-dialect-corpus-2
   figures:
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-1.jpg
         label: Figure 1
         caption: Arabic Map
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-2.jpg
         label: Figure 2
         caption: Screenshot of HIT
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-3.jpg
         label: Figure 3
         caption: Newspaper commentary annotated on MTurk as having high dialectal content
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-4.jpg
         label: Figure 4
         caption: Tweets annotated on MTurk as having high dialectal content
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-6.jpg
         label: Figure 6
         caption: Workers’ Performance on Arabic Commentary HIT
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-5.jpg
         label: Figure 5
         caption: Workers’ Performance on Twitter HIT
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-7.jpg
         label: Figure 7
         caption: Sample Commentary
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-8.jpg
         label: Figure 8
         caption: Experiments on Extended AOC (accuracy reported)
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-9.jpg
         label: Figure 9
         caption: Experiments on Twitter (accuracy reported)
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-10.jpg
         label: Figure 10
         caption: Experiments on Extended AOC
   abstract: This paper presents a multi-dialect, multi-genre, human annotated corpus of dialectal Arabic with data obtained from both online newspaper commentary and Twitter. Most Arabic corpora are small and focus on Modern Standard Arabic (MSA). There has been recent interest, however, in the construction of dialectal Arabic corpora. This work differs from previously constructed corpora in two ways. First, we include coverage of five dialects of Arabic&colon; Egyptian, Gulf, Levantine, Maghrebi and Iraqi. This is the most complete coverage of any dialectal corpus known to the authors. In addition to data, we provide results for the Arabic dialect identification task that outperform those reported in Zaidan and Callison-Burch (2011). 
   bibtex: |
      @InProceedings{Cotterell-Callison-Burch-2014:LREC,
         author =  {Ryan Cotterell and Chris Callison-Burch},
         title =   {A Multi-Dialect, Multi-Genre Corpus of Informal Written {Arabic}},
         booktitle = {The 9th edition of the Language Resources and Evaluation Conference},
         month     = {May},
         year      = {2014},
         address   = {Reykjavik, Iceland},
         pages     = {},
         publisher = {European Language Resources Association},
         url = {http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf}
       }
       
-
   title: An Algerian Arabic-French Code-Switched Corpus
   authors: Ryan Cotterell, Adithya Renduchintala, Naomi Saphra, and Chris Callison-Burch
   venue: LREC Workshop on Free/Open-Source Arabic Corpora and Corpora Processing Tools
   type: workshop
   year: 2014
   url: https://www.cis.upenn.edu/~ccb/publications/arabic-french-codeswitching.pdf
   page_count: 4
   id: arabic-french-codeswitching
   abstract: Arabic is not just one language, but rather a collection of dialects in addition to Modern Standard Arabic (MSA). While MSA is used in formal situations, dialects are the language of every day life. Until recently, there was very little dialectal Arabic in written form. With the advent of social-media, however, the landscape has changed. We provide the first romanized code-switched Algerian Arabic-French corpus annotated for word-level language id. We review the history and sociological factors that make the linguistic situation in Algerian unique and highlight the value of this corpus to the natural language processing and linguistics communities. To build this corpus, we crawled an Algerian newspaper and extracted the comments from the news story. We discuss the informal nature of the language in the corpus and the challenges it will present. Additionally, we provide a preliminary analysis of the corpus. We then discuss some potential uses of our corpus of interest to the computational linguistics community. 
   bibtex: |
      @InProceedings{Cotterell-EtAl-2014:LREC-WS,
         author =  {Ryan Cotterell and Adithya Renduchintala and Naomi Saphra and Chris Callison-Burch},
         title =   {An {Algerian Arabic-French} Code-Switched Corpus},
         booktitle = {Workshop on Free/Open-Source Arabic Corpora and Corpora Processing Tools},
         month     = {May},
         year      = {2014},
         address   = {Reykjavik, Iceland},
         pages     = {},
         publisher = {European Language Resources Association},
         url = {http://cis.upenn.edu/~ccb/publications/arabic-french-codeswitching.pdf}
       }
       
-
   title: Open letter to President Obama
   authors: Chris Callison-Burch
   venue: Unpublished
   type: unpublished
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/letter-to-the-president.pdf
   page_count: 2
   id: letter-to-the-president
   abstract: I wrote an open letter to President Obama about my former PhD student, Omar Zaidan, who had his student visa revoked on the eve of his PhD defense, and who has not been allowed to return to the US in 1.5 years. The letter was read by over 35,000 people in the first week after I published it. 
-
   title: Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus
   authors: Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch and Sanjeev Khudanpur
   venue: IWSLT
   type: workshop
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf
   page_count: 7
   highly_cited: 129
   id: improved-speech-to-speech-translation
   figures:
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-1.jpg
         label: Table 1
         caption: Corpus size and cost. Counts of segments and words were computed after pre-processing (§2).
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-2.jpg
         label: Table 2
         caption: Data splits for Fisher Spanish (top), Callhome Spanish (middle), and Europarl + News Commentary (bottom; for comparison). Words is the number of Spanish word tokens (after tokenization). The mean number of words per sentences ranges from 11.8 to 13.1.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-3.jpg
         label: Table 3
         caption: Lattice statistics for the three Fisher and two Callhome test sets. Word error rates correspond to the 1-best and oracle paths from the lattice, and # Paths denotes the average number of distinct paths through each lattice. The average node density (the number of outgoing arcs) is 1.3 for Fisher and 1.4 for Callhome.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-4.jpg
         label: Table 4
         caption: BLEU scores (four references) on Fisher/Dev2. The columns vary the data used to train the MT system, and the rows alter the interface between the ASR and MT systems.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-5.jpg
         label: Table 5
         caption: BLEU scores (one reference) on Callhome/Evltest.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-figure-1.jpg
         label: Figure 1
         caption: A subgraph of a lattice (sentence 17 of Fisher/Dev2) representing an ASR ambiguity. The oracle path is in bold. With access to the lattice, the MT system avoids the untranslatable word incorporamos, found in the 1-best output, producing a better translation. Above the line are inputs and the reference, with the Lattice line denoting the path selected by the MT system. The Google line is suggestive of the general difficulty in translating conversational speech.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-figure-2.jpg
         label: Figure 2
         caption: Conversation-level WER and BLEU, for conversations found in Fisher/Dev (open points) and Fisher/Dev2 (solid points). The Pearson’s correlation coefficient is -0.72.
   abstract: Research into the translation of the output of automatic speech recognition (ASR) systems is hindered by the dearth of datasets developed for that explicit purpose. For Spanish-English translation, in particular, most parallel data available exists only in vastly different domains and registers. In order to support research on cross-lingual speech applications, we introduce the Fisher and Callhome Spanish-English Speech Translation Corpus, supplementing existing LDC audio and transcripts with (a) ASR 1-best, lattice, and oracle output produced by the Kaldi recognition system and (b) English translations obtained on Amazon’s Mechanical Turk. The result is a four-way parallel dataset of Spanish audio, transcriptions, ASR lattices, and English translations of approximately 38 hours of speech, with defined training, development, and held-out test sets. We conduct baseline machine translation experiments using models trained on the provided training data, and validate the dataset by corroborating a number of known results in the field, including the utility of in-domain (information, conversational) training data, increased performance translating lattices (instead of recognizer 1-best output), and the relationship between word error rate and BLEU score. 
   bibtex: |
      @InProceedings{post-EtAl:2013:IWSLT,
         author    = {Matt Post and Gaurav Kumar and Adam Lopez and Damianos Karakos and Chris Callison-Burch and Sanjeev Khudanpur},
         title     = {Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus},
         booktitle = {Proceedings of the International Workshop on Spoken Language Translation (IWSLT)}
         month     = {December},
         year      = {2013},
         address   = {Heidelberg, Germany},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf}
       }
       
-
   title: Semi-Markov Phrase-based Monolingual Alignment
   authors: Xuchen Yao, Ben Van Durme, Chris Callison-Burch and Peter Clark
   venue: EMNLP
   type: conference
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf
   page_count: 11
   id: semi-markov-phrase-based-monolingual-alignment
   figures:
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-figure-1.jpg
         label: Figure 1
         caption: A semi-Markov phrase-based model example and the desired Viterbi decoding path. Shaded horizontal circles represent the source sentence (Shops are closed up for now until March) and hollow vertical circles represent the hidden states with state IDs for the target sentence (Shops are temporarily closed down). State 0, a NULL state, is designated for deletion. One state (e.g. state 3 and 15) can span multiple consecutive source words (a semi-Markov property) for aligning phrases on the source side. States with an ID larger than the target sentence length indicate “phrasal states” (states 6-15 in this example), where consecutive target tokens are merged for aligning phrases on the target side. Combining the semi-Markov property and phrasal states yields for instance, a 2x2 alignment between closed up in the source and closed down in the target.
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-1.jpg
         label: Table 1
         caption: Statistics of the two manually aligned corpora, divided into training and test in sentence pairs. The length column shows average lengths of source and target sentences in a pair. %align. is the percentage of aligned tokens.
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-2.jpg
         label: Table 2
         caption: Percentage of various alignment sizes (undirectional, e.g., 1x2 and 2x1 are merged) after synthesizing phrasal alignment from token alignment in the training portion of two corpora.
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-3.jpg
         label: Table 3
         caption: Results on original (mostly token) and phrasal (P) alignment corpora, where (x%) indicates how much alignment is identical alignment, such as New$New. E% stands for exact (perfect) match rate. Subscript i stands for corresponding scores for “identical” alignment and n for “non-identical”. *&colon; scores of MANLI-joint were for the original Edinburgh corpus instead of Edinburgh++ (with hand corrections) so it is not a direct comparison.
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-4.jpg
         label: Table 4
         caption: Same results on the phrasal Edinburgh++ corpus but with scores divided by token-only alignment (subscript t) and phrase-only alignment (subscript p).
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-5.jpg
         label: Table 5
         caption: Results (Accuracy, Precision, Recall, Mean Average Precision, Mean Reciprocal Rank) on the tasks of RTE, PP and QA
   abstract: We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves state-of-the-art alignment accuracy on two phrase=based alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model’s alignment score approaches the state of the art. 
   bibtex: |
      @InProceedings{yao-EtAl:2013:EMNLP,
         author    = {Xuchen Yao and Benjamin {Van Durme} and Chris Callison-Burch and Peter Clark},
         title     = {Semi-Markov Phrase-based Monolingual Alignment},
         booktitle = {Proceedings of EMNLP}
         month     = {October},
         year      = {2013},
         address   = {Seattle, Washington},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf}
       }
       
-
   title: Findings of the 2013 Workshop on Statistical Machine Translation
   authors: Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia
   venue: WMT
   type: workshop
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/findings-of-the-wmt13-shared-tasks.pdf
   page_count: 44
   id: findings-of-the-wmt13-shared-tasks
   figures:
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-2.jpg
         label: Figure 2
         caption: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and the outputs of five systems (anonymized and randomly-ordered) and has to rank these according to their translation quality, ties are allowed. For technical reasons, annotators on Amazon’s Mechanical Turk received all three ranking tasks for a single HIT on a single page, one upon the other.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-2.jpg
         label: Table 2
         caption: Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the previous two workshops.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-3.jpg
         label: Table 3
         caption: κ scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re- searcher annotations and MTurk annotations, respectively. See Table 4 for corresponding intra-annotator agreement scores.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-4.jpg
         label: Table 4
         caption: κ scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota- tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that language pair.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-5.jpg
         label: Table 5
         caption: Agreement as a function of threshold for Turkers on the Russian–English task. The threshold is the percentage of controls a Turker must pass for her rankings to be accepted.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-6.jpg
         label: Table 6
         caption: Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between systems indicate clusters according to bootstrap resampling at p-level p ≤ .05. This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints provided for the shared task.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-3.jpg
         label: Figure 3
         caption: In this screen, the annotator is expected to correct the MT output given only the context of at most two neighbouring machine-translated sentences.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-4.jpg
         label: Figure 4
         caption: In this screen, the annotator is expected to validate the monolingual edit, correcting it if necessary. The annotator is expected to add the prefix ‘OK&colon;’ if the correction was more or less cosmetic.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-7.jpg
         label: Table 7
         caption: Distribution of review statuses.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-8.jpg
         label: Table 8
         caption: Annotator agreement when reviewing monolingual edits.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-9.jpg
         label: Table 9
         caption: Understandability of English!Czech systems. The _ values indicate empirical confidence bounds at 95%. Rank ranges were also obtained in the same resampling&colon; in 95% of observations, the system was ranked in the given range
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-10.jpg
         label: Table 10
         caption: Number of source sentences with the given number of distinct reference translations.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-5.jpg
         label: Figure 5
         caption: Correlation of BLEU and WMT13 manual ranks for English→Czech translation
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-6.jpg
         label: Figure 6
         caption: Correlation of NIST and WMT13 manual ranks for English→Czech translation
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-7.jpg
         label: Figure 7
         caption: Projections from Figure 5 of BLEU and WMT13 manual ranks for English→Czech translation
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-8.jpg
         label: Figure 8
         caption: Projections from Figure 6 of NIST and WMT13 manual ranks for English→Czech translation
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-11.jpg
         label: Table 11
         caption: Participants in the WMT13 Quality Estimation shared task.
   abstract: We present the results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task. This year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. The quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries. 
   bibtex: |
      @InProceedings{bojar-EtAl:2013:WMT,
         author    = {Bojar, Ond\v{r}ej  and  Buck, Christian  and  Callison-Burch, Chris  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Monz, Christof  and  Post, Matt  and  Soricut, Radu  and  Specia, Lucia},
         title     = {Findings of the 2013 {Workshop on Statistical Machine Translation}},
         booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
         month     = {August},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         pages     = {1--44},
         url       = {http://www.aclweb.org/anthology/W13-2201}
       }
       
-
   title: Joshua 5.0&colon; Sparser, better, faster, server
   authors: Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan Weese, Yuan Cao, and Chris Callison-Burch
   venue: WMT
   type: workshop
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/joshua-5.0.pdf
   page_count: 7
   id: joshua-5
   figures:
      -
         img: figures/joshua-5.0/joshua-5.0-figure-1.jpg
         label: Figure 1
         caption: End-to-end runtime as a function of the number of threads. Each data point is the minimum of at least fifteen different runs.
      -
         img: figures/joshua-5.0/joshua-5.0-figure-2.jpg
         label: Figure 2
         caption: Decoding time alone.
      -
         img: figures/joshua-5.0/joshua-5.0-figure-3.jpg
         label: Figure 3
         caption: Here, position-aware lexical and part-of-speech n-gram features, labeled dependency links, and features reflecting the phrase’s CCG-style label NP/NN are included in the context vector.
      -
         img: figures/joshua-5.0/joshua-5.0-table-1.jpg
         label: Table 1
         caption: Comparing Hadoop’s intermediate disk space use and extraction time on a selection of Europarl v.7 Hiero grammar extractions. Disk space was measured at its maximum, at the input of Thrax’s final grammar aggregation stage. Runtime was measured on our Hadoop cluster with a capacity of 52 mappers and 26 reducers. On average Thrax 2.0, bundled with Joshua 5.0, is up to 300% faster and more compact.
   abstract: We describe improvements made over the past year to Joshua, an open-source translation system for parsing-based machine translation. The main contributions this past year are significant improvements in both speed and usability of the grammar extraction and decoding steps. We have also rewritten the decoder to use a sparse feature representation, enabling training of large numbers of features with discriminative training methods. 
   bibtex: |
      @InProceedings{post-EtAl:2013:WMT,
         author    = {Post, Matt  and  Ganitkevitch, Juri  and  Orland, Luke  and  Weese, Jonathan  and  Cao, Yuan  and  Callison-Burch, Chris},
         title     = {Joshua 5.0: Sparser, Better, Faster, Server},
         booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
         month     = {August},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         pages     = {206--212},
         url       = {http://www.aclweb.org/anthology/W13-2226}
       }
       
-
   title: Combining Bilingual and Comparable Corpora for Low Resource Machine Translation
   authors: Ann Irvine and Chris Callison-Burch
   venue: WMT
   type: workshop
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/combining-bilingual-and-comparable-corpora.pdf
   page_count: 9
   id: combining-bilingual-and-comparable-corpora
   figures:
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-1.jpg
         label: Table 1
         caption: Information about datasets released by Post et al. (2012)&colon; thousands of words in the source language parallel sentences and dictionaries, and percent of development set word types (unique word tokens) and word tokens that are OOV (do not appear in either section of the training data).
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-2.jpg
         label: Table 2
         caption: Millions of words of time-stamped web crawls and Wikipedia text, by language.
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-figure-1.jpg
         label: Figure 1
         caption: Examples of OOV Bengali words, our top-3 ranked induced translations, and their correct translations.
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-3.jpg
         label: Table 3
         caption: Percent of word types in a held out portion of the training data which are translated correctly by our bilingual lexicon induction technique. Evaluation is over the top-1 and top-10 outputs in the ranked lists for each source word.
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-4.jpg
         label: Table 4
         caption: BLEU performance gains that target coverage (+OOV Trans.) and accuracy (+Features), and both (+Feats & OOV). OOV oracle uses OOV translations from automatic word alignments. Hiero and SAMT results are reported in Post et al. (2012).
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-5.jpg
         label: Table 5
         caption: Varying minimum training data frequency of source words for which new translations are induced and included in the phrase-based model. In all cases, the top-1 induced translation is added to the phrase table and features estimated over comparable corpora are included (i.e. +Feats & Trans model).
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-figure-2.jpg
         label: Figure 2
         caption: Comparison of learning curves over lines of parallel training data for four SMT systems&colon; our baseline phrase-based model (baseline), model that supplements the baseline with translations of OOV words induced using our supervised bilingual lexicon induction framework (+Trans), model that supplements the baseline with additional phrase table features estimated over comparable corpora (+Feats), and a system that supplements the baseline with both OOV translations and additional features (+Trans & Feats).
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-figure-3.jpg
         label: Figure 3
         caption: English to Urdu translation results using varying amounts of comparable corpora to estimate features and induce translations.
   abstract: Statistical machine translation (SMT) performance suffers when models are trained on only small amounts of parallel data. The learned models typically have both low accuracy (incorrect translations and feature scores) and low coverage (high out-of-vocabulary rates). In this work, we use an additional data resource, comparable corpora, to improve both. Beginning with a small bitext and corresponding phrase-based SMT model, we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora. Then, we supplement the model’s feature space with translation scores estimated over comparable corpora in order to improve accuracy. We observe improvements between 0.5 and 1.7 BLEU translating Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu into English. 
   bibtex: |
      @InProceedings{irvine-callisonburch:2013:WMT,
         author    = {Irvine, Ann  and  Callison-Burch, Chris},
         title     = {Combining Bilingual and Comparable Corpora for Low Resource Machine Translation},
         booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
         month     = {August},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         pages     = {262--270},
         url       = {http://www.aclweb.org/anthology/W13-2233}
       }
       
-
   title: A Lightweight and High Performance Monolingual Word Aligner
   authors: Xuchen Yao, Peter Clark, Ben Van Durme and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/monolingual-word-aligner.pdf
   page_count: 6
   id: monolingual-word-aligner
   figures:
      -
         img: figures/monolingual-word-aligner/monolingual-word-aligner-table-1.jpg
         label: Table 1
         caption: Results on the 800 pairs of test data. E% stands for exact (perfect) match rate. Systems marked with ⇤ are reported by MacCartney et al. (2008), with / by Thadani and McKeown (2011).
      -
         img: figures/monolingual-word-aligner/monolingual-word-aligner-table-2.jpg
         label: Table 2
         caption: Alignment runtime in seconds per sentence pair on two corpora&colon; RTE2 (Cohn et al., 2008) and FUSION (McKeown et al., 2010). The MANLI-* results are from Thadani and McKeown (2011), on a Xeon 2.0GHz with 6MB Cache. The runtime for this work takes the longest timing from S2T and T2S, on a Xeon 2.2GHz with 4MB cache (the closest we can find to match their hardware). Horizontally in a real- world application where sentences have similar length, this work is roughly 20x faster (0.096 vs. 2.45). Vertically, the decoding time for our work increases less dramatically when sentence length increases (0.025!0.096 vs. 0.08!2.45).
      -
         img: figures/monolingual-word-aligner/monolingual-word-aligner-table-3.jpg
         label: Table 3
         caption: Performance without POS and/or Word-Net features.
   abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system. 
   bibtex: |
      @InProceedings{yao-EtAl:2013:ACL,
         author    = {Xuchen Yao and Peter Clark and Benjamin {Van Durme} and Chris Callison-Burch},
         title     = {A Lightweight and High Performance Monolingual Word Aligner},
         booktitle = {Proceedings of the 2013 Conference of the Association for Computational Linguistics (ACL 2013)},
         month     = {July},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/monolingual-word-aligner.pdf}
       }
       
-
   title: PARMA&colon; A Predicate Argument Aligner
   authors: Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews, Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder, Jonathan Weese, Tan Xu and Xuchen Yao
   venue: ACL
   type: conference
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/parma.pdf
   page_count: 6
   id: parma
   figures:
      -
         img: figures/parma/parma-figure-1.jpg
         label: Figure 1
         caption: Example of gold-standard alignment pairs from Roth and Frank’s data set and our data set created from the LDC’s Multiple Translation Corpora. The RF data set exhibits high lexical overlap, where most of the alignments are between identical words like police-police and said-said. The LDC MTC was constructed to increase lexical diversity, balcony and tent-camp may relax this assumption.
      -
         img: figures/parma/parma-figure-2.jpg
         label: Figure 2
         caption: We plotted the PARMA’s performance on each of the document pairs. Red squares show the F1 for individual document pairs drawn from Roth and Frank’s data set, and black circles show F1 for our Multiple Translation Corpora test set. The x-axis represents the cosine similarity between the document pairs. On the RF data set, performance is correlated with lexical similarity. On our more lexically diverse set, this is not the case. This could be due to the fact that some of the documents in the RF sets are minor re-writes of the same newswire story, making them easy to align.
      -
         img: figures/parma/parma-table-1.jpg
         label: Table 1
         caption: PARMA outperforms the baseline lemma matching system on the three test sets, drawn from the Extended Event Coreference Bank, Roth and Frank’s data, and our set created from the Multiple Translation Corpora. PARMA achieves a higher F1 and recall score than Roth and Frank’s reported result.
   abstract: We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data which has a low baseline which we beat by 17% F1. 
   bibtex: |
      @InProceedings{wolfe-EtAl:2013:ACL,
         author    = {Travis Wolfe and Benjamin {Van Durme} and Mark Dredze and Nicholas Andrews and Charley Beller and Chris Callison-Burch and Jay DeYoung and Justin Snyder and Jonathan Weese and Tan Xu and Xuchen Yao},
         title     = {{PARMA}: A Predicate Argument Aligner},
         booktitle = {Proceedings of the 2013 Conference of the Association for Computational Linguistics (ACL 2013)},
         month     = {July},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/parma.pdf}
       }
-
   title: Learning to translate with products of novices&colon; a suite of open-ended challenge problems for teaching MT
   authors: Adam Lopez, Matt Post, Chris Callison-Burch, Jonathan Weese, Juri Ganitkevitch, Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin, Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert, Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao
   venue: TACL
   type: journal
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/teaching-machine-translation.pdf
   page_count: 13
   id: teaching-machine-translation
   figures:
      -
         img: figures/teaching-machine-translation/teaching-machine-translation-figure-1.jpg
         label: Figure 1
         caption: Submission history for the alignment challenge. Dashed lines represent the default and baseline system performance. Each colored line represents a student, and each dot represents a submission. For clarity, we show only submissions that improved the student’s AER.
      -
         img: figures/teaching-machine-translation/teaching-machine-translation-figure-2.jpg
         label: Figure 2
         caption: Submission history for the decoding challenge. The dotted green line represents the oracle over submissions.
      -
         img: figures/teaching-machine-translation/teaching-machine-translation-figure-3.jpg
         label: Figure 3
         caption: Submission history for the evaluation challenge.
      -
         img: figures/teaching-machine-translation/teaching-machine-translation-table-1.jpg
         label: Table 1
         caption: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).
   abstract: Machine translation (MT) draws from several different disciplines, making it a complex subject to teach. There are excellent pedagogical texts, but problems in MT and current algorithms for solving them are best learned by doing. As a centerpiece of our MT course, we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key MT tasks&colon; alignment, decoding, evaluation, and reranking. Students brought a diverse set of techniques to the problems, including some novel solutions which performed remarkably well. A surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks, demonstrating that even newcomers to the field can help improve the state-of-the-art on hard NLP problems while simultaneously learning a great deal. The problems, baseline code, and results are freely available. 
   bibtex: |
      @article{Lopez-etal:TACL:2013,
         author    = {Adam Lopez and Matt Post and Chris Callison-Burch and Jonathan Weese and Juri Ganitkevitch and Narges Ahmidi and Olivia Buzek and Leah Hanson and Beenish Jamil and Matthias Lee and Ya-Ting Lin and Henry Pao and Fatima Rivera and Leili Shahriyari and Debu Sinha and Adam Teichert and Stephen Wampler and Michael Weinberger and Daguang Xu and Lin Yang and and Shang Zhao},
         title =   {Learning to translate with products of novices: a suite of open-ended challenge problems for teaching {MT}},
         journal = {Transactions of the Association for Computational Linguistics},
         year =    {2013},
         volume = {1},
         number = {May},
         pages = {166--177}
       }
       
-
   title: Dirt Cheap Web-Scale Parallel Text from the Common Crawl
   authors: Jason Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch and Adam Lopez
   venue: ACL
   type: conference
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf
   page_count: 10
   highly_cited: 168
   id: bitexts-from-common-crawl
   figures:
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-1.jpg
         label: Table 1
         caption: The amount of parallel data mined from CommonCrawl for each language paired with English. Source tokens are counts of the foreign language tokens, and target tokens are counts of the English language tokens.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-2.jpg
         label: Table 2
         caption: Manual evaluation of precision (by sen- tence pair) on the extracted parallel data for Span- ish, French, and German (paired with English).
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-3.jpg
         label: Table 3
         caption: Automatic evaluation of precision through language identification for several lan- guages paired with English.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-4.jpg
         label: Table 4
         caption: The top five domains from the Spanish-English portion of the data. The domains are ranked by the combined number of source and target tokens.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-5.jpg
         label: Table 5
         caption: A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely tokens.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-6.jpg
         label: Table 6
         caption: A sample of topics along with the number of Europarl and CommonCrawl documents where they are the most likely topic in the mixture. We include topics that are mostly found in Europarl or CommonCrawl, and some that are somewhat prominent in both.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-7.jpg
         label: Table 7
         caption: Percentage of useful (non-boilerplate) sentences found by domain and language pair. hotel.info was not found in our German-English data.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-8.jpg
         label: Table 8
         caption: BLEU scores for several language pairs before and after adding the mined parallel data to systems trained on data from WMT data.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-9.jpg
         label: Table 9
         caption: BLEU scores for French-English and English-French before and after adding the mined parallel data to systems trained on data from WMT data including the French-English Giga- word (Callison-Burch et al., 2011).
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-10.jpg
         label: Table 10
         caption: The size (in English tokens) of the training corpora used in the SMT experiments from Tables 8 and 9 for each language pair.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-11.jpg
         label: Table 11
         caption: n-gram coverage percentages (up to 4-grams) of the source side of our test sets given our different parallel training corpora computed at the type level.
   abstract: Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource. 
   bibtex: |
      @InProceedings{smith-EtAl:2013:ACL,
         author    = {Jason Smith and Herve Saint-Amand and Magdalena Plamada and Philipp Koehn and Chris Callison-Burch and Adam Lopez},
         title     = {Dirt Cheap Web-Scale Parallel Text from the {Common Crawl}},
         booktitle = {Proceedings of the 2013 Conference of the Association for Computational Linguistics (ACL 2013)},
         month     = {July},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf}
       }
       
-
   title: PPDB&colon; The Paraphrase Database
   authors: Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/ppdb.pdf
   page_count: 7
   id: ppdb
   highly_cited: 921
   figures:
      -
         img: figures/ppdb/ppdb-figure-1.jpg
         label: Figure 1
         caption: Phrasal paraphrases are extracted via bilingual pivoting.
      -
         img: figures/ppdb/ppdb-figure-2.jpg
         label: Figure 2
         caption: Features extracted for the phrase the long term from the n-gram corpus (2a) and Annotated Gigaword (2b). (a) The n-gram corpus records the long-term as preceded by revise (43 times), and followed by plans (97 times). We add corresponding features to the phrase’s distributional signature retaining the counts of the original n-grams. (b) Here, position-aware lexical and part-of-speech n-gram features, labeled dependency links , and features reflecting the phrase’s CCG-style label NP/NN are included in the context vector.
      -
         img: figures/ppdb/ppdb-table-1.jpg
         label: Table 1
         caption: A breakdown of PPDB&colon;Eng size by paraphrase type. We distinguish lexical (i.e. one-word) paraphrases, phrasal paraphrases and syntactically labeled paraphrase patterns.
      -
         img: figures/ppdb/ppdb-table-2.jpg
         label: Table 2
         caption: An overview of PPDB&colon;Spa. Again, we partition the resource into lexical (i.e. one-word) paraphrases, phrasal paraphrases and syntactically labeled paraphrase patterns.
      -
         img: figures/ppdb/ppdb-figure-3.jpg
         label: Figure 3
         caption: To inspect our coverage, we use the Penn Treebank’s parses to map from Propbank annotations to PPDB’s syntactic patterns. For the above annotation predicate, we extract VBP ! expect, which is matched by paraphrase rules like VBP ! expect | anticipate and VBP ! expect | hypothesize. To search for the entire relation, we replace the argument spans with syntactic nonterminals. Here, we obtain S ! NP expect S, for which PPDB has matching rules like S ! NP expect S | NP would hope S, and S ! NP expect S | NP trust S. This allows us to apply sophisticated paraphrases to the predicate while capturing its arguments in a generalized fashion.
      -
         img: figures/ppdb/ppdb-figure-4.jpg
         label: Figure 4
         caption: An illustration of PPDB’s coverage of the manually annotated Propbank predicate phrases (4a) and binary relations with argument non-terminals (4b). The curves indicate the coverage on tokens (solid) and types (dotted), as well as the average number of paraphrases per covered type (dashed) at the given pruning level. (a) PPDB&colon;Eng coverage of Propbank predicates (top), and average human judgment score (bottom) for varying pruning thresholds. (b) PPDB&colon;Eng’s coverage of Propbank predicates with up to two arguments. Here we consider rules that paraphrase the full predicate-argument expression.
   abstract: We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB&colon;Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB&colon;Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff. 
   bibtex: |
      @InProceedings{ganitkevitch-EtAl:2013:NAACL,
         author    = {Juri Ganitkevitch and Benjamin VanDurme and Chris Callison-Burch},
         title     = {{PPDB}: The Paraphrase Database},
         booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)},
         month     = {June},
         year      = {2013},
         address   = {Atlanta, Georgia},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/ppdb.pdf}
       }
       
-
   title: Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals
   authors: Ann Irvine and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf
   page_count: 6
   id: supervised-bilingual-lexicon-induction
   figures:
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-table-1.jpg
         label: Table 1
         caption: Millions of monolingual web crawl and Wikipedia word tokens
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-figure-1.jpg
         label: Figure 1
         caption: Each box-and-whisker plot summarizes performance on the development set using the given feature(s) across all 22 languages. For each source word in our development sets, we rank all English target words according to the monolingual similarity metric(s) listed. All but the last plot show the performance of individual features. Discrim-All uses supervised data to train classifiers for each language based on all of the features.
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-figure-2.jpg
         label: Figure 2
         caption: Performance on the development set goes up as features are greedily added to the feature space. Mean performance is slightly higher using this subset of six features (second to last bar) than using all features (last bar).
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-figure-3.jpg
         label: Figure 3
         caption: Learning curves over number of positive training instances, up to 1250. For some languages, 1250 positive training instances are not available. In all cases, evaluation is on the development data and the number of negative training instances is three times the number of positive. For all languages, performance is fairly stable after about 300 positive training instances.
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-table-2.jpg
         label: Table 2
         caption: Top-10 Accuracy on test set. Performance increases for all languages moving from the baseline (MRR) to discriminative training (Supv).
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-figure-4.jpg
         label: Figure 4
         caption: Millions of monolingual word tokens vs. Lexicon Induction Top-10 Accuracy
   abstract: Prior research into learning translations from source and target language monolingual texts has treated the task as an unsupervised learning problem. Although many techniques take advantage of a seed bilingual lexicon, this work is the first to use that data for supervised learning to combine a diverse set of signals derived from a pair of monolingual corpora into a single discriminative model. Even in a low resource machine translation setting, where induced translations have the potential to improve performance substantially, it is reasonable to assume access to some amount of data to perform this kind of optimization. Our work shows that only a few hundred translation pairs are needed to achieve strong performance on the bilingual lexicon induction task, and our approach yields an average relative gain in accuracy of nearly 50% over an unsupervised baseline. Large gains in accuracy hold for all 22 languages (low and high resource) that we investigate. 
   bibtex: |
      @InProceedings{irvine-callisonburch:2013:NAACL,
         author    = {Ann Irvine and Chris Callison-Burch},
         title     = {Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals},
         booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)},
         month     = {June},
         year      = {2013},
         address   = {Atlanta, Georgia},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf}
       }
       
-
   title: Answer Extraction as Sequence Tagging with Tree Edit Distance
   authors: Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch and Peter Clark
   venue: NAACL
   type: conference
   year: 2013
   url: https://www.cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf
   page_count: 10
   highly_cited: 245
   id: answer-extraction-as-sequence-tagging
   figures:
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-table-1.jpg
         label: Table 1
         caption: Features for ranking QA pairs.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-figure-1.jpg
         label: Figure 1
         caption: Edits transforming a source sentence (left) to a question (right). Each node consists of&colon; lemma, POS tag and dependency relation, with root nodes and punctuation not shown. Shown includes deletion (⇥ and strikethrough on the left), alignment (arrows) and insertion (shaded area). Order of operations is not displayed. The standard TED model does not capture the alignment between tennis and sport (see Section 2.2).
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-table-2.jpg
         label: Table 2
         caption: Distribution of data, with imbalance towards negative examples (sentences without an answer).
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-table-3.jpg
         label: Table 3
         caption: Results on the QA Sentence Ranking task.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-figure-2.jpg
         label: Figure 2
         caption: An example of linear-chain CRF for an- swer sequence tagging.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-table-4.jpg
         label: Table 4
         caption: Features based on edit script for answer se- quence tagging.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-figure-3.jpg
         label: Figure 3
         caption: A sample sequence tagging output that fails to predict an answer. From line 2 on, the first column is the reference output and the second column is the model output with the marginal probability for predicated labels. Note that World War II has much lower probabilities as an O than others.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-figure-4.jpg
         label: Figure 4
         caption: Impact of adding features based on chunking and question-type (CHUNKING) and tree edits (TED), e.g., EDIT and ALIGN.
   abstract: Our goal is to extract answers from pre-retrieved sentences for Question Answering (QA). We construct a linear-chain Conditional Random Field based on pairs of questions and their possible answer sentences, learning the association between questions and answer types. This casts answer extraction as an answer sequence tagging problem for the first time, where knowledge of shared structure between question and source sentence is incorporated through features based on Tree Edit Distance (TED). Our model is free of manually created question and answer templates, fast to run (processing 200 QA pairs per second excluding parsing time), and yields an F1 of 63.3% on a new public dataset based on prior TREC QA evaluations. The developed system is open-source, and includes an implementation of the TED model that is state of the art in the task of ranking QA pairs. 
   bibtex: |
      @InProceedings{yao-EtAl:2013:NAACL,
         author    = {Xuchen Yao and Benjamin {Van Durme} and Chris Callison-Burch and Peter Clark},
         title     = {Answer Extraction as Sequence Tagging with Tree Edit Distance},
         booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)},
         month     = {June},
         year      = {2013},
         address   = {Atlanta, Georgia},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf}
       }
       
-
   title: Findings of the 2012 Workshop on Statistical Machine Translation
   authors: Chris Callison-Burch, Philipp Koehn, Christof  Monz, Matt Post, Radu Soricut, and Lucia Specia
   venue: WMT
   type: workshop
   year: 2012
   url: https://www.cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf
   page_count: 42
   id: findings-of-the-wmt12-shared-tasks
   figures:
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial, online, and rule-based systems were crawled by us, not submitted by the respective companies, and are therefore anonymized. Anonymized identifiers were chosen so as to correspond with the WMT11 systems.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-2.jpg
         label: Table 2
         caption: A summary of the WMT12 ranking task, showing the number of systems and number of labels (rankings) collected for each of the language translation tasks.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-3.jpg
         label: Table 3
         caption: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7).
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-7.jpg
         label: Table 7
         caption: System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average absolute value.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-4.jpg
         label: Table 4
         caption: Official results for the WMT12 translation task. Systems are ordered by their > others score, reflecting how often their translations won in pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-5.jpg
         label: Table 5
         caption: Overall ranking with different methods (English–German)
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-figure-2.jpg
         label: Figure 2
         caption: Ratio of statistically significant pairwise comparisons at different p-levels, based on number of pair-wise judgments collected.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-6.jpg
         label: Table 6
         caption: Participants in the metrics task.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-8.jpg
         label: Table 8
         caption: System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-9.jpg
         label: Table 9
         caption: Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average correlation.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-10.jpg
         label: Table 10
         caption: Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average correlation.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-11.jpg
         label: Table 11
         caption: Participants in the WMT12 Quality Evaluation shared task.
   abstract: This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and evaluated submissions from 11 teams. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2012:WMT,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Monz, Christof  and  Post, Matt  and  Soricut, Radu  and  Specia, Lucia},
         title     = {Findings of the 2012 Workshop on Statistical Machine Translation},
         booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {10--51},
         url = {http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf}
       }
       
-
   title: Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing
   authors: Matt Post, Chris Callison-Burch, and Miles Osborne
   venue: WMT
   type: workshop
   year: 2012
   data: https://github.com/joshua-decoder/indian-parallel-corpora
   url: https://www.cis.upenn.edu/~ccb/publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf
   page_count: 9
   highly_cited: 164
   id: constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing
   figures:
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-figure-1.jpg
         label: Figure 1
         caption: An example of SOV word ordering in Tamil. Translation&colon; The senator prepared her remarks.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-figure-2.jpg
         label: Figure 2
         caption: An example of the morphology of the Bengali word হাত্সিলাম, meaning [I] was walking. CONT denotes the continuous aspect, while PAST denotes past tense.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-3.jpg
         label: Table 3
         caption: Dictionary statistics. Entries is the number of source-language types, while translations lists the number of words or phrases they translated to (i.e., the number of pairs in the dictionary). Controls for Hindi were obtained using Google translate, the only one of these languages that were available at the outset of this project.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-figure-3.jpg
         label: Figure 3
         caption: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-4.jpg
         label: Table 4
         caption: Data set sizes for each language pair&colon; words in the first row, parallel sentences in the second. (The dictionaries contains short phrases in addition to words, which accounts for the difference in dictionary word and line counts.)
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-5.jpg
         label: Table 5
         caption: BLEU scores translating into English (four references). BLEU scores are the mean of three MERT runs.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-6.jpg
         label: Table 6
         caption: Some example translations.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-7.jpg
         label: Table 7
         caption: BLEU scores translating into English on a quarter of the training data (plus dictionary), selected in two ways&colon; best (result of vote), and random. There is little difference, suggesting quality control may not be terribly important. We did not collect votes for Malayalam.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-8.jpg
         label: Table 8
         caption: Misspellings of japanese (947) in the training portion of the Urdu-English data, along with their counts.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-9.jpg
         label: Table 9
         caption: Hiero translation results using Berkeley align- ments instead of GIZA++ heuristics. The gain columns denotes improvements relative to the Hiero systems in Ta- ble 5. In many cases (bold gains), the BLEU scores are at or above even the SAMT models from that table.
   abstract: Recent work has established the efficacy of Amazon's Mechanical Turk for constructing parallel corpora for machine translation research. We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent&colon; Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu. These languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation. We conduct a variety of baseline experiments and analysis, and release the data to the community. 
   bibtex: |
      @InProceedings{post-callisonburch-osborne:2012:WMT,
         author    = {Post, Matt  and  Callison-Burch, Chris  and  Osborne, Miles},
         title     = {Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing},
         booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {401--409},
         url       = {http://www.aclweb.org/anthology/W12-3152}
       }
       
-
   title: Using Categorial Grammar to Label Translation Rules
   authors: Jonathan Weese, Chris Callison-Burch, and Adam Lopez
   venue: WMT
   type: workshop
   year: 2012
   url: https://www.cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf
   page_count: 10
   id: using-categorial-grammar-to-label-translation-rules
   figures:
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-table-1.jpg
         label: Table 1
         caption: An example lexicon, mapping words to categories.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-1.jpg
         label: Figure 1
         caption: An example CCG derivation for the sentence “They own properties in various cities and villages” using the lexicon from Table 1. Φ indicates a conjunction operation; > and < are forward and backward function application, respectively.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-2.jpg
         label: Figure 2
         caption: A word-aligned sentence pair fragment, with a box indicating a consistent phrase pair.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-3.jpg
         label: Figure 3
         caption: A consistent phrase pair with a sub-phrase that is also consistent. We may extract a hierarchical SCFG rule from this training example.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-4.jpg
         label: Figure 4
         caption: A portion of the parse chart for a sentence starting with “For most people . . . .” Note that the gray chart cell is not included in the 1-best derivation of this fragment in Section 3.5.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-table-2.jpg
         label: Table 2
         caption: Number of translation rules and non- terminal labels in an Urdu–English grammar under various models.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-5.jpg
         label: Figure 5
         caption: Histograms of label frequency for each model, illustrating the sparsity of each model.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-table-3.jpg
         label: Table 3
         caption: Results of translation experiments on Urdu–English. Higher BLEU scores are better. BLEU’s brevity penalty is reported in parentheses.
   abstract: Adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in GHKM (Galley et al., 2004), excludes potentially useful translation rules. SAMT (Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. We show that our labeling scheme performs comparably to SAMT on an Urdu–English translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast. 
   bibtex: |
      @InProceedings{weese-callisonburch-lopez:2012:WMT,
         author    = {Weese, Jonathan  and  Callison-Burch, Chris  and  Lopez, Adam},
         title     = {Using Categorial Grammar to Label Translation Rules},
         booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {222--231},
         url = {http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf}
       }
       
-
   title: Joshua 4.0&colon; Packing, PRO, and Paraphrases
   authors: Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post, and Chris Callison-Burch
   venue: WMT
   type: workshop
   year: 2012
   url: https://www.cis.upenn.edu/~ccb/publications/joshua-4.0.pdf
   page_count: 9
   id: joshua-4
   figures:
      -
         img: figures/joshua-4.0/joshua-4.0-figure-1.jpg
         label: Figure 1
         caption: An illustration of our packed grammar data structures. The source sides of the grammar rules are stored in a packed trie. Each node may contain n children and the symbols linking to them, and m entries for rules that share the same source side. Each rule entry links to a node in the target-side trie, where the full target string can be retrieved by walking up the trie until the root is reached. The rule entries also contain a data block id, which identifies feature data attached to the rule. The features are encoded according to a type/quantization specification and stored as variable-length blocks of data in a byte buffer.
      -
         img: figures/joshua-4.0/joshua-4.0-table-1.jpg
         label: Table 1
         caption: Decoding-time memory use for the packed grammar versus the standard grammar format. Even without lossy quantization the packed grammar representation yields significant savings in memory consumption. Adding 8-bit quantization for the real- valued features in the grammar reduces even large syntactic grammars to a manageable size.
      -
         img: figures/joshua-4.0/joshua-4.0-figure-2.jpg
         label: Figure 2
         caption: A visualization of the loading and decoding speed on the WMT12 French-English development set contrasting the packed grammar representation with the standard format. Grammar loading for the packed grammar representation is substantially faster than that for the baseline setup. Even with a slightly slower decoding speed (note the difference in the slopes) the packed grammar finishes in less than half the time, compared to the standard format.
      -
         img: figures/joshua-4.0/joshua-4.0-figure-3.jpg
         label: Figure 3
         caption: Experimental results on the development and test sets. The x-axis is the number of iterations (up to 30) and the y-axis is the BLEU score. The three curves in each figure correspond to three classifiers. Upper row&colon; results trained using only dense features (10 features); Lower row&colon; results trained using dense+sparse features (1026 features). Left column&colon; development set (MT03); Middle column&colon; test set (MT04); Right column&colon; test set (MT05).
      -
         img: figures/joshua-4.0/joshua-4.0-table-2.jpg
         label: Table 2
         caption: Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).
      -
         img: figures/joshua-4.0/joshua-4.0-table-3.jpg
         label: Table 3
         caption: Extraction times and grammar sizes for Hiero grammars using the Europarl and News Commentary training data for each listed language pair.
      -
         img: figures/joshua-4.0/joshua-4.0-table-4.jpg
         label: Table 4
         caption: Extraction times and grammar sizes for the SAMT grammars using the Europarl and News Commentary training data for each listed language pair.
      -
         img: figures/joshua-4.0/joshua-4.0-table-5.jpg
         label: Table 5
         caption: Large paraphrase grammars extracted from EuroParl data using Thrax. The sentence and word counts refer to the English side of the bitexts used.
   abstract: We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. The main contributions in this release are the introduction of a compact grammar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 
   bibtex: |
      @InProceedings{ganitkevitch-EtAl:2012:WMT,
         author    = {Ganitkevitch, Juri  and  Cao, Yuan  and  Weese, Jonathan  and  Post, Matt  and  Callison-Burch, Chris},
         title     = {Joshua 4.0: Packing, PRO, and Paraphrases},
         booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {283--291},
         url = {http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf}
       }
       
-
   title: Expectations of Word Sense in Parallel Corpora
   authors: Xuchen Yao, Benjamin Van Durme and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2012
   url: https://www.cis.upenn.edu/~ccb/publications/expectations-of-word-sense-in-parallel-corpora.pdf
   page_count: 5
   id: expectations-of-word-sense-in-parallel-corpora
   figures:
      -
         img: figures/expectations-of-word-sense-in-parallel-corpora/expectations-of-word-sense-in-parallel-corpora-table-1.jpg
         label: Table 1
         caption: MTurk result on testing Turker reliability. Krippendorff’s Alpha is used to measure agreement. ↵- Turker&colon; how Turkers agree among themselves, ↵-maj.&colon; how the majority agrees with true value, maj.-agr.&colon; agreement between the majority vote and true value. ↵-maj. indicates the confidence level about the maj.-agr. value. Subscripts denote either 5 Turkers, or 3 randomly selected of the 5.
      -
         img: figures/expectations-of-word-sense-in-parallel-corpora/expectations-of-word-sense-in-parallel-corpora-table-2.jpg
         label: Table 2
         caption: Statistics for words sampled from parallel corpora. Average #senses observed over all words&colon; 2.6 (French-English), and 2.4 (Chinese-English). The sampled word keep has 18 senses in OntoNotes, with 5 observed.
      -
         img: figures/expectations-of-word-sense-in-parallel-corpora/expectations-of-word-sense-in-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: French-English values, by number of senses.
      -
         img: figures/expectations-of-word-sense-in-parallel-corpora/expectations-of-word-sense-in-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: French-English values, by number of senses.
   abstract: Given a parallel corpus, if two distinct words in language A, a and a2, are aligned to the same word b in language B, then this might signal that b is polysemous, or it might signal a and a2 are synonyms. Both assumptions with successful work have been put forward in the literature. We investigate these assumptions, along with other questions of word sense, by looking at sampled parallel sentences containing tokens of the same type in English, asking how often they mean the same thing when they are&colon; 1. aligned to the same foreign type; and 2. aligned to different foreign types. Results for French-English and Chinese-English parallel corpora show similar behavior&colon; Synonymy is only very weakly the more prevalent scenario, where both cases regularly occur. 
   bibtex: |
      @InProceedings{yao-vandurme-callisonburch:2012:NAACL-HLT,
         author    = {Yao, Xuchen, {Van Durme}, Benjamin and Callison-Burch, Chris},
         title     = {Expectations of Word Sense in Parallel Corpora},
         booktitle = {The 2012 Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {621--625},
         url       = {http://www.aclweb.org/anthology/N12-1078}
       }
       
       
-
   title: Processing Informal, Romanized Pakistani Text Messages
   authors: Ann Irvine, Jonathan Weese, and Chris Callison-Burch
   venue: the NAACL Workshop on Language in Social Media
   type: workshop
   year: 2012
   url: https://www.cis.upenn.edu/~ccb/publications/pakistani-SMS-corpus.pdf
   page_count: 4
   id: pakistani-SMS-corpus
   figures:
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-figure-1.jpg
         label: Figure 1
         caption: Example of SMS with MTurk annotations
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-figure-2.jpg
         label: Figure 2
         caption: Productivity vs. percent of annotations voted best among three deromanizations gathered on MTurk.
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-figure-3.jpg
         label: Figure 3
         caption: Urdu words romanized in multiple ways. The Urdu word for “2” is pronounced approximately “du.”
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-figure-4.jpg
         label: Figure 4
         caption: Illustration of HMM with an example from SMS data. English translation&colon; “What’s the situation?”
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-table-1.jpg
         label: Table 1
         caption: Deromanization and normalization results on 500 SMS test set. Evaluation is by character (CER) and word error rate (WER); lower scores are better. “LM” indicates the data used to estimate the language model probabilities&colon; News refers to Urdu news corpus and SMS to deromanized side of our SMS training data. “Translit” column refers to the training data that was used to train the transliterator&colon; SMS; SMS training data; Eng; English-Urdu transliterations. α refers to the data used to estimate emissions&colon; transliterations, dictionary entries, or both.
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-table-2.jpg
         label: Table 2
         caption: Results on tokens in the test set, binned by training frequency or difference in character length with their reference. Length differences are number of characters in romanized token minus the number of characters in its deromanization. α = 0.5 for all.
   abstract: Regardless of language, the standard character set for text messages (SMS) and many other social media platforms is the Roman alphabet. There are romanization conventions for some character sets, but they are used inconsistently in informal text, such as SMS. In this work, we convert informal, romanized Urdu messages into the native Arabic script and normalize non-standard SMS language. Doing so prepares the messages for existing downstream processing tools, such as machine translation, which are typically trained on well-formed, native script text. Our model combines information at the word and character levels, allowing it to handle out-of-vocabulary items. Compared with a baseline deterministic approach, our system reduces both word and character error rate by over 50%. 
   bibtex: |
      @InProceedings{irvine-weese-callisonburch:2012:LSM,
         author    = {Irvine, Ann  and  Weese, Jonathan  and  Callison-Burch, Chris},
         title     = {Processing Informal, Romanized Pakistani Text Messages},
         booktitle = {Proceedings of the Second Workshop on Language in Social Media},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {75--78},
         url       = {http://www.aclweb.org/anthology/W12-2109}
       }
       
-
   title: Monolingual Distributional Similarity for Text-to-Text Generation
   authors: Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch
   venue: STARSEM
   type: conference
   year: 2012
   url: https://www.cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf
   page_count: 9
   id: monolingual-distributional-similarity-for-text-to-text-generation
   figures:
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-1.jpg
         label: Figure 1
         caption: Pivot-based paraphrase extraction for contiguous phrases. Two phrases translating to the same phrase in the foreign language are assumed to be paraphrases of one another.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-2.jpg
         label: Figure 2
         caption: Extraction of syntactic paraphrases via the pivoting approach&colon; We aggregate over different sur- face realizations, matching the lexicalized portions of the rule and generalizing over the nonterminals.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-3.jpg
         label: Figure 3
         caption: An example of a synchronous paraphrastic derivation, here a sentence compression. Shaded words are deleted in the indicated rule applications.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-4.jpg
         label: Figure 4
         caption: Scoring a rule by extracting and scoring contiguous phrases consistent with the alignment. The overall score of the rule is determined by averaging across all pairs of contiguous subphrases
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-5.jpg
         label: Figure 5
         caption: An example of the n-gram feature extraction on an n-gram corpus. Here, “the long-term” is seen preceded by “revise” (43 times) and followed by “plans” (97 times). The corresponding left- and right-side features are added to the phrase signature with the counts of the n-grams that gave rise to them.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-6.jpg
         label: Figure 6
         caption: An example of the syntactic feature-set. The phrase “the long-term” is annotated with position-aware lexical and part-of-speech n-gram features (e.g. “on to” on the left, and “investment” and “NN” to its right), labeled dependency links (e.g. amod − investment) and features derived from the phrase’s CCG label NP /NN .
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-table-1.jpg
         label: Table 1
         caption: Results of the human evaluation on longer compressions&colon; pairwise compression rates (CR), meaning and grammaticality scores. Bold indicates a statistically significance difference at p < 0.05.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-7.jpg
         label: Figure 7
         caption: A pairwise breakdown of the human judgments comparing the systems. Dark grey regions show the number of times the two systems were tied, and light grey shows how many times one system was judged to be better than the other.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-table-2.jpg
         label: Table 2
         caption: Example compressions produced by our systems and the baselines Table 1 for three input sentences from our test data.
   abstract: Previous work on paraphrase extraction and application has relied on either parallel datasets, or on distributional similarity metrics over large text corpora. Our approach combines these two orthogonal sources of information and directly integrates them into our paraphrasing system’s log-linear model. We compare different distributional similarity feature-sets and show significant improvements in grammaticality and meaning retention on the example text-to-text generation task of sentence compression, achieving state-of-the-art quality. 
   bibtex: |
      @InProceedings{Ganitkevitch-etal:2012:StarSEM,
         author =  {Juri Ganitkevitch and Benjamin {Van Durme} and Chris Callison-Burch},
         title     = {Monolingual Distributional Similarity for Text-to-Text Generation},
         booktitle = {*SEM First Joint Conference on Lexical and Computational Semantics},
         month     = {June},
         year      = {2012},
         address   = {Montreal},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf}
       }
       
-
   title: Machine Translation of Arabic Dialects
   authors: Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Makhoul, Omar F. Zaidan and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2012
   url: https://www.cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf
   page_count: 11
   highly_cited: 202
   id: machine-translation-of-arabic-dialects
   figures:
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-figure-1.jpg
         label: Figure 1
         caption: One possible breakdown of spoken Arabic into dialect groups&colon; Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-1.jpg
         label: Table 1
         caption: The total costs for the three MTurk subtasks involved with the creation of our Dialectal Arabic-English parallel corpus.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-2.jpg
         label: Table 2
         caption: Statistics about the training/tuning/test datasets used in our experiments. The token counts are calculated before MADA segmentation.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-3.jpg
         label: Table 3
         caption: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-4.jpg
         label: Table 4
         caption: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora. The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-5.jpg
         label: Table 5
         caption: The most frequent OOV’s (with counts ≥ 10) of our test sets against the MSA training data.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-figure-2.jpg
         label: Figure 2
         caption: Examples of improvement in MT output when training on our Dialectal Arabic-English parallel corpus instead of an MSA-English parallel corpus.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-figure-3.jpg
         label: Figure 3
         caption: Examples of ambiguous words that are translated incorrectly by the MSA-English system, but correctly by the Dialectal Arabic-English system.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-figure-4.jpg
         label: Figure 4
         caption: Learning curves showing the effects of increasing the size of dialectal training data, when combined with the 150M-word MSA parallel corpus, and when used alone. Adding the MSA training data is only useful when the dialectal data is scarce (200k words).
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-6.jpg
         label: Table 6
         caption: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable&colon; +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-7.jpg
         label: Table 7
         caption: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English, versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system, the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
   abstract: Arabic dialects present many challenges for machine translation, not least of which is the lack of data resources. We use crowdsourcing to cheaply and quickly build Levantine-English and Egyptian-English parallel corpora, consisting of 1.1M words and 380k words, respectively. The dialect sentences are selected from a large corpus of Arabic web text, and translated using Mechanical Turk. We use this data to build Dialect Arabic MT systems. Small amounts of dialect data have a dramatic impact on the quality of translation. When translating Egyptian and Levantine test sets, our Dialect Arabic MT system performs 5.8 and 6.8 BLEU points higher than a Modern Standard Arabic MT system trained on a 150 million word Arabic-English parallel corpus -- over 100 times the amount of data as our dialect corpora. 
   bibtex: |
      @InProceedings{Zbib-etal:2012:NAACL,
         author =  {Rabih Zbib and Erika Malchiodi and Jacob Devlin and David Stallard and Spyros Matsoukas and Richard Schwartz and John Makhoul and Omar F. Zaidan and Chris Callison-Burch},
         title     = {Machine Translation of Arabic Dialects},
         booktitle = {The 2012 Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2012},
         address   = {Montreal},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf}
       }
       
-
   title: Toward Statistical Machine Translation without Parallel Corpora
   authors: Alex Klementiev, Ann Irvine, Chris Callison-Burch, and David Yarowsky
   venue: EACL
   type: conference
   year: 2012
   url: https://www.cis.upenn.edu/~ccb/publications/toward-statistical-machine-translation-without-parallel-corpora.pdf
   page_count: 11
   id: toward-statistical-machine-translation-without-parallel-corpora
   figures:
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: The reordering probabilities from the phrasebased models are estimated from bilingual data by calculating how often in the parallel corpus a phrase pair (f; e) is orientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous).
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: Accuracy of single-word translations induced using contextual similarity as a function of the source word corpus frequency. Accuracy is the proportion of the source words with at least one correct (bilingual dictionary) translation in the top 1 and top 10 candidate lists.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: Scoring contextual similarity of phrases&colon; first, contextual vectors are projected using a small seed dictionary and then compared with the target language candidates.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-4.jpg
         label: Figure 4
         caption: Temporal histograms of the English phrase terrorist, its Spanish translation terrorista, and riqueza (wealth) collected from monolingual texts spanning a 13 year period. While the correct translation has a good temporal match, the non-translation riqueza has a distinctly different signature.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-5.jpg
         label: Figure 5
         caption: Algorithm for estimating reordering probabilities from monolingual data.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-6.jpg
         label: Figure 6
         caption: Collecting phrase orientation statistics for a English-German phrase pair (“profile”, “Profils”) from non-parallel sentences (the German sentence translates as “Creating a Facebook profile is easy”).
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-table-1.jpg
         label: Table 1
         caption: Statistics about the monolingual training data and the phrase table that was used in all of the experiments.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-7.jpg
         label: Figure 7
         caption: Much of the loss in BLEU score when bilingually estimated features are removed from a Spanish-English translation system (experiments 1-4) can be recovered when they are replaced with monolingual equivalents estimated from monolingual Europarl data (experiments 5-10). The labels indicate how the different types of parameters are estimated, the first part is for phrase-table features, the second is for reordering probabilities.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-8.jpg
         label: Figure 8
         caption: Performance of monolingual features derived from truly monolingual corpora. Over 82% of the BLEU score loss can be recovered.
   abstract: We estimate the parameters of a phrase-based statistical machine translation system from monolingual corpora instead of a bilingual parallel corpus. We extend existing research on bilingual lexicon induction to estimate both lexical and phrasal translation probabilities for MT-scale phrase-tables. We propose a novel algorithm to estimate re-ordering probabilities from monolingual data. We report translation results for an end-to-end translation system using these monolingual features alone. Our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights. In this paper, we examine an idealization where a phrase-table is given. We examine the degradation in translation performance when bilingually estimated translation probabilities are removed, and show that 82%+ of the loss can be recovered with monolingually estimated features alone. We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features. 
   bibtex: |
      @InProceedings{klementiev-etal:2012:EACL,
         author =  {Alex Klementiev and Ann Irvine and Chris Callison-Burch and David Yarowsky},
         title     = {Toward Statistical Machine Translation without Parallel Corpora},
         booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics},
         month     = {April},
         year      = {2012},
         address   = {Avignon, France}
         publisher = {Association for Computational Linguistics},
       }
       
-
   title: Use of Modality and Negation in Semantically-Informed Syntactic MT
   authors: Kathryn Baker, Bonnie Dorr, Michael Bloodgood, Chris Callison-Burch, Wes Filardo, Christine Piatko, Lori Levin, and Scott Miller
   venue: Computational Linguistics
   type: journal
   year: 2012
   url: https://www.cis.upenn.edu/~ccb/publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf
   page_count: 28
   id: modality-and-negation-in-semantically-informed-syntactic-mt
   figures:
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-1.jpg
         label: Figure 1
         caption: Modality/Negation Tagging Examples
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-2.jpg
         label: Figure 2
         caption: An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a phrase-based model (Moses) without linguistic information, which is representative of state-of-the-art MT quality before the SIMT effort
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-table-1.jpg
         label: Table 1
         caption: The size of the various data sets used for the experiments in this paper including the training, development (dev), incremental test set (devtest) and blind test set (test). The dev/devtest was a split of the NIST08 Urdu-English test set, and the blind test set was NIST09.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-3.jpg
         label: Figure 3
         caption: The evolution of a semantically informed approach to our synchronous context free grammars (SCFGs). At the start of summer the decoder used translation rules with a single generic non-terminal symbol, later syntactic categories were used, and by the end of the summer the translation rules included semantic elements such as modalities and negation, as well as named entities.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-4.jpg
         label: Figure 4
         caption: Eight Modalities Used for Tagging. H stands for the Holder of the modality, and P is the proposition over which the modality has scope.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-5.jpg
         label: Figure 5
         caption: Thirteen Menu Choices for Modality/Negation Annotation
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-6.jpg
         label: Figure 6
         caption: Modality Lexicon Entry for need
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-7.jpg
         label: Figure 7
         caption: Sample output from the structure-based MN tagger
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-8.jpg
         label: Figure 8
         caption: Example of Embedded Target Head found inside VP must be found
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-9.jpg
         label: Figure 9
         caption: Example of Modality Composed with Negation&colon; TrigAble and TrigNegation combine to form NOTAble
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-10.jpg
         label: Figure 10
         caption: A sentence on the English side of the bilingual parallel training corpus is parsed with a syntactic parser, and also tagged with our modality tagger. The tags are then grafted onto the syntactic parse tree to form new categories like VP-TargNOTAble and VP-TargRequire. Grafting happens prior to extracting translation rules, which happens normally except for the use of the augmented trees.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-11.jpg
         label: Figure 11
         caption: Example translation rules with tags for modality, negation, and entities combined with syntactic categories.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-table-2.jpg
         label: Table 2
         caption: Modality Tags with their Negated Versions. Note that Require and Permit are in a dual relation, and thus RequireNegation is represented as NOTPermit and PermitNegation is represented as NOTRequire.
   abstract: This article describes the resource- and system-building efforts of an eight-week JHU Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation (SIMT). We describe a new modality/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two automated MN taggers that we built using the annotation scheme and lexicon. Our annotation scheme isolates three components of modality and negation&colon; a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation) and a holder (an experiencer of modality). We describe how our MN lexicon was produced semi-automatically and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data set. We apply our MN annotation scheme to statistical machine translation using a syntactic framework that supports the inclusion of semantic annotations. Syntactic tags enriched with semantic annotations are assigned to parse trees in the target-language training texts through a process of tree grafting. While the focus of our work is modality and negation, the tree grafting procedure is general and supports other types of semantic information. We exploit this capability by including named entities, produced by a pre-existing tagger, in addition to the MN elements produced by the taggers described in this paper. The resulting system significantly outperformed a linguistically naïve baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English test set. This finding supports the hypothesis that both syntactic and semantic information can improve translation quality. 
   bibtex: |
      @article{baker-etal:2012:CL,
         author =  {Kathryn Baker and Bonnie Dorr and Michael Bloodgood and Chris Callison-Burch and Nathaniel Filardo and Christine Piatko and Lori Levin and Scott Miller},
         title =   {Use of Modality and Negation in Semantically-Informed Syntactic MT},
         journal = {Computational Linguistics},
         year =    {2012},
         volume = {38},
         number = {2},
         pages = {411-438}
       }
       
-
   title: Findings of the 2011 Workshop on Statistical Machine Translation
   authors: Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan
   venue: WMT
   type: workshop
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/findings-of-the-wmt11-shared-tasks.pdf
   page_count: 43
   id: findings-of-the-wmt11-shared-tasks
   figures:
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task (European language pairs; individual system track). Not all teams participated in all language pairs. The translations from commercial and online systems were crawled by us, not submitted by the respective companies, and are therefore anonymized.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-2.jpg
         label: Table 2
         caption: Participants in the shared system combination task. Not all teams participated in all language pairs. ∗ The Quaero Project entry combined outputs they received directly from LIMSI, KIT, SYSTRAN, and RWTH.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-3.jpg
         label: Table 3
         caption: Participants in the featured translation task (Haitian Creole SMS into English; individual system track). Not all teams participated in both the ‘Clean’ and ‘Raw’ tracks.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-4.jpg
         label: Table 4
         caption: Examples of some of the Haitian Creole SMS messages that were sent to the 4636 short code along with their translations into English. Translations were done by volunteers who wanted to help with the relief effort. Prior to being distributed, the messages were anonymized to remove names, phone numbers, email addresses, etc. The anonymization guidelines specified that addresses be retained to facilitate work on mapping technologies.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-5.jpg
         label: Table 5
         caption: Training data for the Haitian Creole-English featured translation task. The in-domain SMS data consists primarily of raw (noisy) SMS data. The in-domain data was provided by Mission 4636. The other data is out-of- domain. It comes courtesy of Carnegie Mellon University, Microsoft Research, Haitisurf.com, and Krengle.net.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-6.jpg
         label: Table 6
         caption: A summary of the WMT11 ranking task, showing the number of systems and number of labels collected in each of the individual and system combination tracks. The system count does not include the reference translation, which was included in the evaluation, and so a value under “Labels per System” can be obtained only after adding 1 to the system count, before dividing the label count (e.g. in German-English, 4, 620/21 = 220.0).
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-11.jpg
         label: Table 11
         caption: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines.
   abstract: This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot ‘tunable metrics’ task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2011:WMT,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Monz, Christof  and  Zaidan, Omar},
         title     = {Findings of the 2011 Workshop on Statistical Machine Translation},
         booktitle = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
         month     = {July},
         year      = {2011},
         address   = {Edinburgh, Scotland},
         publisher = {Association for Computational Linguistics},
         pages     = {22--64},
         url       = {http://www.aclweb.org/anthology/W11-2103}
       }
       
-
   title: Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation
   authors: Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme
   venue: EMNLP
   type: conference
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf
   page_count: 12
   id: learning-sentential-paraphrases-from-bilingual-parallel-corpora
   figures:
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: Synchronous grammar rules for translation are extracted from sentence pairs in a bixtext which have been automatically parsed and word-aligned. Extraction methods vary on whether they extract only minimal rules for phrases dominated by nodes in the parse tree, or more complex rules that include non-constituent phrases.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: An example derivation produced by a syntactic machine translation system. Although the synchronous trees are unlike the derivations found in the Penn Treebank, their yield is a good translation of the German.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: An example of a synchronous paraphrastic derivation. A few of the rules applied in the parse are show in the left column, with the pivot phrases that gave rise to them on the right.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-1.jpg
         label: Table 1
         caption: A selection of meaning-preserving transformations and hand-picked examples of syntactic paraphrases that our system extracts capturing these.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-2.jpg
         label: Table 2
         caption: Number and distribution of rules in our para- phrase grammar. Note the significant number of identity paraphrases and rules with complex nonterminal labels.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-4.jpg
         label: Table 4
         caption: Human evaluation for shorter compressions and for variations of our paraphrase system. +Feat. includes the compression features from Section 6.1, +Aug. includes optional deletion rules from Section 6.4.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-3.jpg
         label: Table 3
         caption: Results of the human evaluation on longer compressions&colon; pairwise compression rates (CR), meaning and grammaticality scores. Bold indicates a statistically significance difference at p < 0.05.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-5.jpg
         label: Table 5
         caption: Example compressions produced by the two systems in Table 3 for three input sentences from our test data.
   abstract: Previous work has shown that high quality phrasalparaphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 
   bibtex: |
      @InProceedings{ganitkevitch-EtAl:2011:EMNLP,
         author    = {Ganitkevitch, Juri  and  Callison-Burch, Chris  and  Napoles, Courtney  and  {Van Durme}, Benjamin},
         title     = {Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation},
         booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
         month     = {July},
         year      = {2011},
         address   = {Edinburgh, Scotland, UK.},
         publisher = {Association for Computational Linguistics},
         pages     = {1168--1179},
         url       = {http://www.aclweb.org/anthology/D11-1108}
       }
       
-
   title: Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity
   authors: Charley Chan, Chris Callison-Burch, and Benjamin Van Durme
   venue: GEMS
   type: workshop
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf
   page_count: 10
   id: reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity
   figures:
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-1.jpg
         label: Figure 1
         caption: Using a bilingual parallel corpus to extract paraphrases.
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-table-1.jpg
         label: Table 1
         caption: Paraphrases for huge amount of according to the bilingual pivoting (BiP), syntactic-constrainted bilingual pivoting (SyntBiP) translation score and the monolingual similarity score via LSH (MonoDS), ranked by corresponding scores listed next to each paraphrase. Syntactic type of the phrase is [JJ+NN+IN].
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-table-2.jpg
         label: Table 2
         caption: Ordered reranked paraphrase candidates for the phrase reluctant according to monolingual distributional similarity (MonoDShand−selected) and bilingual pivoting paraphrase (BiP) method. Two hand-selected phrases are labeled with asterisks.
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-table-3.jpg
         label: Table 3
         caption: Kendall’s Tau rank correlation coefficients be- tween human judgment of meaning and grammaticality for the different paraphrase scoring methods. Bottom panel&colon; SyntBiPmatched is the same as SyntBiP except paraphrases must match with the original phrase in syn- tactic type. SyntBiP* and MonoDS* are the same as before except they share the same phrase support with SyntBiPmatched . (‡&colon; MonoDS outperforms the corresponding BiP reranking at p-value ≤0.01, and † at ≤0.05)
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-2.jpg
         label: Figure 2
         caption: Averaged scores in the top K paraphrase candidates as a function of K for different reranking metrics. All methods performs similarly in meaning preservation, but SyntBiP-MonoDS outperforms other scoring methods in grammaticality, as shown in the bottom graph.
   abstract: This paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases. Raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivot-based methods. Our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a significantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does. The results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection. 
   bibtex: |
      @InProceedings{chan-callisonburch-vandurme:2011:GEMS,
         author    = {Chan, Tsz Ping  and  Callison-Burch, Chris  and  {Van Durme}, Benjamin},
         title     = {Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity},
         booktitle = {Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics},
         month     = {July},
         year      = {2011},
         address   = {Edinburgh, UK},
         publisher = {Association for Computational Linguistics},
         pages     = {33--42},
         url       = {http://www.aclweb.org/anthology/W11-2504}
       }
       
-
   title: Joshua 3.0&colon; Syntax-based Machine Translation with the Thrax Grammar Extractor
   authors: Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post and Adam Lopez
   venue: WMT
   type: workshop
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/joshua-3.0.pdf
   page_count: 7
   id: joshua-3
   figures:
      -
         img: figures/joshua-3.0/joshua-3.0-figure-1.jpg
         label: Figure 1
         caption: An aligned sentence pair.
      -
         img: figures/joshua-3.0/joshua-3.0-table-1.jpg
         label: Table 1
         caption: A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.
      -
         img: figures/joshua-3.0/joshua-3.0-table-2.jpg
         label: Table 2
         caption: Training data size after subsampling.
      -
         img: figures/joshua-3.0/joshua-3.0-table-3.jpg
         label: Table 3
         caption: Single-reference BLEU-4 scores.
      -
         img: figures/joshua-3.0/joshua-3.0-figure-2.jpg
         label: Figure 2
         caption: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals and non-terminals. The unshaded terminals are directly dominated by a nonterminal symbol.
   abstract: We present progress on Joshua, an open source decoder for hierarchical and syntax-based machine translation. The main focus is describing Thrax, a flexible, open source synchronous context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats. 
   bibtex: |
      @InProceedings{weese-EtAl:2011:WMT,
         author    = {Weese, Jonathan  and  Ganitkevitch, Juri  and  Callison-Burch, Chris  and  Post, Matt  and  Lopez, Adam},
         title     = {Joshua 3.0: Syntax-based Machine Translation with the Thrax Grammar Extractor},
         booktitle = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
         month     = {July},
         year      = {2011},
         address   = {Edinburgh, Scotland},
         publisher = {Association for Computational Linguistics},
         pages     = {478--484},
         url       = {http://www.aclweb.org/anthology/W11-2160}
       }
       
-
   title: WikiTopics&colon; What is Popular on Wikipedia and Why
   authors: Byung Gyu Ahn, Ben Van Durme and Chris Callison-Burch
   venue: ACL Workshop on Automatic Summarization for Different Genres, Media, and Languages
   type: workshop
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf
   page_count: 8
   id: wikitopics-what-is-popular-on-wikipedia-and-why
   figures:
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-1.jpg
         label: Figure 1
         caption: Automatically selected articles for Jan 27, 2009.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-2.jpg
         label: Figure 2
         caption: Process diagram&colon; (a) Topic selection&colon; select interesting articles based on increase in pageviews. (b) Clustering&colon; cluster the articles according to relevant events using topic models or Wikipedia’s hyperlink structure. (c) Textualization&colon; select the sentence that best summarizes the relevant event
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-3.jpg
         label: Figure 3
         caption: Pageviews for all the hand-curated articles related to the inauguration of Barack Obama. Pageviews spike on the same day as the event took place–January 20, 2009.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-4.jpg
         label: Figure 4
         caption: Log ratio of the increase in pageviews&colon; log􏰅i = 115dik/􏰅i = 1630. Zero means no change in pageviews. WikiTopics articles show pageviews increase in a few orders of magnitude as opposed to hand-curated articles.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-5.jpg
         label: Figure 5
         caption: Illustrative articles for January 27, 2009. WikiTopics articles here do not appear in hand-curated articles within fifteen days before or after, and vice versa. The hand-curated articles shown here are all linked from a single event “Florida hedge fund manager Arthur Nadel is arrested by the United States Federal Bureau of Investigation and charged with fraud.”
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-table-1.jpg
         label: Table 1
         caption: Clustering evaluation&colon; F-scores are averaged across gold standard datasets. ConComp and OneHop are using the link structure. K-means clustering with tf-idf performs best. Manual clusters were evaluated against those of the other two annotators to determine inter-annotator agreement.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-6.jpg
         label: Figure 6
         caption: Examples of clusters&colon; K-means clustering on the articles of January 27, 2009 and May 12, 2009. The centroid article for each cluster, defined as the closest article to the center of the cluster in vector space, is in bold.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-7.jpg
         label: Figure 7
         caption: Selected examples of temporal expressions identified by Serif from 247 such date and time expressions extracted from the article Abraham Lincoln.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-table-2.jpg
         label: Table 2
         caption: Textualization&colon; evaluation results of sentence selection schemes. Self fallback scheme first tries to select the best sentence as the Self scheme, and if it fails to select one it falls back to the Recent scheme.
   abstract: We establish a novel task in the spirit of news summarization and topic detection and tracking (TDT)&colon; daily determination of the topics newly popular with Wikipedia readers. Central to this effort is a new public dataset consisting of the hourly page view statistics of all Wikipedia articles over the last three years. We give baseline results for the tasks of&colon; discovering individual pages of interest, clustering these pages into coherent topics, and extracting the most relevant summarizing sentence for the reader. When compared to human judgements, our system shows the viability of this task, and opens the door to a range of exciting future work. 
   bibtex: |
      @InProceedings{ahn-vandurme-callisonburch:2011:SummarizationWorkshop,
         author    = {Ahn, Byung Gyu  and  {Van Durme}, Benjamin  and  Callison-Burch, Chris},
         title     = {WikiTopics: What is Popular on Wikipedia and Why},
         booktitle = {Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon},
         publisher = {Association for Computational Linguistics},
         pages     = {33--40},
         url       = {http://www.aclweb.org/anthology/W11-0505}
       }
       
-
   title: Evaluating sentence compression&colon; Pitfalls and suggested remedies
   authors: Courtney Napoles, Ben Van Durme
   venue: Workshop on Monolingual Text-To-Text Generation
   type: workshop
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/evaluating-sentence-compression-pitfalls-and-suggested-remedies.pdf
   page_count: 7
   id: evaluating-sentence-compression-pitfalls-and-suggested-remedies
   figures:
      -
         img: figures/evaluating-sentence-compression-pitfalls-and-suggested-remedies/evaluating-sentence-compression-pitfalls-and-suggested-remedies-table-1.jpg
         label: Table 1
         caption: Three acceptable compressions of a sentence created by different annotators (the first is the original).
      -
         img: figures/evaluating-sentence-compression-pitfalls-and-suggested-remedies/evaluating-sentence-compression-pitfalls-and-suggested-remedies-figure-1.jpg
         label: Figure 1
         caption: Compression rate strongly correlates with human judgments of meaning and grammaticality. Gold represents gold-standard compression and Deletion the results of a leading deletion model. Gold.1 grammar judgments were made alongside the original sentence and Gold.2 were made in isolation.
      -
         img: figures/evaluating-sentence-compression-pitfalls-and-suggested-remedies/evaluating-sentence-compression-pitfalls-and-suggested-remedies-table-2.jpg
         label: Table 2
         caption: Mean quality ratings of two competing models once the compression rates have been standardized, and as reported in the original work (denoted ∗). There is no significant improvement, but the numerically better model changes.
   abstract: This work surveys existing evaluation methodologies for the task of sentence compression, identifies their shortcomings, and proposes alternatives. In particular, we examine the problems of evaluating paraphrastic compression and comparing the output of different models. We demonstrate that compression rate is a strong predictor of compression quality and that perceived improvement over other models is often a side effect of producing longer output. 
   bibtex: |
      @InProceedings{napoles-vandurme-callisonburch:2011:T2TW-2011,
         author    = {Napoles, Courtney  and  {Van Durme}, Benjamin  and  Callison-Burch, Chris},
         title     = {Evaluating Sentence Compression: Pitfalls and Suggested Remedies},
         booktitle = {Proceedings of the Workshop on Monolingual Text-To-Text Generation},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon},
         publisher = {Association for Computational Linguistics},
         pages     = {91--97},
         url       = {http://www.aclweb.org/anthology/W11-1611}
       }
       
-
   title: Paraphrastic Sentence Compression with a Character-based Metric&colon; Tightening without Deletion
   authors: Courtney Napoles, Chris Callison-Burch, Juri Ganitevitch, Ben Van Durme
   venue: Workshop on Monolingual Text-To-Text Generation
   type: workshop
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/paraphrastic-sentence-compression.pdf
   page_count: 7
   id: paraphrastic-sentence-compression
   figures:
      -
         img: figures/paraphrastic-sentence-compression/paraphrastic-sentence-compression-figure-1.jpg
         label: Figure 1
         caption: Using a bilingual parallel corpus to extract paraphrases.
      -
         img: figures/paraphrastic-sentence-compression/paraphrastic-sentence-compression-table-1.jpg
         label: Table 1
         caption: Candidate paraphrases for study in detail with corresponding approximate cosine similarity (Monolingual) and translation model (Bilingual) scores.
      -
         img: figures/paraphrastic-sentence-compression/paraphrastic-sentence-compression-table-2.jpg
         label: Table 2
         caption: Mean ratings of compressions using just deletion or substitution at different paraphrase thresholds (Cos). Deletion performed better in all settings.
      -
         img: figures/paraphrastic-sentence-compression/paraphrastic-sentence-compression-table-3.jpg
         label: Table 3
         caption: Mean ratings of compressions generated by a substitution oracle, deletion only, deletion on the oracle compression, and the gold standard. Being able to choose the best paraphrases would enable our substitution model to outperform the deletion model.
   abstract: We present a substitution-only approach to sentence compression which “tightens” a sentence by reducing its character length. Replacing phrases with shorter paraphrases yields paraphrastic compressions as short as 60% of the original length. In support of this task, we introduce a novel technique for re-ranking paraphrases extracted from bilingual corpora. At high compression rates1 paraphrastic compressions outperform a state-of-the-art deletion model in an oracle experiment. For further compression, deleting from oracle paraphrastic compressions preserves more meaning than deletion alone. In either setting, paraphrastic compression shows promise for surpassing deletion-only methods. 
   bibtex: |
      @InProceedings{napoles-EtAl:2011:T2TW-2011,
         author    = {Napoles, Courtney  and  Callison-Burch, Chris  and  Ganitkevitch, Juri  and  {Van Durme}, Benjamin},
         title     = {Paraphrastic Sentence Compression with a Character-based Metric: Tightening without Deletion},
         booktitle = {Proceedings of the Workshop on Monolingual Text-To-Text Generation},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon},
         publisher = {Association for Computational Linguistics},
         pages     = {84--90},
         url       = {http://www.aclweb.org/anthology/W11-1610}
       }
       
-
   title: Paraphrase Fragment Extraction from Monolingual Comparable Corpora
   authors: Rui Wang and Chris Callison-Burch
   venue: BUCC
   type: workshop
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf
   page_count: 9
   id: paraphrase-fragment-extraction-from-monolingual-comparable-corpora
   figures:
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-1.jpg
         label: Table 1
         caption: Previous work in paraphrase acquisition and machine translation.
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-figure-1.jpg
         label: Figure 1
         caption: A three stage pipeline is used to extract paraphrases from monolingual texts
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-figure-2.jpg
         label: Figure 2
         caption: Results of the sentence pair extraction. The x-axis is the threshold for the comparability scores; and the y-axis is the distribution of the annotations.
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-figure-3.jpg
         label: Figure 3
         caption: An example of fragment pair extraction. Stop words are all set to 1 initially. Zero is the threshold, and the underscored phrases are the outputs.
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-2.jpg
         label: Table 2
         caption: Distribution of the Extracted Fragment Pairs of our Corpus and MSR Corpus. We manually evaluated 1051 sentence pairs in all. We use LCS or word aligner as the initialization and apply n-gram-based or chunk-based phrase extraction. The first column serves as the baseline.
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-5.jpg
         label: Table 5
         caption: Some examples of the extracted paraphrase fragment pairs
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-3.jpg
         label: Table 3
         caption: The size of our corpus. We only used ca. 10% of the GIGAWORD corpus in the experiments and the size of the collection at each stage are shown in the table
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-4.jpg
         label: Table 4
         caption: The (partial) distribution of N-grams (N=1-4) in different paraphrase collections
   abstract: We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different articles about the same topics or events. The procedure consists of document pair extraction, sentence pair extraction, and fragment pair extraction. At each stage, we evaluate the intermediate results manually, and tune the later stages accordingly. With this minimally supervised approach, we achieve 62% of accuracy on the paraphrase fragment pairs we collected and 67% extracted from the MSR corpus. The results look promising, given the minimal supervision of the approach, which can be further scaled up. 
   bibtex: |
      @InProceedings{wang-callisonburch:2011:BUCC,
         author    = {Wang, Rui  and  Callison-Burch, Chris},
         title     = {Paraphrase Fragment Extraction from Monolingual Comparable Corpora},
         booktitle = {Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon},
         publisher = {Association for Computational Linguistics},
         pages     = {52--60},
         url       = {http://www.aclweb.org/anthology/W11-1208}
       }
       
-
   title: The Arabic Online Commentary Dataset&colon; An Annotated Dataset of Informal Arabic with High Dialectal Content
   authors: Omar Zaidan and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/arabic-dialect-corpus.pdf
   page_count: 5
   highly_cited: 206
   id: arabic-dialect-corpus
   figures:
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-figure-1.jpg
         label: Figure 1
         caption: Two roughly equivalent Arabic sentences, one in MSA and one in Levantine Arabic, translated by the same MT system into English. An acceptable translation would be When will we see this group of criminals undergo trial (or tried)?. The MSA variant is handled well, while the dialectal variant is mostly transliterated
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-figure-2.jpg
         label: Figure 2
         caption: One possible breakdown of spoken Arabic into dialect groups&colon; Maghrebi, Egyptian, Levantine, Gulf, and Iraqi. Habash (2010) also gives a very similar breakdown
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-table-1.jpg
         label: Table 1
         caption: A summary of the different components of the AOC dataset. Overall, 1.4M comments were harvested from 86.1K articles, corresponding to 52.1M words.
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-table-2.jpg
         label: Table 2
         caption: A breakdown of sentences for which ≥ 2 annotators agreed on whether dialectal content exists or not.
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-table-3.jpg
         label: Table 3
         caption: Accuracy, dialect precision, and dialect recall (10-fold cross validation) for various classification tasks.
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-figure-3.jpg
         label: Figure 3
         caption: Dialect precision vs. recall for the classification task over Al-Ghad sentences (MSA vs. Levantine). The square point corresponds to the first line in Table 3.
   abstract: The written form of Arabic, Modern Standard Arabic (MSA), differs quite a bit from the spoken dialects of Arabic, which are the true “native” languages of Arabic speakers used in daily life. However, due to MSA’s prevalence in written form, almost all Arabic datasets have predominantly MSA content. We present the Arabic Online Commentary Dataset, a 52M-word monolingual dataset rich in dialectal content, and we describe our long-term annotation effort to identify the dialect level (and dialect itself) in each sentence of the dataset. So far, we have labeled 108K sentences, 41% of which as having dialectal content. We also present experimental results on the task of automatic dialect identification, using the collected labels for training and evaluation. 
   bibtex: |
      @InProceedings{zaidan-callisonburch:2011:ACL-HLT2011,
         author    = {Zaidan, Omar F.  and  Callison-Burch, Chris},
         title     = {The Arabic Online Commentary Dataset: an Annotated Dataset of Informal Arabic with High Dialectal Content},
         booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon, USA},
         publisher = {Association for Computational Linguistics},
         pages     = {37--41},
         url       = {http://www.aclweb.org/anthology/P11-2007}
       }
       
-
   title: Crowdsourcing Translation&colon; Professional Quality from Non-Professionals
   authors: Omar Zaidan and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/crowdsourcing-translation.pdf
   page_count: 10
   id: crowdsourcing-translation
   highly_cited: 500
   figures:
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-1.jpg
         label: Figure 1
         caption: A comparison of professional translations provided by the LDC to non-professional translations created on Mechanical Turk.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-2.jpg
         label: Figure 2
         caption: We redundantly translate each source sentence by soliciting multiple translations from different Turkers. These translations are put through a subsequent editing set, where multiple edited versions are produced. We select the best translation from the set using features that predict the quality of each translation and each translator.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-3.jpg
         label: Figure 3
         caption: BLEU scores for different selection methods, measured against the reference sets. Each score is an average of four BLEU scores, each calculated against three LDC reference translations. The five right-most bars are colored in orange to indicate selection over a set that includes both original translations as well as edited versions of them.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-table-1.jpg
         label: Table 1
         caption: Correlation (± std. dev.) for different selection methods, compared against the reference sets.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-4.jpg
         label: Figure 4
         caption: BLEU scores for the five right-most setups from Figure 3, constrained over the original translations.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-5.jpg
         label: Figure 5
         caption: The effect of varying the amount of calibration data (and using only the calibration feature). The 10% point (BLEU = 37.82) and the dashed line (BLEU = 39.06) correspond to the two right-most bars of Figure 3.
   abstract: Naively collecting translations by crowdsourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-toEnglish evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation. 
   bibtex: |
      @InProceedings{zaidan-callisonburch:2011:ACL-HLT2011,
         author    = {Zaidan, Omar F.  and  Callison-Burch, Chris},
         title     = {Crowdsourcing Translation: Professional Quality from Non-Professionals},
         booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon, USA},
         publisher = {Association for Computational Linguistics},
         pages     = {1220--1229},
         url       = {http://www.aclweb.org/anthology/P11-1122}
       }
       
-
   title: Incremental Syntactic Language Models for Phrase-based Translation
   authors: Lane Schwartz, Chris Callison-Burch, William Schuler and Stephen Wu
   venue: ACL
   type: conference
   year: 2011
   url: https://www.cis.upenn.edu/~ccb/publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf
   page_count: 12
   id: incremental-syntactic-language-models-for-phrase-based-translation
   figures:
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-1.jpg
         label: Figure 1
         caption: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German sentence Der Pra ̈sident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the application of a translation option, and includes the source sentence coverage vector, target language n-gram state, and syntactic language model state τ ̃ . Hypothesis combination is also shown, indicating where lattice paths with identical n-gram histories converge. We use the English translation The president meets the board on Friday as a running example throughout all Figures.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-2.jpg
         label: Figure 2
         caption: Sample binarized phrase structure tree.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-3.jpg
         label: Figure 3
         caption: Sample binarized phrase structure tree af- ter application of right-corner transform.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-4.jpg
         label: Figure 4
         caption: Graphical representation of the dependency structure in a standard Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. Circles denote random variables, and edges denote conditional dependencies. Shaded circles denote variables with observed values.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-5.jpg
         label: Figure 5
         caption: Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence The president meets the board on Friday. The shaded path through the parse lattice illustrates the recognized right-corner tree structure of Figure 3.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-6.jpg
         label: Figure 6
         caption: A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation option the board of source phrase den Vorstand. Syntactic language model state τ ̃31 contains random variables s1..3; likewise τ ̃51 contains s1..3. The intervening random variables r1..3, s1..3, and r1..3 are calculated by transition function δ (Eq. 6, as defined by §4.1), but are not stored. Observed random variables (e3..e5) are shown for clarity, but are not explicitly stored in any syntactic language model state.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-7.jpg
         label: Figure 7
         caption: Average per-word perplexity values. HHMM was run with beam size of 2000. Bold indicates best single-model results for LMs trained on WSJ sections 2-21. Best overall in italics.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-8.jpg
         label: Figure 8
         caption: Mean per-sentence decoding time (in seconds) for dev set using Moses with and without syntactic language model. HHMM parser beam sizes are indicated for the syntactic LM.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-9.jpg
         label: Figure 9
         caption: Results for Ur-En devtest (only sentences with 1-20 words) with HHMM beam size of 2000 and Moses settings of distortion limit 10, stack size 200, and ttable limit 20.
   abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporating syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity. 
   bibtex: |
      @InProceedings{schwartz-EtAl:2011:ACL-HLT20111,
         author    = {Schwartz, Lane  and  Callison-Burch, Chris  and  Schuler, William  and  Wu, Stephen},
         title     = {Incremental Syntactic Language Models for Phrase-based Translation},
         booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon, USA},
         publisher = {Association for Computational Linguistics},
         pages     = {620--631},
         url       = {http://www.aclweb.org/anthology/P11-1063}
       }
       
-
   title: Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators
   authors: Omar Zaidan and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/predicting-HTER-from-untrained-annotators.pdf
   page_count: 4
   id: predicting-HTER-from-untrained-annotators
   figures:
      -
         img: figures/predicting-HTER-from-untrained-annotators/predicting-HTER-from-untrained-annotators-table-1.jpg
         label: Table 1
         caption: The 4 genres of the dataset.
      -
         img: figures/predicting-HTER-from-untrained-annotators/predicting-HTER-from-untrained-annotators-figure-1.jpg
         label: Figure 1
         caption: Rank correlation between predicted ranking and HTER ranking for different prediction schemes, across the four genres, and across various sizes of the worker verification set.
   abstract: In the field of machine translation, automatic metrics have proven quite valuable in system development for tracking progress and measuring the impact of incremental changes. However, human judgment still plays a large role in the context of evaluating MT systems. For example, the GALE project uses human-targeted translation edit rate (HTER), wherein the MT output is scored against a post-edited version of itself (as opposed to being scored against an existing human reference). This poses a problem for MT researchers, since HTER is not an easy metric to calculate, and would require hiring and training human annotators to perform the editing task. In this work, we explore soliciting those edits from untrained human annotators, via the online service Amazon Mechanical Turk. We show that the collected data allows us to predict HTER-ranking of documents at a significantly higher level than the ranking obtained using automatic metrics. 
   bibtex: |
      @InProceedings{zaidan-callisonburch:2010:NAACLHLT,
         author    = {Zaidan, Omar F.  and  Callison-Burch, Chris},
         title     = {Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators},
         booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles, California},
         publisher = {Association for Computational Linguistics},
         pages     = {369--372},
         url       = {http://www.aclweb.org/anthology/N10-1057}
       }
       
-
   title: Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach
   authors: Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie Dorr, Scott Miller, Christine Piatko, Nathaniel W. Filardo, and Lori Levin
   venue: AMTA
   type: conference
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/semantically-informed-syntactic-machine-translation.pdf
   page_count: 10
   id: semantically-informed-syntactic-machine-translation
   figures:
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-1.jpg
         label: Figure 1
         caption: An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a phrase-based model (Moses) without linguistic information, which is representative of state-of-the-art MT quality before the SIMT effort.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-2.jpg
         label: Figure 2
         caption: The evolution of a semantically informed approach to our synchronous context free grammars (SCFGs). At the start of summer the decoder used translation rules with a single generic non-terminal symbol, later syntactic categories were used, and by the end of the summer the translation rules included semantic elements such as named entities and modalities.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-table-1.jpg
         label: Table 1
         caption: The size of the various data sets used for the experiments in this paper including the training, development (dev), incremental test set (devtest) and blind test set (test). The dev/devtest was a split of the NIST08 Urdu-English test set, and the blind test set was NIST09.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-3.jpg
         label: Figure 3
         caption: Workflow for producing semantically-grafted parse trees. The English side of the parallel corpus is automatically parsed, and also tagged with modality and named-entity markers. These tags are then grafted onto the syntactic parse trees. The relation finder was designed for additional tagging but was not implemented in the current work. (Future work will test relations as another component of meaning that may contribute toward im- proved MT ouput.)
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-4.jpg
         label: Figure 4
         caption: A sentence on the English side of the bilingual parallel training corpus is parsed with a syntactic parser, and also tagged with a named entity tagger. The tags are then grafted onto the syntactic parse tree to form new categories like NP-GPE and NP-weapon. Grafting happens prior to extracting translation rules, which happens normally except for the use of the augmented trees.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-5.jpg
         label: Figure 5
         caption: Example translation rules with named entity tags and modalities combined with syntactic categories.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-table-2.jpg
         label: Table 2
         caption: Named entity tags
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-table-3.jpg
         label: Table 3
         caption: Modality tags with their negated versions
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-6.jpg
         label: Figure 6
         caption: Results for a range of experiments conducted during the SIMT effort. Results show scores for base- line systems, which here include a phrase-based model (Moses) and a hierarchical phrase-based model (Hiero), neither of which make use of syntactic information. These also show the substantial improvements when syn- tax is introduced, along with different numbers of feature functions (FFs), and further improvements from semantic elements. The scores are lowercased Bleu calculated on the held-out devtest set.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-7.jpg
         label: Figure 7
         caption: An example of the improvements to Urdu-English translation before and after the SIMT effort. Output is from the baseline Hiero model, which does not use linguistic information, and from the final model, which incorporates syntactic and semantic information.
   abstract: We describe a unified and coherent syntactic framework for supporting a semantically-informed syntactic approach to statistical machine translation. Semantically enriched syntactic tags assigned to the target-language training texts improved translation quality. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English translation task. This finding supports the hypothesis (posed by many researchers in the MT community, e.g., in DARPA GALE) that both syntactic and semantic information are critical for improving translation quality—and further demonstrates that large gains can be achieved for low-resource languages with different word order than English. 
   bibtex: |
      @InProceedings{Baker-EtAl:2010:AMTA,
         author = {Kathryn Baker and Michael Bloodgood and Chris Callison-Burch and Bonnie J. Dorr and Nathaniel W. Filardo and Lori Levin and Scott Miller and Christine Piatko},
         title = {Semantically-Informed Machine Translation: A Tree-Grafting Approach},
         booktitle = {Proceedings of The Ninth Biennial Conference of the Association for Machine Translation in the Americas},
         address = {Denver, Colorado},
         url = {http://www.mt-archive.info/AMTA-2010-Baker.pdf},
         year = {2010}
       }
       
-
   title: Transliterating From All Languages
   authors: Ann Irvine, Alex Klementiev, and Chris Callison-Burch
   venue: AMTA
   type: conference
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf
   page_count: 8
   id: transliterating-from-all-languages
   figures:
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-1.jpg
         label: Table 1
         caption: Examples of Russian to English and Greek to English transliteration rules learned by Joshua along with the following associated log probabilities&colon; a character sequence mapping probability, a character substitution probability, and a character-based language model probability.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-2.jpg
         label: Table 2
         caption: The 100 languages with the largest number of name pairs with English. The counts are for Wikipedia pages describing people that have a inter-language link with English, and whose title is not identical to the English page title.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-3.jpg
         label: Table 3
         caption: Languages of interest and the number of harvested person names. There are many more English names than there are for other languages and, correspondingly, its overlap with other languages is relatively large. Consequently, the amount of training data for transliterating between English and other languages is greater than between any other pair of languages.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-4.jpg
         label: Table 4
         caption: Examples of multi-word Russian-English name pairs that require word alignments and filtering.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-5.jpg
         label: Table 5
         caption: A comparison of our performance against the systems submitted to the Russian and Hindi transliteration shared tasks at the 2009 Named Entities Workshop.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-6.jpg
         label: Table 6
         caption: Examples of candidate transliterations and their corresponding reference transliterations, and the edit distances and normalized edit distances between them. The normalized edit distance is the minimum number of insertions, deletions, and substitutions that must be made to transform one string into the other, normalized by the length of the reference string, and multiplied by 100.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-figure-1.jpg
         label: Figure 1
         caption: Number of training pairs vs. system performance as measured by average normalized edit distance from the reference. The normalized edit distance is the minimum number of insertions, deletions, and substitutions that must be made to transform one string into the other, normalized by the length of the reference string, and multiplied by 100.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-figure-2.jpg
         label: Figure 2
         caption: Learning curves resulting from holding out some training pairs from the models. The normalized edit distance is the minimum number of insertions, deletions, and substitutions that must be made to transform one string into the other, normalized by the length of the reference string, and multiplied by 100.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-figure-3.jpg
         label: Figure 3
         caption: The percent of perfect transliterations found in the n-best output vs. n in n-best.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-7.jpg
         label: Table 7
         caption: The 10 most common errors. The reference is on the left, and hypothesis is on the right. E indicates that the letter E is dropped from the hypothesis, and I indicates I is inserted.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-8.jpg
         label: Table 8
         caption: Examples of Russian to English transliteration output. The system produced the reference transliteration in the first three examples, and it produced a correct alternative English transliteration in the second three examples. It incorrectly transliterated the final three.
   abstract: Much of the previous work on transliteration has depended on resources and attributes specific to particular language pairs. In this work, rather than focus on a single language pair, we create robust models for transliterating from all languages in a large, diverse set to English. We create training data for 150 languages by mining name pairs from Wikipedia. We train 13 systems and analyze the effects of the amount of training data on transliteration performance. We also present an analysis of the types of errors that the systems make. Our analyses are particularly valuable for building machine translation systems for low resource languages, where creating and integrating a transliteration module for a language with few NLP resources may provide substantial gains in translation performance. 
   bibtex: |
      @InProceedings{Irvine-EtAl:2010:AMTA,
         author = {Ann Irvine and Chris Callison-Burch and Alexandre Klementiev}
         title = {Transliterating From All Languages},
         booktitle = {Proceedings of The Ninth Biennial Conference of the Association for Machine Translation in the Americas},
         address = {Denver, Colorado},
         url = {http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf},
         year = {2010}
       }
       
-
   title: Joshua 2.0&colon; A Toolkit for Parsing-Based Machine Translation with Syntax, Semirings, Discriminative Training and Other Goodies
   authors: Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Ann Irvine, Lane Schwartz, Wren N. G. Thornton, Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
   venue: WMT
   type: workshop
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/joshua-2.0.pdf
   page_count: 5
   id: joshua-2
   abstract: We describe the progress we have made in the past year on Joshua (Li et al., 2009a), an open source toolkit for parsing-based machine translation. The new functionality includes&colon; support for translation grammars with a rich set of syntactic nonterminals, the ability for external modules to posit constraints on how spans in the input sentence should be translated, lattice parsing for dealing with input uncertainty, a semiring framework that provides a unified way of doing various dynamic programming calculations, variational decoding for approximating the intractable MAP decoding, hypergraph-based discriminative training for better feature engineering, a parallelized MERT module, document-level and tail-based MERT, visualization of the derivation trees, and a cleaner pipeline for MT experiments. 
   bibtex: |
      @InProceedings{li-EtAl:2010:WMT,
         author    = {Li, Zhifei  and  Callison-Burch, Chris  and  Dyer, Chris  and  Ganitkevitch, Juri  and  Irvine, Ann  and  Khudanpur, Sanjeev  and  Schwartz, Lane  and  Thornton, Wren  and  Wang, Ziyuan  and  Weese, Jonathan  and  Zaidan, Omar},
         title     = {Joshua 2.0: A Toolkit for Parsing-Based Machine Translation with Syntax, Semirings, Discriminative Training and Other Goodies},
         booktitle = {Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR},
         month     = {July},
         year      = {2010},
         address   = {Uppsala, Sweden},
         publisher = {Association for Computational Linguistics},
         pages     = {133--137},
         url       = {http://www.aclweb.org/anthology/W10-1718}
       }
       
-
   title: Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation
   authors: Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, Omar Zaidan
   venue: WMT
   type: workshop
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/findings-of-wmt10-and-metrics-matr.pdf
   page_count: 33
   id: findings-of-wmt10-and-metrics-matr
   highly_cited: 254
   figures:
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words is based on the provided tokenizer.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task. Not all groups participated in all language pairs.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-2.jpg
         label: Table 2
         caption: Participants in the system combination task.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-3.jpg
         label: Table 3
         caption: The number of items that were collected for each task during the manual evaluation. An item is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no judgment in the judgment task.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-4.jpg
         label: Table 4
         caption: Inter- and intra-annotator agreement for the sentence ranking task. In this task, P(E) is 0.333.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-figure-2.jpg
         label: Figure 2
         caption: This screenshot shows what an annotator sees when beginning to edit the output of a machine translation system.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-5.jpg
         label: Table 5
         caption: Official results for the WMT10 translation task, based on the human evaluation (ranking translations relative to each other)
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-6.jpg
         label: Table 6
         caption: Official results for the WMT10 system combination task, based on the human evaluation (ranking translations relative to each other)
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-figure-3.jpg
         label: Figure 3
         caption: The percent of time that each system’s edited output was judged to be an acceptable translation. These numbers also include judgments of the system’s output when it was marked either incomprehen- sible or acceptable and left unedited. Note that the reference translation was edited alongside the system outputs. Error bars show one positive and one negative standard deviation for the systems in that lan- guage pair.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-7.jpg
         label: Table 7
         caption: System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average absolute value.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-9.jpg
         label: Table 9
         caption: Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average absolute value. Number of pairs included in comparison&colon; cz-en 3575, fr-en 5844, de-en 7585, es-en 7911.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-8.jpg
         label: Table 8
         caption: System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-10.jpg
         label: Table 10
         caption: Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value. Number of pairs included in comparison&colon; en-cz 9613, en-fr 5904, en-de 10892, en-es 3813.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-11.jpg
         label: Table 11
         caption: Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have potentially obtained 600 _ 5 _ 5 = 15,000 labels for each language pair. The Label count row indicates to what extent that potential was met (over the 30-day lifetime of our tasks), and the “Completed...” rows give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group, 2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5 workers, with 100% of the sets completed at least once. The total cost of this data collection effort was roughly $200.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-figure-4.jpg
         label: Figure 4
         caption: The effect of removing an increasing number of MTurk workers. The order in which workers are removed is by Kexp(w), the kappa agreement coefficient with expert data (excluding references).
   abstract: This paper presents the results of the WMT10 and MetricsMATR10 shared tasks,1 which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon’s Mechanical Turk. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2010:WMT,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Monz, Christof  and  Peterson, Kay  and  Przybocki, Mark  and  Zaidan, Omar},
         title     = {Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation},
         booktitle = {Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR},
         month     = {July},
         year      = {2010},
         address   = {Uppsala, Sweden},
         publisher = {Association for Computational Linguistics},
         pages     = {17--53},
         url       = {http://www.aclweb.org/anthology/W10-1703}
       }
       
-
   title: Large-Scale, Cost-Focused Active Learning for Statistical Machine Translation
   authors: Michael Bloodgood and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/cost-focused-active-learning-for-statistical-machine-translation.pdf
   page_count: 11
   id: cost-focused-active-learning-for-statistical-machine-translation
   figures:
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-1.jpg
         label: Figure 1
         caption: Syntax-based and Hierarchical Phrase-Based MT systems’ learning curves on the LDC Urdu-English language pack. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score. Note the diminishing returns as more data is added. Also note how relatively early on in the process previous studies were terminated. In contrast, the focus of our main experiments doesn’t even be- gin until much higher performance has already been achieved with a period of diminishing returns firmly established.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-2.jpg
         label: Figure 2
         caption: The VG sentence selection algorithm
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-3.jpg
         label: Figure 3
         caption: Random vs VG selection. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-4.jpg
         label: Figure 4
         caption: Random vs VG selection. The x-axis measures the number of foreign words in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-5.jpg
         label: Figure 5
         caption: Random vs Shortest vs Longest selection. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-6.jpg
         label: Figure 6
         caption: Random vs Shortest vs Longest selection. The x-axis measures the number of foreign words in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-7.jpg
         label: Figure 7
         caption: VG vs MostNew vs ModerateNew selection. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-8.jpg
         label: Figure 8
         caption: Screenshot of the interface we used for soliciting translations for triggers.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-9.jpg
         label: Figure 9
         caption: HNG vs Random collection of new data via MTurk. y-axis measures BLEU. x-axis measures annotation time in seconds.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-10.jpg
         label: Figure 10
         caption: Distribution of translation speeds (in seconds per word) for HNG postings versus complete sentence postings. The y-axis measures relative frequency. The x-axis measures translation speed in seconds per word (so farther to the left is faster).
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-11.jpg
         label: Figure 11
         caption: Bucking the trend&colon; performance of HNG-selected additional data from BBC web crawl data annotated via Amazon Mechanical Turk. y-axis measures BLEU. x-axis measures number of words annotated.
   abstract: We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement. 
   bibtex: |
      @InProceedings{bloodgood-callisonburch:2010:ACL,
         author    = {Bloodgood, Michael  and  Callison-Burch, Chris},
         title     = {Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation},
         booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
         month     = {July},
         year      = {2010},
         address   = {Uppsala, Sweden},
         publisher = {Association for Computational Linguistics},
         pages     = {854--864},
         url       = {http://www.aclweb.org/anthology/P10-1088}
       }
       
-
   title: Creating Speech and Language Data With Amazon’s Mechanical Turk
   authors: Chris Callison-Burch and Mark Dredze
   venue: NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk
   type: workshop
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/creating-speech-and-language-data-with-amazon-mechanical-turk.pdf
   page_count: 12
   highly_cited: 430
   id: creating-speech-and-language-data-with-amazon-mechanical-turk
   figures:
      -
         img: figures/creating-speech-and-language-data-with-amazon-mechanical-turk/creating-speech-and-language-data-with-amazon-mechanical-turk-figure-1.jpg
         label: Figure 1
         caption: Time spent, HITs completed, and amount earned from a survey of 1,000 Turkers by Ipeirotis (2010).
   abstract: In this paper we give an introduction to using Amazon's Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies. We survey the papers published in the NAACL2010 Workshop. 24 researchers participated in the workshop's $100 challenge to create data for speech and language applications. 
   bibtex: |
      @InProceedings{callisonburch-dredze:2010:MTURK,
         author    = {Callison-Burch, Chris  and  Dredze, Mark},
         title     = {Creating Speech and Language Data With {Amazon's Mechanical Turk}},
         booktitle = {Proceedings of the {NAACL HLT} 2010 Workshop on Creating Speech and Language Data with {Amazon's Mechanical Turk}},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles},
         publisher = {Association for Computational Linguistics},
         pages     = {1--12},
         url       = {http://www.aclweb.org/anthology/W10-0701}
       }
       
-
   title: Using Mechanical Turk to Build Machine Translation Evaluation Sets
   authors: Michael Bloodgood and Chris Callison-Burch
   venue: NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk
   type: workshop
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/using-mechanical-turk-to-build-machine-translation-evaluation-sets.pdf
   page_count: 4
   id: using-mechanical-turk-to-build-machine-translation-evaluation-sets
   figures:
      -
         img: figures/using-mechanical-turk-to-build-machine-translation-evaluation-sets/using-mechanical-turk-to-build-machine-translation-evaluation-sets-table-1.jpg
         label: Table 1
         caption: This table shows three MT systems evaluated on five different test sets. For each system-test set pair, two numbers are displayed. The top number is the BLEU score for that system when using that test set. For example, ISI-Syntax tested on the NIST-2009 test set has a BLEU score of 33.10. The bottom number is the percentage of baseline system performance that is achieved. ISI-Syntax (the highest-performing system on NIST2009 to our knowledge) is used as the baseline. Thus, it will always have 100% as the percentage performance for all of the test sets. To illustrate computing the percentage performance for the other systems, consider for JHU-Syntax tested on NIST2009, that its BLEU score of 32.77 divided by the BLEU score of the baseline system is 32.77/33.10 99.00%
      -
         img: figures/using-mechanical-turk-to-build-machine-translation-evaluation-sets/using-mechanical-turk-to-build-machine-translation-evaluation-sets-table-2.jpg
         label: Table 2
         caption: This table shows three MT systems evaluated using the official NIST2009 test set and the two test sets we constructed (MTurk-NoEditing and MTurk-Edited). For each system-test set pair, two numbers are displayed. The top number is the BLEU score for that system when using that test set. For example, ISI-Syntax tested on the NIST-2009 test set has a BLEU score of 33.10. The bottom number is the percentage of baseline system performance that is achieved. ISI-Syntax (the highest-performing system on NIST2009 to our knowledge) is used as the baseline.
   abstract: Building machine translation (MT) test sets is a relatively expensive task. As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case. In this paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT test sets cheaply. We find that MTurk can be used to make test sets much cheaper than professionally-produced test sets. More importantly, in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield. 
   bibtex: |
      @InProceedings{bloodgood-callisonburch:2010:MTURK,
         author    = {Bloodgood, Michael  and  Callison-Burch, Chris},
         title     = {Using {Mechanical Turk} to Build Machine Translation Evaluation Sets},
         booktitle = {Proceedings of the {NAACL HLT} 2010 Workshop on Creating Speech and Language Data with {Amazon's Mechanical Turk}},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles},
         publisher = {Association for Computational Linguistics},
         pages     = {208--211},
         url       = {http://www.aclweb.org/anthology/W10-0733}
       }
       
-
   title: Crowdsourced Accessibility&colon; Elicitation of Wikipedia Articles
   authors: Scott Novotoney and Chris Callison-Burch
   venue: NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk
   type: workshop
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/crowdsourced-accessibility-elicitation-of-wikipedia-articles.pdf
   page_count: 4
   id: crowdsourced-accessibility-elicitation-of-wikipedia-articles
   abstract: Mechanical Turk is useful for generating complex speech resources like conversational speech transcription. In this work, we explore the next step of eliciting narrations of Wikipedia articles to improve accessibility for low-literacy users. This task proves a useful test-bed to implement qualitative vetting of workers based on difficult to define metrics like narrative quality. Working with the Mechanical Turk API, we collected sample narrations, had other Turkers rate these samples and then granted access to full narration HITs depending on aggregate quality. While narrating full articles proved too onerous a task to be viable, using other Turkers to perform vetting was very successful. Elicitation is possible on Mechanical Turk, but it should conform to suggested best practices of simple tasks that can be completed in a streamlined workflow. 
   bibtex: |
      @InProceedings{novotney-callisonburch:2010:MTURK,
         author    = {Novotney, Scott  and  Callison-Burch, Chris},
         title     = {Crowdsourced Accessibility: Elicitation of Wikipedia Articles},
         booktitle = {Proceedings of the {NAACL HLT} 2010 Workshop on Creating Speech and Language Data with {Amazon's Mechanical Turk}},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles},
         publisher = {Association for Computational Linguistics},
         pages     = {41--44},
         url       = {http://www.aclweb.org/anthology/W10-0706}
       }
       
-
   title: Cheap Facts and Counter-Facts
   authors: Rui Wang and Chris Callison-Burch
   venue: NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk
   type: workshop
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/cheap-facts-and-counter-facts.pdf
   page_count: 5
   id: cheap-facts-and-counter-facts
   figures:
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-1.jpg
         label: Table 1
         caption: The statistics of the (valid) data we collect. The Total column presents the number of extracted NEs and generated hypotheses and the Average column shows the average numbers per text respectively.
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-2.jpg
         label: Table 2
         caption: The comparison between the generated (counter-)facts and the original hypotheses from the RTE dataset. The Ave. Length column represents the average number of words in each hypothesis; The Ave. BoW shows the average bag-of-words similarity compared with the text. The three columns on the right are all about the position of the NE appearing in the sentence, how likely it is at the head, middle, or tail of the sentence.
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-4.jpg
         label: Table 4
         caption: Examples of facts and counter-facts, compared with the original texts and hypotheses. We ask the Turkers to write several (counter-)facts about the highlighted NEs, and only part of the results are shown here.
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-3.jpg
         label: Table 3
         caption: The comparison of the generated (counter-)facts with the original hypotheses. The Valid column shows the percentage of the valid (counter-)facts; and other columns present the distribution of harder, easier cases than the original hypotheses or with the same difficulty.
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-5.jpg
         label: Table 5
         caption: The results of baseline RTE systems on the data we collected, compared with the original RTE-5 dataset. The Counter-/Facts column shows the number of T-H pairs; and the other scores in percentage are accuracy of the systems.
   abstract: This paper describes our experiments of using Amazon's Mechanical Turk to generate (counter-)facts from texts for certain named entities. We give the human annotators a paragraph of text and a highlighted named entity. They will write down several (counter-)facts about this named entity in that context. The analysis of the results is performed by comparing the acquired data with the recognizing textual entailment (RTE) challenge dataset. 
   bibtex: |
      @InProceedings{wang-callisonburch:2010:MTURK,
         author    = {Wang, Rui  and  Callison-Burch, Chris},
         title     = {Cheap Facts and Counter-Facts},
         booktitle = {Proceedings of the {NAACL HLT} 2010 Workshop on Creating Speech and Language Data with {Amazon's Mechanical Turk}},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles},
         publisher = {Association for Computational Linguistics},
         pages     = {163--167},
         url       = {http://www.aclweb.org/anthology/W10-0725}
       }
       
-
   title: Stream-based Translation Models for Statistical Machine Translation
   authors: Abby Levenberg, Chris Callison-Burch, and Miles Osborne
   venue: NAACL
   type: conference
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/stream-based-translation-models.pdf
   page_count: 9
   id: stream-based-translation-models
   figures:
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-figure-1.jpg
         label: Figure 1
         caption: Streaming coverage conditions. In traditional batch based modeling the coverage of a trained model never changes. Unbounded coverage operates without any memory constraints so the model is able to continually add data from the input stream. Bounded coverage uses just a fixed window.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-figure-2.jpg
         label: Figure 2
         caption: Recency effects to SMT performance. Depicted are the differences in BLEU scores for multiple test points decoded by a static baseline system and a system batched retrained on a fixed sized window prior to the test point in question. The results are accentuated at the end of the timeline when more time has passed confirming that recent data impacts translation performance.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-table-1.jpg
         label: Table 1
         caption: Date ranges, total sentence pairs, and source and target word counts encountered in the input stream for example epochs. Epoch 00 is baseline data that is also used as a seed corpus for the online models.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-table-2.jpg
         label: Table 2
         caption: Test and Hiero grammar rules extracted for the corresponding test set. Translation model statistics for example epochs and the next test dates grouped by experimental condition. Train Sent. is the number of sentence pairs in test and training data respectively. Rules is the count of unique
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-figure-3.jpg
         label: Figure 3
         caption: Static vs. online TM performance. Gains in translation performance measured by BLEU are achieved when recent German-English sentence pairs are automatically incorporated into the TM. Shown are relative BLEU improvements for the online models against the static baseline.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-table-3.jpg
         label: Table 3
         caption: Sample BLEU results for all baseline and online EM model conditions. The static baseline is a traditional model that is never retrained. The batch unbounded and batch bounded models incorporate new data from the stream but retraining is slow and computationally expensive (best results are bolded). In contrast both unbounded and bounded online models incrementally retrain only the mini-batch of new sentences collected from the incoming stream so quickly adopt the new data (best results are italicized).
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-figure-4.jpg
         label: Figure 4
         caption: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent sentences. In both examples we get longer matching phrases in the online translation compared to the static one.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-table-4.jpg
         label: Table 4
         caption: Unbounded LM coverage improvements. Shown are the BLEU scores for each experimental conditional when we allow the LM coverage to increase.
   abstract: Typical statistical machine translation systems are trained with static parallel corpora. Here we account for scenarios with a continuous incoming stream of parallel training data. Such scenarios include daily governmental proceedings, sustained output from translation agencies, or crowd-sourced translations. We show incorporating recent sentence pairs from the stream improves performance compared with a static baseline. Since frequent batch retraining is computationally demanding we introduce a fast incremental alternative using an online version of the EM algorithm. To bound our memory requirements we use a novel data-structure and associated training regime. When compared to frequent batch retraining, our online time and space-bounded model achieves the same performance with significantly less computational overhead. 
   bibtex: |
      @InProceedings{levenberg-callisonburch-osborne:2010:NAACLHLT,
         author    = {Levenberg, Abby  and  Callison-Burch, Chris  and  Osborne, Miles},
         title     = {Stream-based Translation Models for Statistical Machine Translation},
         booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles, California},
         publisher = {Association for Computational Linguistics},
         pages     = {394--402},
         url       = {http://www.aclweb.org/anthology/N10-1062}
       }
       
-
   title: Cheap, Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription
   authors: Scott Novotney and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/automatic-speech-recognition-with-non-expert-transcription.pdf
   page_count: 9
   highly_cited: 211
   id: automatic-speech-recognition-with-non-expert-transcription
   figures:
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-1.jpg
         label: Figure 1
         caption: Histogram of per-turker transcription rate for twenty hours of English CTS data. Historical estimates for high quality transcription are 50xRT. The 2004 Fisher transcription effort achieved 6xRT and the average here is 11xRT.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-table-1.jpg
         label: Table 1
         caption: Quality of Non-Professional Transcription on 20 hours of English Switchboard. Even though disagreement for random selection without quality control has 23% disagreement with professional transcription, an ASR system trained on the data is only 2.5% worse than using LDC transcriptions. The upper bound for quality control (row 3) recovers only 50% of the total loss.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-2.jpg
         label: Figure 2
         caption: WER with a varied amount of LM training data and a fixed 16hr acoustic model. MTurk transcription degrades WER by 0.8% absolute across LM size. When interpolated with 1M words of broadcast news, this degradation shrinks to 0.6%.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-3.jpg
         label: Figure 3
         caption: Historical cost estimates are $150 per hour of transcription (blue cirlces). The company Casting Words uses Turkers to transcribe English at $90 per hour which we estimated to be high quality (green triangles). Transcription without quality control on Mechanical Turk (red squares) is drastically cheaper at $5 per hour. With a fixed budget, it is better to transcribe more data at lower quality than to improve quality. Contrast the oracle WER for 20 hours transcribed three times (red diamond) with 60 hours transcribed once (bottom red square).
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-4.jpg
         label: Figure 4
         caption: Each Turker was judged against professional and non-professional reference and assigned an overall disagreement. The distribution of Turker disagreement follows a gamma distribution, with a tight cluster of average Turkers and a long-tail of bad Turkers. Estimating with non-professionals (even though the reference is 23% wrong on average) is surprisingly well matched to professional estimate. Turker estimation over-estimated disagreement by only 2%.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-5.jpg
         label: Figure 5
         caption: Boxplot of the difference of non-professional disagreement with a fixed number of utterances to professional disagreement over all utterances. While error is expectedly high with one utterance, 50% of the estimates are within 3% of the truth after ten utterances and 75% of the estimates are within 6% after fifteen utterances.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-6.jpg
         label: Figure 6
         caption: Each Turker is a point with professional (X axis) plotted against non-professional (Y axis) disagreement. The non-professional disagreement correlates surprisingly well with professional disagreement even though the transcripts used as reference are 23% wrong on average. By setting a selection threshold, the space is divided into four quadrants. The bottom left are correctly accepted&colon; both non-professional and professional disagreement are below the threshold. The top left are incorrectly rejected&colon; using their transcripts would have helped, but they don’t hurt system performance, just waste money. The top right are correctly rejected for having high disagreement. The bottom right are the troublesome false positives that are included in training but actually may hurt performance. Luckily, the ratio of false negatives to false positives is usually much larger.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-7.jpg
         label: Figure 7
         caption: It is difficult to find only good Turkers since the false positives outnumber the few good workers. However, rejecting bad Turkers becomes very easy once past the mean error rate of 23%. It is better to use disagreement estimation to reject poor workers instead of finding good workers.
   abstract: Deploying an automatic speech recognition system with reasonable performance requires expensive and time-consuming in-domain transcription. Previous work demonstrated that non-professional annotation through Amazon’s Mechanical Turk can match professional quality. We use Mechanical Turk to transcribe conversational speech for as little as one thirtieth the cost of professional transcription. The higher disagreement of non-professional transcribers does not have a significant effect on system performance. While previous work demonstrated that redundant transcription can improve data quality, we found that resources are better spent collecting more data. Finally, we describe a quality control method without needing professional transcription. 
   bibtex: |
      @InProceedings{novotney-callisonburch:2010:NAACLHLT,
         author    = {Novotney, Scott  and  Callison-Burch, Chris},
         title     = {Cheap, Fast and Good Enough: Automatic Speech Recognition with Non-Expert Transcription},
         booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles, California},
         publisher = {Association for Computational Linguistics},
         pages     = {207--215},
         url       = {http://www.aclweb.org/anthology/N10-1024}
       }
       
-
   title: Integrating Output from Specialized Modules in Machine Translation&colon; Transliteration in Joshua
   authors: Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton, and Chris Callison-Burch
   venue: PBML
   type: journal
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/integrating-output-from-specialized-modules-in-machine-translation.pdf
   page_count: 10
   id: integrating-output-from-specialized-modules-in-machine-translation
   figures:
      -
         img: figures/integrating-output-from-specialized-modules-in-machine-translation/integrating-output-from-specialized-modules-in-machine-translation-table-1.jpg
         label: Table 1
         caption: Impact of transliteration on BLEU in submissions to NIST MT09 evaluation.
      -
         img: figures/integrating-output-from-specialized-modules-in-machine-translation/integrating-output-from-specialized-modules-in-machine-translation-table-2.jpg
         label: Table 2
         caption: Examples of improvements from transliteration.
      -
         img: figures/integrating-output-from-specialized-modules-in-machine-translation/integrating-output-from-specialized-modules-in-machine-translation-table-3.jpg
         label: Table 3
         caption: Impact of transliteration. Note that the location name “Swabia” was incorrectly transliterated to “Cuba.” This example indicates the future room for improvement.
   abstract: In many cases in SMT we want to allow specialized modules to propose translation fragments to the decoder and allow them to compete with translations contained in the phrase table. Transliteration is one module that may produce such specialized output. In this paper, as an example, we build a specialized Urdu transliteration module and integrate its output into an Urdu–English MT system. The module marks-up the test text using an XML format, and the decoder allows alternate translations (transliterations) to compete. 
   bibtex: |
      @article{Irvine-EtAl:2010:PBML,
       author = {Ann Irvine and Mike Kayser and Zhifei Li and Wren Thornton and Chris Callison-Burch },
       title = {Integrating Output from Specialized Modules in Machine Translation: Transliteration in {J}oshua},
       journal = {The Prague Bulletin of Mathematical Linguistics},
       volume = {93},
       pages = {107--116},
       year = {2010}
       }
       
-
   title: Visualizing Data Structures in Parsing-Based Machine Translation
   authors: Jonathan Weese and Chris Callison-Burch
   venue: PBML
   type: journal
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/visualizing-data-structures-in-parsing-based-machine-translation.pdf
   page_count: 10
   id: visualizing-data-structures-in-parsing-based-machine-translation
   figures:
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-1.jpg
         label: Figure 1
         caption: A hypergraph showing two candidate translations of Je suis mon maître.
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-2.jpg
         label: Figure 2
         caption: The Derivation Tree browser’s sentence selection and tree-viewing windows.
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-3.jpg
         label: Figure 3
         caption: An example visualization of two derivation trees for SCFGs that use a Hiero-style grammar and a syntactically-motivated grammar.
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-4.jpg
         label: Figure 4
         caption: The visualization window for the hypergraph browser.
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-5.jpg
         label: Figure 5
         caption: An example of bad production rules that parse pieces of the source sentence without producing any target-side output.
   abstract: As machine translation (MT) systems grow more complex and incorporate more linguistic knowledge, it becomes more difficult to evaluate independent pieces of the MT pipeline. Being able to inspect many of the intermediate data structures used during MT decoding allows a more fine-grained evaluation of MT performance, helping to determine which parts of the current process are effective and which are not. In this article, we present an overview of the visualization tools that are currently distributed with the Joshua (Li et al., 2009) MT decoder. We explain their use and present an example of how visually inspecting the decoder’s data structures has led to useful improvements in the MT model. 
   bibtex: |
      @article{Weese-CallisonBurch:2010:PBML,
       author = {Jonathan Weese and Chris Callison-Burch},
       title = {Visualizing Data Structures in Parsing-based Machine Translation},
       journal = {The Prague Bulletin of Mathematical Linguistics},
       volume = {93},
       pages = {127--136},
       year = {2010}
       }
       
-
   title: Hierarchical Phrase-Based Grammar Extraction in Joshua&colon; Suffix Arrays and Prefix Trees
   authors: Lane Schwartz and Chris Callison-Burch
   venue: PBML
   type: journal
   year: 2010
   url: https://www.cis.upenn.edu/~ccb/publications/hiero-grammar-extraction-with-suffix-arrays.pdf
   page_count: 10
   id: hiero-grammar-extraction-with-suffix-arrays
   figures:
      -
         img: figures/hiero-grammar-extraction-with-suffix-arrays/hiero-grammar-extraction-with-suffix-arrays-figure-1.jpg
         label: Figure 1
         caption: During suffix array creation, the contents of a corpus array are sorted using the element comparison function Compare_Elements
      -
         img: figures/hiero-grammar-extraction-with-suffix-arrays/hiero-grammar-extraction-with-suffix-arrays-figure-2.jpg
         label: Figure 2
         caption: Query intersection algorithm implemented in Joshua. This algorithm is adapted from a corrected version (Lopez, p.c.) of query intersection (Lopez, 2008).
   abstract: While example-based machine translation has long used corpus information at run-time, statistical phrase-based approaches typically include a preprocessing stage where an aligned parallel corpus is split into phrases, and parameter values are calculated for each phrase using simple relative frequency estimates. This paper describes an open source implementation of the crucial algorithms presented in (Lopez, 2008) which allow direct run-time calculation of SCFG translation rules in Joshua. 
   bibtex: |
      @article{Schwartz-CallisonBurch:2010:PBML,
       author = {Lane Schwartz and Chris Callison-Burch },
       title = {Hierarchical Phrase-Based Grammar Extraction in Joshua: Suffix Arrays and Prefix Tree},
       journal = {The Prague Bulletin of Mathematical Linguistics},
       volume = {93},
       pages = {157--166},
       year = {2010}
       }
       
-
   title: Semantically Informed Machine Translation (SIMT)
   authors: Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf Brown, Chris Callison-Burch, Glen Coppersmith, Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayﬁeld, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane Schwartz and David Zajic.  SCAL Summer Workshop Final Report
   venue: HLTCOE
   type: report
   year: 2009
   url: https://www.cis.upenn.edu/~ccb/publications/scale-2009-report.pdf
   page_count: 152
   id: scale-2009-report
   figures:
      -
         img: figures/scale-2009-report/scale-2009-report-figure-1.jpg
         label: Figure 1
         caption: An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a state-of-the-art system before the SIMT SCALE.
      -
         img: figures/scale-2009-report/scale-2009-report-figure-2.jpg
         label: Figure 2
         caption: Translation Errors due to Missing or Incorrect Named Entities
      -
         img: figures/scale-2009-report/scale-2009-report-table-3.jpg
         label: Table 3
         caption: Example clusters derived from the Brown bigram mutual information clustering.
      -
         img: figures/scale-2009-report/scale-2009-report-table-4.jpg
         label: Table 4
         caption: The BBC English and Urdu data.
      -
         img: figures/scale-2009-report/scale-2009-report-table-6.jpg
         label: Table 6
         caption: Coverage of new Urdu phrases in the (relatively small) test set.
      -
         img: figures/scale-2009-report/scale-2009-report-table-7.jpg
         label: Table 7
         caption: Examples of English-to-English transfer rules learned via pivoting.
      -
         img: figures/scale-2009-report/scale-2009-report-figure-10.jpg
         label: Figure 10
         caption: PhysicallocationrelationforIn the West Bank, a passenger was wounded.
   abstract: This report describes the findings of the machine translation team from the first Summer Camp for Applied Language Exploration (SCALE) hosted at the Human Language Technology Center of Excellence located at Johns Hopkins University. This intensive, eight week workshop brought together 20 students, faculty and researchers to conduct research on the topic of Semantically Informed Machine Translation (SIMT). The type of semantics that were examined at the SIMT workshop were "High Information Value Elements," or HIVEs, which include named entities (such as people or organizations) and modalities (indications that a statement represents something that has taken place or is a belief or an intention). These HIVEs were examined in the context of machine translation between Urdu and English. The goal of the workshop was to identify and translate HIVEs from the foreign language, and to investigate whether incorporating this sort of structured semantic information into machine translation (MT) systems could produce better translations. 
   bibtex: |
      @techreport{Baker-EtAl:2010:HLTCOE,
           author = {Kathy Baker and Steven Bethard and Michael Bloodgood and Ralf Brown and Chris Callison-Burch and Glen Coppersmith and Bonnie Dorr and Wes Filardo and Kendall Giles and Anni Irvine and Mike Kayser and Lori Levin and Justin Martineau and Jim Mayﬁeld and Scott Miller and Aaron Phillips and Andrew Philpot and Christine Piatko and Lane Schwartz and David Zajic},
           title = {Semantically Informed Machine Translation},
           address = {Human Language Technology Center of Excellence},
           institution = {Johns Hopkins University, Baltimore, MD},
           number = {002},
           url = {http://web.jhu.edu/bin/u/l/HLTCOE-TechReport-002-SIMT.pdf}, 
           year = {2010}
       }
       
-
   title: Fast, Cheap, and Creative&colon; Evaluating Translation Quality Using Amazon's Mechanical Turk
   authors: Chris Callison-Burch
   venue: EMNLP
   type: conference
   award: Nominated for the ACL 2019 Test of Time Award 
   note: This was one of 4 papers nominated for the 10 year Test of Time Award at ACL 2019. 
   year: 2009
   url: https://www.cis.upenn.edu/~ccb/publications/mechanical-turk-for-machine-translation-evaluation.pdf
   page_count: 10
   highly_cited: 632
   id: mechanical-turk-for-machine-translation-evaluation
   figures:
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-figure-1.jpg
         label: Figure 1
         caption: Agreement on ranking translated sentences increases as more non-experts vote. Weighting non-experts' votes based on agreement with either experts or other non-expert increases it up further. Five weighted non-experts reached the top line agreement between experts.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-figure-2.jpg
         label: Figure 2
         caption: The agreement of individual Turkers with the experts. The most prolific Turker performed barely above chance, indicating random clicking. This suggests that users who contribute more tend to have lower quality.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-figure-3.jpg
         label: Figure 3
         caption: Correlation with experts’ ranking of systems. All of the different ways of combining the non-expert judgments perform at the upper bound of expert-expert correlation. All correlate more strongly than Bleu.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-figure-4.jpg
         label: Figure 4
         caption: Bleu scores quantifying the quality of Turkers’ translations. The chart shows the average Bleu score when one LDC translator is compared against the other 10 translators (or the other 2 translators in the case of Urdu). This gives an upper bound on the expected quality. The Turkers’ translation quality falls within a standard deviation of LDC translators for Spanish, German and Chinese. For all languages, Turkers produce significantly better translations than an online machine translation system.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-table-1.jpg
         label: Table 1
         caption: Self-reported demographic information from Turkers who completed the translation HIT. The statistics on the left are for people who appeared to do the task honestly. The statistics on the right are for people who appeared to be using MT (marked as using it 20% or more in the Detect MT HIT).
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-table-2.jpg
         label: Table 2
         caption: HTER scores for five MT systems. The edit rate decreases as the number of editors in- creases from zero (where HTER is simply the TER score between the MT output and the reference translation) and five.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-table-3.jpg
         label: Table 3
         caption: The results of evaluating the MT output using a reading comprehension test
   abstract: Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations. 
   bibtex: |
      @InProceedings{callisonburch:2009:EMNLP,
         author    = {Callison-Burch, Chris},
         title     = {Fast, Cheap, and Creative: Evaluating Translation Quality Using {Amazon's} {Mechanical Turk}},
         booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
         month     = {August},
         year      = {2009},
         address   = {Singapore},
         publisher = {Association for Computational Linguistics},
         pages     = {286--295},
         url       = {http://www.aclweb.org/anthology/D/D09/D09-1030}
       }
       
-
   title: Feasibility of Human-in-the-loop Minimum Error Rate Training
   authors: Omar Zaidan and Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2009
   url: https://www.cis.upenn.edu/~ccb/publications/HMERT.pdf
   page_count: 10
   id: HMERT
   figures:
      -
         img: figures/HMERT/HMERT-figure-1.jpg
         label: Figure 1
         caption: Och’s method applied to a set of two foreign sentences. This figure is essentially a visualization of equation (1). We show here sufficient statistics for TER for simplicity, since there are only 2 of them, but the metric optimized in MERT is usually BLEU.
      -
         img: figures/HMERT/HMERT-figure-2.jpg
         label: Figure 2
         caption: The source parse tree (top) and the candidate derivation tree (bottom). Nodes in the parse tree with a thick border correspond to the frontier node set with maxLen = 4. The human annotator only sees the portion surrounded by the dashed rectangle, including the highlighting (though excluding the word alignment links).
      -
         img: figures/HMERT/HMERT-table-1.jpg
         label: Table 1
         caption: Ranking comparison results. The left half corresponds to the experiment (open to all workers) where the English reference was shown, whereas the right half corresponds to the experiment (open only to workers living in Germany) where the English reference was not shown.
      -
         img: figures/HMERT/HMERT-table-2.jpg
         label: Table 2
         caption: Ranking comparison results, grouped by sentence. This table corresponds to the left half of Table 1. 3 judgments were collected for each comparison, with the “aggregate” for a comparison calculated from these 3 judgments. For instance, an aggregate of “RYPT +3” means all 3 judgments favored RYPT’s choice, and “RYPT +1” means one more judgment favored RYPT than did BLEU.
      -
         img: figures/HMERT/HMERT-figure-3.jpg
         label: Figure 3
         caption: Label percolation under different maxLen values. The bottom two curves are the breakdown of the difference between the middle two. Accuracy is measured against majority votes.
   abstract: Minimum error rate training (MERT) involves choosing parameter values for a machine translation (MT) system that maximize performance on a tuning set as measured by an automatic evaluation metric, such as BLEU. The method is best when the system will eventually be evaluated using the same metric, but in reality, most MT evaluations have a human-based component. Although performing MERT with a human-based metric seems like a daunting task, we describe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU. 
   bibtex: |
      @InProceedings{zaidan-callisonburch:2009:EMNLP,
         author    = {Zaidan, Omar F.  and  Callison-Burch, Chris},
         title     = {Feasibility of Human-in-the-loop Minimum Error Rate Training},
         booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
         month     = {August},
         year      = {2009},
         address   = {Singapore},
         publisher = {Association for Computational Linguistics},
         pages     = {52--61},
         url       = {http://www.aclweb.org/anthology/D/D09/D09-1006}
       }
       
-
   title: Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases
   authors: Yuval Marton, Chris Callison-Burch and Philip Resnik
   venue: EMNLP
   type: conference
   year: 2009
   url: https://www.cis.upenn.edu/~ccb/publications/improved-translation-with-monolingually-derived-paraphrases.pdf
   page_count: 10
   highly_cited: 198
   id: improved-translation-with-monolingually-derived-paraphrases
   figures:
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-1.jpg
         label: Table 1
         caption: Training set sizes (million tokens).
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-2.jpg
         label: Table 2
         caption: E2C Results&colon; character-based BLEU and TER scores. All models have one additional feature over baseline, except for the "1 + 2-6" models that have one feature for unigrams and another feature for bigrams to 6-grams. Paraphrases with score < &colon;3 were filtered out. *** = significance test over baseline with p < 0&colon;0001, using Koehn’s (2004) pair-wise bootstrap resampling test for BLEU with 95% confidence interval.
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-3.jpg
         label: Table 3
         caption: English paraphrases from E2C 29Kbitext systems.
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-4.jpg
         label: Table 4
         caption: S2E Results&colon; Lowercase BLEU and TER. Paraphrases with score < minScore were filtered out. *** = significance test over baseline with p < 0&colon;0001, using Koehn’s (2004) pair-wise bootstrap test for BLEU with 95% confidence interval.
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-5.jpg
         label: Table 5
         caption: Comparison of Spanish paraphrases&colon; by pivoting, and by two monolingual corpora. Ordered from best to worst score.
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-6.jpg
         label: Table 6
         caption: S2E translation examples on 10k-bitext systems. Some translation differences are in bold.
   abstract: Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called “low density” languages. But pivoting requires additional parallel texts. We address this problem by deriving paraphrases monolingually, using distributional semantic similarity measures, thus providing access to larger training resources, such as comparable and unrelated monolingual corpora. We present what is to our knowledge the first successful integration of a collocational approach to untranslated words with an end-to-end, state of the art SMT system demonstrating significant translation improvements in a low-resource setting. 
   bibtex: |
      @InProceedings{marton-callisonburch-resnik:2009:EMNLP,
         author    = {Marton, Yuval  and  Callison-Burch, Chris  and  Resnik, Philip},
         title     = {Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases},
         booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
         month     = {August},
         year      = {2009},
         address   = {Singapore},
         publisher = {Association for Computational Linguistics},
         pages     = {381--390},
         url       = {http://www.aclweb.org/anthology/D/D09/D09-1040}
       }
       
-
   title: Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences
   authors: Nikesh Garera, Chris Callison-Burch and David Yarowsky
   venue: CoNLL
   type: conference
   year: 2009
   url: https://www.cis.upenn.edu/~ccb/publications/improving-translation-lexicon-induction.pdf
   page_count: 9
   id: improving-translation-lexicon-induction
   figures:
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-1.jpg
         label: Figure 1
         caption: Illustration of (Rapp, 1999) model for translating spanish word “crecimiento (growth)” via dependency context vectors extracted from respective monolingual corpora as explained in Section 3.1.2
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-2.jpg
         label: Figure 2
         caption: Illustration of using dependency trees to model richer contexts for projection
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-1.jpg
         label: Table 1
         caption: Contrasting context words derived from the adjacent vs dependency models for the above example.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-2.jpg
         label: Table 2
         caption: Top 10 translation candidates for the spanish word “camino (way)” for the best adjacent context model (Adjbow) and best dependency context model (Depposn). The bold English terms show the acceptable translations.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-3.jpg
         label: Figure 3
         caption: Precision/Recall curve showing superior performance of dependency context model as compared to adjacent context at different recall points. Precision is the fraction of tested Spanish words with Top 1 translation correct and Recall is fraction of the 1000 Spanish words tested upon.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-3.jpg
         label: Table 3
         caption: Performance of various context-based models learned from monolingual corpora and phrase-table learned from parallel corpora on Noun translation.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-4.jpg
         label: Table 4
         caption: List of 20 most confident mappings using the dependency context based model for noun translation. Note that although the first mapping is the correct one, it was not present in the lexicon used for evaluation and hence is marked as incorrect.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-5.jpg
         label: Table 5
         caption: Performance of dependency context-based model along with addition of part-of-speech mapping model on translating all word-types.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-4.jpg
         label: Figure 4
         caption: Illustration of using part-of-speech tag mapping to restrict candidate space of translations
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-5.jpg
         label: Figure 5
         caption: Illustration of mapping Spanish part-of-speech tagset to English tagset. The tagsets vary greatly in notation and the morphological/syntactic constituents represented and need to be mapped first, using the algorithm described in Section 6.1.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-6.jpg
         label: Figure 6
         caption: Precision/Recall curve showing superior performance of using part-of-speech equivalences for translating all word-types. Precision is the fraction of tested Spanish words with Top 1 translation correct and Recall is fraction of the 1000 Spanish words tested upon.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-6.jpg
         label: Table 6
         caption: List of 20 most confident mappings using the dependency context with the part-of-speech mapping model translating all word-types. Note that although the second best mapping in Table4 for noun-translation is for xenofobia with score 0.87, xenofobia is not among the 1000 most frequent words (of all word-types) and thus is not in this test set.
   abstract: This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses. We introduce a dependency-based context model that incorporates long-range dependencies, variable context sizes, and reordering. It provides a 16% relative improvement over the baseline approach that uses a fixed context window of adjacent words. Its Top 10 accuracy for noun translation is higher than that of a statistical translation model trained on a Spanish-English parallel corpus containing 100,000 sentence pairs. We generalize the evaluation to other word-types, and show that the performance can be increased to 18% relative by preserving part-of-speech equivalencies during translation. 
   bibtex: |
      @InProceedings{garera-callisonburch-yarowsky:2009:CoNLL,
         author    = {Garera, Nikesh  and  Callison-Burch, Chris  and  Yarowsky, David},
         title     = {Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences},
         booktitle = {Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)},
         month     = {June},
         year      = {2009},
         address   = {Boulder, Colorado},
         publisher = {Association for Computational Linguistics},
         pages     = {129--137},
         url       = {http://www.aclweb.org/anthology/W09-1117}
       }
       
-
   title: Findings of the 2009 Workshop on Statistical Machine Translation
   authors: Chris Callison-Burch, Philipp Koehn, Christof Monz and  Josh Schroeder
   venue: WMT
   type: workshop
   year: 2009
   url: https://www.cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf
   page_count: 28
   highly_cited: 834
   id: findings-of-the-wmt09-shared-tasks
   figures:
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words is based on the provided tokenizer and the number of distinct words is the based on lowercased tokens.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task. Not all groups participated in all language pairs.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-2.jpg
         label: Table 2
         caption: Participants in the system combination task.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-3.jpg
         label: Table 3
         caption: The number of items that were judged for each task during the manual evaluation.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-figure-2.jpg
         label: Figure 2
         caption: This screenshot shows an annotator editing the output of a machine translation system.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-figure-3.jpg
         label: Figure 3
         caption: This screenshot shows an annotator judging the acceptability of edited translations.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-4.jpg
         label: Table 4
         caption: Inter- and intra-annotator agreement for the two types of manual evaluation
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-6.jpg
         label: Table 6
         caption: Official results for the WMT09 translation task, based on the human evaluation (ranking translations relative to each other)
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-5.jpg
         label: Table 5
         caption: A comparison between the best system combinations and the best individual systems. It was generally difficult to draw a statistically significant differences between the two groups, and between the combinations themselves.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-figure-6.jpg
         label: Figure 6
         caption: The percent of time that each system’s edited output was judged to be an acceptable translation. These numbers also include judgments of the system’s output when it was marked either incomprehensible or acceptable and left unedited. Note that the reference translation was edited alongside the system outputs. Error bars show one positive and one negative standard deviation for the systems in that language pair.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-7.jpg
         label: Table 7
         caption: The system-level correlation of the automatic evaluation metrics with the human judgments for translation into English.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-8.jpg
         label: Table 8
         caption: The system-level correlation of the automatic evaluation metrics with the human judgements for translation out of English.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-9.jpg
         label: Table 9
         caption: The system-level correlation for automatic metrics ranking five English-Czech systems
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-10.jpg
         label: Table 10
         caption: Sentence-level consistency of the automatic metrics with human judgments for translations into English. Italicized numbers do not beat the random-choice baseline. (This table was corrected after publication.)
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-11.jpg
         label: Table 11
         caption: Sentence-level consistency of the automatic metrics with human judgments for translations out of English. Italicized numbers do not beat the random-choice baseline. (This table was corrected after publication.)
   abstract: This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2009:WMT,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Monz, Christof  and  Schroeder, Josh},
         title     = {Findings of the 2009 {W}orkshop on {S}tatistical {M}achine {T}ranslation},
         booktitle = {Proceedings of the Fourth Workshop on Statistical Machine Translation},
         month     = {March},
         year      = {2009},
         address   = {Athens, Greece},
         publisher = {Association for Computational Linguistics},
         pages     = {1--28},
         url       = {http://www.aclweb.org/anthology/W09-0401}
       }
       
-
   title: Joshua&colon; An Open Source Toolkit for Parsing-based Machine Translation
   authors: Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese and Omar Zaidan
   venue: WMT
   type: workshop
   year: 2009
   url: https://www.cis.upenn.edu/~ccb/publications/joshua-open-source-toolkit-for-statistical-machine-translation.pdf
   page_count: 5
   highly_cited: 230
   id: joshua-open-source-toolkit-for-statistical-machine-translation
   figures:
      -
         img: figures/joshua-open-source-toolkit-for-statistical-machine-translation/joshua-open-source-toolkit-for-statistical-machine-translation-table-1.jpg
         label: Table 1
         caption: The uncased BLEU scores on WMT-09 French-English Task. The test set consists of 2525 segments, each with one reference translation.
   abstract: We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs)&colon; chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. 
   bibtex: |
      @InProceedings{li-EtAl:2009:WMT1,
         author    = {Li, Zhifei  and  Callison-Burch, Chris  and  Dyer, Chris  and  Khudanpur, Sanjeev  and  Schwartz, Lane  and  Thornton, Wren  and  Weese, Jonathan  and  Zaidan, Omar},
         title     = {{Joshua}: An Open Source Toolkit for Parsing-Based Machine Translation},
         booktitle = {Proceedings of the Fourth Workshop on Statistical Machine Translation},
         month     = {March},
         year      = {2009},
         address   = {Athens, Greece},
         publisher = {Association for Computational Linguistics},
         pages     = {135--139},
         url       = {http://www.aclweb.org/anthology/W09-0424}
       }
       
-
   title: Decoding in Joshua&colon; Open Source, Parsing-Based Machine Translation
   authors: Zhifei Li, Chris Callison-Burch,  Sanjeev Khudanpur, and Wren Thornton
   venue: PBML
   type: journal
   year: 2009
   url: https://www.cis.upenn.edu/~ccb/publications/decoding-in-joshua.pdf
   page_count: 10
   id: decoding-in-joshua
   figures:
      -
         img: figures/decoding-in-joshua/decoding-in-joshua-table-1.jpg
         label: Table 1
         caption: An example configuration file. For conciseness, this file neglects some standard configuration options (e.g. k-best size).
      -
         img: figures/decoding-in-joshua/decoding-in-joshua-table-2.jpg
         label: Table 2
         caption: Decoder Comparison&colon; Translation speed and quality on the 2003 and 2005 NIST MT benchmark tests.
      -
         img: figures/decoding-in-joshua/decoding-in-joshua-table-3.jpg
         label: Table 3
         caption: Distributed language model&colon; the 7-gram LM cannot be loaded alongside the SCFG on a single machine; via distributed computing, it yields significant improvement in BLEU-4 over a 5-gram.
   abstract: We describe a scalable decoder for parsing-based machine translation. Thee decoder is written in Java and implements all the essential algorithms described in (Chiang, 2007) and (Li and Khudanpur, 2008b)&colon; chart-parsing, n-gram language model integration, beamand cube-pruning, and k-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in Python. 
   bibtex: |
      @article{Li-EtAl:2010:PBML,
          author = {Lane Schwartz and Chris Callison-Burch },
          title = {Hierarchical Phrase-Based Grammar Extraction in Joshua: Suffix Arrays and Prefix Tree},
          journal = {The Prague Bulletin of Mathematical Linguistics},
          volume = {91},
          pages = {47--56},
          year = {2009}
       }
       
-
   title: Syntactic Constraints on Paraphrases Extracted from Parallel Corpora
   authors: Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2008
   url: https://www.cis.upenn.edu/~ccb/publications/syntactic-constraints-on-paraphrases.pdf
   page_count: 10
   highly_cited: 229
   id: syntactic-constraints-on-paraphrases
   figures:
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words is based on the provided tokenizer
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-1.jpg
         label: Table 1
         caption: The baseline method’s paraphrases of equal and their probabilities (excluding items with p < .01).
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-2.jpg
         label: Table 2
         caption: The baseline’s paraphrases of create equal. Most are clearly bad, and the most probable e2 ̸= e1 is a sub- string of e1 .
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-3.jpg
         label: Table 3
         caption: Syntactically constrained paraphrases for equal when it is labeled as an adjective or adjectival phrase.
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-figure-2.jpg
         label: Figure 2
         caption: In addition to extracting phrases that are dominated by a node in the parse tree, we also generate labels for non-syntactic constituents. Three labels are possible for create equal.
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-4.jpg
         label: Table 4
         caption: Paraphrases and syntactic labels for the non- constituent phrase create equal.
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-5.jpg
         label: Table 5
         caption: Annotators rated paraphrases along two 5-point scales.
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-6.jpg
         label: Table 6
         caption: The results of the manual evaluation for each of the eight conditions. Correct meaning is the percent of time that a condition was assigned a 3, 4, or 5, and correct grammar is the percent of time that it was given a 4 or 5, using the scales from Table 5.
   abstract: We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method. 
   bibtex: |
      @InProceedings{callisonburch:2008:EMNLP,
         author    = {Callison-Burch, Chris},
         title     = {Syntactic Constraints on Paraphrases Extracted from Parallel Corpora},
         booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
         month     = {October},
         year      = {2008},
         address   = {Honolulu, Hawaii},
         publisher = {Association for Computational Linguistics},
         pages     = {196--205},
         url       = {http://www.aclweb.org/anthology/D08-1021}
       }
       
-
   title: ParaMetric&colon; An Automatic Evaluation Metric for Paraphrasing
   authors: Chris Callison-Burch, Trevor Cohn, Mirella Lapata
   venue: CoLing
   type: conference
   year: 2008
   url: https://www.cis.upenn.edu/~ccb/publications/parametric.pdf
   page_count: 8
   id: parametric
   figures:
      -
         img: figures/parametric/parametric-figure-1.jpg
         label: Figure 1
         caption: Pairs of English sentences were aligned by hand. Black squares indicate paraphrase corre- spondences.
      -
         img: figures/parametric/parametric-table-1.jpg
         label: Table 1
         caption: Non-identical words and phrases which are identified as being in correspondence by the alignments in Figure 1.
      -
         img: figures/parametric/parametric-figure-2.jpg
         label: Figure 2
         caption: Pang et al. (2003) created word graphs by merging parse trees. Paths with the same start and end nodes are treated as paraphrases.
      -
         img: figures/parametric/parametric-figure-3.jpg
         label: Figure 3
         caption: Bannard and Callison-Burch (2005) extracted paraphrases by equating English phrases that share a common translation.
      -
         img: figures/parametric/parametric-table-2.jpg
         label: Table 2
         caption: Summary results for scoring the different paraphrasing techniques using our proposed automatic evaluations.
      -
         img: figures/parametric/parametric-table-3.jpg
         label: Table 3
         caption: Results for paraphrases of continuous subphrases of various lengths.
   abstract: We present ParaMetric, an automatic evaluation metric for data-driven approaches to paraphrasing. ParaMetric provides an objective measure of quality using a collection of multiple translations whose paraphrases have been manually annotated. ParaMetric calculates precision and recall scores by comparing the paraphrases discovered by automatic paraphrasing techniques against gold standard alignments of words and phrases within equivalent sentences. We report scores for several established paraphrasing techniques. 
   bibtex: |
      @InProceedings{callisonburch-cohn-lapata:2008:Coling,
         author    = {Callison-Burch, Chris  and  Cohn, Trevor  and  Lapata, Mirella},
         title     = {ParaMetric: An Automatic Evaluation Metric for Paraphrasing},
         booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)},
         month     = {August},
         year      = {2008},
         address   = {Manchester, UK},
         publisher = {Coling 2008 Organizing Committee},
         pages     = {97--104},
         url       = {http://www.aclweb.org/anthology/C08-1013}
       }
       
-
   title: Further Meta-Evaluation of Machine Translation
   authors: Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz and  Josh Schroeder
   venue: WMT
   type: workshop
   year: 2008
   url: https://www.cis.upenn.edu/~ccb/publications/further-meta-evaluation-of-machine-tranlsion.pdf
   page_count: 37
   highly_cited: 306
   id: further-meta-evaluation-of-machine-tranlsion
   figures:
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-1.jpg
         label: Table 1
         caption: Difficulty of the test set parts based on the original language. For each part, we average BLEU scores from the Edinburgh systems for 12 language pairs of the shared task.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-figure-1.jpg
         label: Figure 1
         caption: Properties of the training and test sets used in the shared task. The training data is drawn from the Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple languages. For Czech and Hungarian we use other available parallel corpora. Note that the number of words is computed based on the provided tokenizer and that the number of distinct words is the based on lowercased tokens.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-2.jpg
         label: Table 2
         caption: Participants in the shared translation task. Not all groups participated in all language pairs.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-figure-2.jpg
         label: Figure 2
         caption: In constituent-based evaluation, the source sentence was parsed, and automatically aligned with the reference translation and systems’ translations
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-3.jpg
         label: Table 3
         caption: The number of items that were judged for each task during the manual evaluation. The All-English judgments were reused in the News task for individual language pairs.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-4.jpg
         label: Table 4
         caption: Summary results for the sentence ranking judgments. The numbers report the percent of time that each system was judged to be greater than or equal to any other system. Bold indicates the highest score for that task.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-5.jpg
         label: Table 5
         caption: Summary results for the constituent ranking judgments. The numbers report the percent of time that each system was judged to be greater than or equal to any other system. Bold indicates the highest score for that task.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-6.jpg
         label: Table 6
         caption: Summary results for the Yes/No judgments for constituent translations judgments. The numbers report the percent of each system’s translations that were judged to be acceptable. Bold indicates the highest score for that task.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-7.jpg
         label: Table 7
         caption: The average number of times that each system was judged to be better than or equal to all other systems in the sentence ranking task for the All-English condition. The subscript indicates the source language of the system.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-9.jpg
         label: Table 9
         caption: Average system-level correlations for the automatic evaluation metrics on translations into French, German and Spanish (This table was corrected after publication)
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-8.jpg
         label: Table 8
         caption: Average system-level correlations for the automatic evaluation metrics on translations into English (This table was corrected after publication)
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-10.jpg
         label: Table 10
         caption: The percent of time that each automatic metric was consistent with human judgments for translations into English
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-11.jpg
         label: Table 11
         caption: The percent of time that each automatic metric was consistent with human judgments for translations into other languages
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-figure-3.jpg
         label: Figure 3
         caption: Distributions of the amount of time it took to judge single sentences for the three types of man- ual evaluation
   abstract: This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information.Note&colon; This paper was corrected subsequent to publication.
   bibtex: |
      @InProceedings{callisonburch-EtAl:2008:WMT,
         author    = {Callison-Burch, Chris  and  Fordyce, Cameron  and  Koehn, Philipp  and  Monz, Christof  and  Schroeder, Josh},
         title     = {Further Meta-Evaluation of Machine Translation},
         booktitle = {Proceedings of the Third Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2008},
         address   = {Columbus, Ohio},
         publisher = {Association for Computational Linguistics},
         pages     = {70--106},
         url       = {http://www.aclweb.org/anthology/W/W08/W08-0309}
       }
       
-
   title: Constructing Corpora for the Development and Evaluation of Paraphrase Systems
   authors: Trevor Cohn, Chris Callison-Burch, Mirella Lapata
   venue: Computational Linguistics
   type: journal
   year: 2008
   url: https://www.cis.upenn.edu/~ccb/publications/constructing-corpora-for-paraphrase-systems.pdf
   page_count: 18
   id: constructing-corpora-for-paraphrase-systems
   figures:
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-1.jpg
         label: Figure 1
         caption: Manual alignment between two sentence pairs from the MTC corpus, displayed as a grid.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-2.jpg
         label: Figure 2
         caption: Sample sentence pair showing the word alignments from two annotators.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-1.jpg
         label: Table 1
         caption: Single word pairs specified by the word alignments from Figure 2, for two annotators A and B. The column entries specify the alignment type for each annotator, either sure (S) or possible (P). Dashes indicate that the word-pair was not predicted by the annotator. Italics denote lexically identical word pairs.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-3.jpg
         label: Figure 3
         caption: Validity of phrase pairs according to the phrase extraction heuristic. Only the leftmost phrase pair is valid. The others are inconsistent with the alignment or have an unaligned word on a boundary, respectively, indicated by a cross.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-2.jpg
         label: Table 2
         caption: Phrase pairs specified by the word alignments from Figure 2, using the possible alignments. The entire set of atomic phrase pairs for either annotator (labeled A or B) are shown and a selection of the remaining 57 composite phrase pairs. The italics denote lexically identical phrase pairs. ∗This phrase pair is atomic in A but composite in B.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-3.jpg
         label: Table 3
         caption: Inter-annotator agreement using precision, recall, F1, and Cˆ; the agreement is measured over words in the (monolingual) parallel corpus.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-4.jpg
         label: Table 4
         caption: Inter-annotator agreement using precision, recall, F1 and Cˆ; the agreement is measured over atomic phrase pairs in the (monolingual) parallel corpus.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-4.jpg
         label: Figure 4
         caption: Agreement statistics plotted against sentence length for the three sub-corpora. Each group of three columns correspond to πˆ, πˆ0 and Cˆ, respectively. The statistics were measured over non-identical phrase pairs using all phrase pairs&colon; atomic and composite.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-5.jpg
         label: Table 5
         caption: Agreement between automatic Giza++ predicted word-alignments and our manually corrected alignments, measured over atomic phrase pairs.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-5.jpg
         label: Figure 5
         caption: Synchronous grammar rules extracted from the MTC sub-corpus.
   abstract: Automatic paraphrasing is an important component in many natural language processing tasks. In this paper we present a new parallel corpus with paraphrase annotations. We adopt a definition of paraphrase based on word-alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall and F1) and also in developing linguistically rich paraphrase models based on syntactic structure 
   bibtex: |
      @article{cohn-callisonburch-lapata:2008:CL,
         author =  {Trevor Cohn and Chris Callison-Burch and Mirella Lapata},
         title =   {Constructing Corpora for the Development and Evaluation of Paraphrase Systems},
         journal = {Computational Linguistics},
         year =    {2008},
         volume = {34},
         number = {4},
         pages = {597--614}
       }
       
-
   title: Affinity Measures based on the Graph Laplacian
   authors: Delip Rao, David Yarowsky, Chris Callison-Burch
   venue: of the 3rd Textgraphs workshop on Graph-based Algorithms for Natural Language Processing at CoLing
   type: workshop
   year: 2008
   url: https://www.cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf
   page_count: 8
   id: graph-laplacian-affinity-measures
   figures:
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-figure-1.jpg
         label: Figure 1
         caption: Shortest path distances on graphs
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-1.jpg
         label: Table 1
         caption: Similarity using shortest-path measure
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-2.jpg
         label: Table 2
         caption: Similarity using bounded random walks (m = 20).
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-figure-2.jpg
         label: Figure 2
         caption: Effect of m in Bounded walk
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-3.jpg
         label: Table 3
         caption: Similarity via pagerank (® = 0.1).
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-5.jpg
         label: Table 5
         caption: Denoising graph Laplacian via SVD
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-4.jpg
         label: Table 4
         caption: Similarity via inverse Laplacian.
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-figure-3.jpg
         label: Figure 3
         caption: Noise reduction via SVD.
   abstract: Several language processing tasks can be inherently represented by a weighted graph where the weights are interpreted as a measure of relatedness between two vertices. Measuring similarity between arbitrary pairs of vertices is essential in solving several language processing problems on these datasets. Random walk based measures perform better than other path based measures like shortest-path. We evaluate several random walk measures and propose a new measure based on commute time. We use the psuedo inverse of the Laplacian to derive estimates for commute times in graphs. Further, we show that this pseudo inverse based measure could be improved by discarding the least significant eigenvectors, corresponding to the noise in the graph construction process, using singular value decomposition. 
   bibtex: |
      @InProceedings{rao-yarowsky-callisonburch:2008:TG3,
         author    = {Rao, Delip  and  Yarowsky, David  and  Callison-Burch, Chris},
         title     = {Affinity Measures Based on the Graph {L}aplacian},
         booktitle = {Coling 2008: Proceedings of the 3rd Textgraphs workshop on Graph-based Algorithms for Natural Language Processing},
         month     = {August},
         year      = {2008},
         address   = {Manchester, UK},
         publisher = {Coling 2008 Organizing Committee},
         pages     = {41--48},
         url       = {http://www.aclweb.org/anthology/W08-2006}
       }
       
-
   title: Paraphrasing and Translation
   authors: Chris Callison-Burch
   venue: PhD Thesis, University of Edinburgh
   type: thesis
   year: 2007
   url: https://www.cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf
   page_count: 206
   id: callison-burch-thesis
   figures:
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-1.jpg
         label: Figure 1
         caption: The Spanish word cada ́veres can be used to discover that the English phrase dead bodies can be paraphrased as corpses.
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-2.jpg
         label: Figure 2
         caption: Barzilay and McKeown (2001) extracted paraphrases from multiple transla- tions using identical surrounding substrings
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-3.jpg
         label: Figure 3
         caption: A phrase can be aligned to many foreign phrases, which in turn can be aligned to multiple possible paraphrases
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-4.jpg
         label: Figure 4
         caption: In machine translation evaluation the following scales are used by judges to assign adequacy and fluency scores to each translation
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-5.jpg
         label: Figure 5
         caption: Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-6.jpg
         label: Figure 6
         caption: Scatterplot of the length of each translation against its number of possible permutations due to bigram mismatches for an entry in the 2005 NIST MT Eval
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-7.jpg
         label: Figure 7
         caption: The decoder for the baseline system has translation options only for those words which have phrases that occur in the phrase table. In this case there are no translations for the source word votare ́.
   abstract: Paraphrasing and translation have previously been treated as unconnected natural language processing tasks. Whereas translation represents the preservation of meaning when an idea is rendered in the words in a different language, paraphrasing represents the preservation of meaning when an idea is expressed using different words in the same language. We show that the two are intimately related. The major contributions of this thesis are as follows&colon;We define a novel technique for automatically generating paraphrases using bilingual parallel corpora, which are more commonly used as training data for statistical models of translation.We show that paraphrases can be used to improve the quality of statistical machine translation by addressing the problem of coverage and introducing a degree of generalization into the models.We explore the topic of automatic evaluation of translation quality, and show that the current standard evaluation methodology cannot be guaranteed to correlate with human judgments of translation quality.Whereas previous data-driven approaches to paraphrasing were dependent upon either data sources which were uncommon such as multiple translation of the same source text, or language specific resources such as parsers, our approach is able to harness more widely parallel corpora and can be applied to any language which has a parallel corpus. The technique was evaluated by replacing phrases with their paraphrases, and asking judges whether the meaning of the original phrase was retained and whether the resulting sentence remained grammatical. Paraphrases extracted from a parallel corpus with manual alignments are judged to be accurate (both meaningful and grammatical) 75% of the time, retaining the meaning of the original phrase 85% of the time. Using automatic alignments, meaning can be retained at a rate of 70%.Being a language independent and probabilistic approach allows our method to be easily integrated into statistical machine translation. A paraphrase model derived from parallel corpora other than the one used to train the translation model can be used to increase the coverage of statistical machine translation by adding translations of previously unseen words and phrases. If the translation of a word was not learned, but a translation of a synonymous word has been learned, then the word is paraphrased and its paraphrase is translated. Phrases can be treated similarly. Results show that augmenting a state-of-the-art SMT system with paraphrases in this way leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs, we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches.
   bibtex: |
      @PhdThesis{callisonburch:2007:thesis,
         author =  {Chris Callison-Burch},
         title =   {Paraphrasing and Translation},
         school = {University of Edinburgh},
         address =   {Edinburgh, Scotland},
         year =    {2007},
         url = {http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf}
       }
       
-
   title: (Meta-) Evaluation of Machine Translation
   authors: Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz and  Josh Schroeder
   venue: WMT
   type: workshop
   year: 2007
   url: https://www.cis.upenn.edu/~ccb/publications/meta-evaluation-of-machine-tranlsion.pdf
   page_count: 
   highly_cited: 188
   id: meta-evaluation-of-machine-tranlsion
   figures:
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-figure-1.jpg
         label: Figure 1
         caption: Properties of the training and test sets used in the shared task. The training data is drawn from the Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple languages.
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-1.jpg
         label: Table 1
         caption: Participants in the shared task. Not all groups participated in all translation directions.
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-figure-2.jpg
         label: Figure 2
         caption: In constituent-based evaluation, the source sentence was parsed, and automatically aligned with the reference translation and systems’ translations
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-figure-3.jpg
         label: Figure 3
         caption: For each of the types of evaluation, judges were shown screens containing up to five different system translations, along with the source sentence and reference translation.
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-2.jpg
         label: Table 2
         caption: The number of items that were judged for each task during the manual evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-3.jpg
         label: Table 3
         caption: The proportion of time that participants’ entries were top-ranked in the human evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-4.jpg
         label: Table 4
         caption: The proportion of time that participants’ entries were top-ranked by the automatic evaluation metrics
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-5.jpg
         label: Table 5
         caption: Kappa coefficient values representing the inter-annotator agreement for the different types of manual evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-6.jpg
         label: Table 6
         caption: Kappa coefficient values for intra-annotator agreement for the different types of manual evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-figure-4.jpg
         label: Figure 4
         caption: Distributions of the amount of time it took to judge single sentences for the three types of manual evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-7.jpg
         label: Table 7
         caption: Average corrections for the different automatic metrics when they are used to evaluate translations into English
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-8.jpg
         label: Table 8
         caption: Average corrections for the different automatic metrics when they are used to evaluate translations into the other languages
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-9.jpg
         label: Table 9
         caption: Human evaluation for German-English submissions
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-10.jpg
         label: Table 10
         caption: Human evaluation for Spanish-English submissions
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-11.jpg
         label: Table 11
         caption: Human evaluation for French-English submissions
   abstract: This paper evaluates the translation quality of machine translation systems for 8 language pairs&colon; translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intra- and inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2007:WMT,
         author    = {Callison-Burch, Chris  and  Fordyce, Cameron  and  Koehn, Philipp  and  Monz, Christof  and  Schroeder, Josh},
         title     = {(Meta-) Evaluation of Machine Translation},
         booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2007},
         address   = {Prague, Czech Republic},
         publisher = {Association for Computational Linguistics},
         pages     = {136--158},
         url       = {http://www.aclweb.org/anthology/W/W07/W07-0718}
       }
       
-
   title: Open Source Toolkit for Statistical Machine Translation&colon; Factored Translation Models and Confusion Network Decoding
   authors: Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris Callison-Burch, Alexandra Constantin, Brooke Cowan, Chris Dyer, Marcello Federico, Evan Herbst, Hieu Hoang, Christine Moran, Wade Shen, and Richard Zens
   venue: CLSP Summer Workshop Final Report WS, Johns Hopkins University
   type: workshop
   year: 2007
   url: https://www.cis.upenn.edu/~ccb/publications/open-source-toolkit-for-statistical-machine-translation.pdf
   page_count: 
   id: open-source-toolkit-for-statistical-machine-translation
   abstract: The 2006 Language Engineering Workshop Open Source Toolkit for Statistical Machine Translationhad the objective to advance the current state-of-the-art in statistical machine translation through richer input and richer annotation of the training data. The workshop focused on three topics&colon; factored translation models, confusion network decoding, and the development of an open source toolkit that incorporates this advancements. This report describes the scientific goals, the novel methods, and experimental results of the workshop. It also documents details of the implementation of the open source toolkit. 
   bibtex: |
      @techreport{Koehn-EtAl:2007:CLSP,
          author = { Philipp Koehn and Nicola Bertoldi and Ondrej Bojar and Chris Callison-Burch and Alexandra Constantin and  Brooke Cowan and Chris Dyer and Marcello Federico and Evan Herbst and Hieu Hoang and Christine Moran and Wade Shen and Richard Zens},
          title = {Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Confusion Network Decoding. },
          institution = {Johns Hopkins University},
          number = {WS-2006},
          type = {CLSP Summer Workshop Final Report},
          year = {2007}
       }
       
-
   title: Moses&colon; Open source toolkit for statistical machine translation
   authors: Philipp Koehn, Hieu Hoang,  Alexandra Birch,  Chris Callison-Burch,    Marcello Federico,  Nicola Bertoldi,   Brooke Cowan,  Wade Shen,  Christine Moran,  Richard Zens, Chris Dyer, Ondřej Bojar,  Alexandra Constantin,  and Evan Herbst
   venue: ACL
   type: conference
   year: 2007
   url: https://www.cis.upenn.edu/~ccb/publications/moses-toolkit.pdf
   page_count: 
   id: moses-toolkit
   highly_cited: 7047
   figures:
      -
         img: figures/moses-toolkit/moses-toolkit-figure-1.jpg
         label: Figure 1
         caption: Non-factored translation
      -
         img: figures/moses-toolkit/moses-toolkit-figure-2.jpg
         label: Figure 2
         caption: Factored translation
      -
         img: figures/moses-toolkit/moses-toolkit-figure-3.jpg
         label: Figure 3
         caption: Example of graph of decoding steps
   abstract: We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 
   bibtex: |
      @InProceedings{koehn-EtAl:2007:PosterDemo,
         author    = {Koehn, Philipp  and  Hoang, Hieu  and  Birch, Alexandra  and  Callison-Burch, Chris  and  Federico, Marcello  and  Bertoldi, Nicola  and  Cowan, Brooke  and  Shen, Wade  and  Moran, Christine  and  Zens, Richard  and  Dyer, Chris  and  Bojar, Ondrej  and  Constantin, Alexandra  and  Herbst, Evan},
         title     = {Moses: Open Source Toolkit for Statistical Machine Translation},
         booktitle = {Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions},
         month     = {June},
         year      = {2007},
         address   = {Prague, Czech Republic},
         publisher = {Association for Computational Linguistics},
         pages     = {177--180},
         url       = {http://www.aclweb.org/anthology/P07-2045}
       }
       
-
   title: Paraphrase Substitution for Recognizing Textual Entailment
   authors: Wauter Bosma and Chris Callison-Burch
   venue: Evaluation of Multilingual and Multimodalformation Retrieval, Lecture Notes in Computer Science, C Peters et al editors
   type: chapter
   year: 2007
   url: https://www.cis.upenn.edu/~ccb/publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf
   page_count: 
   id: paraphrase-substitution-for-recognizing-textual-entailment
   figures:
      -
         img: figures/paraphrase-substitution-for-recognizing-textual-entailment/paraphrase-substitution-for-recognizing-textual-entailment-table-1.jpg
         label: Table 1
         caption: Examples paraphrases and probabilities for the phrase dead bodies
      -
         img: figures/paraphrase-substitution-for-recognizing-textual-entailment/paraphrase-substitution-for-recognizing-textual-entailment-figure-1.jpg
         label: Figure 1
         caption: A bilingual parallel corpus can be used to extract paraphrases
   abstract: We describe a method for recognizing textual entailment that uses the length of the longest common subsequence (LCS) between two texts as its decision criterion. Rather than requiring strict word matching in the common subsequences, we perform a flexible match using automatically generated paraphrases. We find that the use of paraphrases over strict word matches represents an average F-measure improvement from 0.22 to 0.36 on the CLEF 2006 Answer Validation Exercise for 7 languages. 
   bibtex: |
      @InProceedings{bosma-callisonburch:2006:CLEF,
         author = {Wauter Bosma and Chris Callison-Burch},
         title = {Paraphrase Substitution for Recognizing Textual Entailment},
         booktitle = {Proceedings of CLEF},
         year = {2006}
         url = {http://cis.upenn.edu/~ccb/publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf}
       }
       
-
   title: Improved Statistical Machine Translation Using Paraphrases
   authors: Chris Callison-Burch, Philipp Koehn and Miles Osborne
   venue: NAACL
   type: conference
   year: 2006
   url: https://www.cis.upenn.edu/~ccb/publications/improved-statistical-machine-translation-using-paraphrases.pdf
   page_count: 
   highly_cited: 391
   id: improved-statistical-machine-translation-using-paraphrases
   figures:
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-figure-1.jpg
         label: Figure 1
         caption: Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-table-1.jpg
         label: Table 1
         caption: Example of automatically generated para- phrases for the Spanish words encargarnos and us- ado along with their English translations which were automatically learned from the Europarl corpus
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-figure-2.jpg
         label: Figure 2
         caption: Using a bilingual parallel corpus to extract paraphrases
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-figure-4.jpg
         label: Figure 4
         caption: Judges were asked whether the highlighted phrase retained the same meaning as the highlighted phrase in the reference translation (top)
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-figure-3.jpg
         label: Figure 3
         caption: Test sentences and reference translations were manually word-aligned. This allowed us to equate unseen phrases with their corresponding English phrase. In this case enumeradas with listed.
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-table-2.jpg
         label: Table 2
         caption: Bleu scores for the various training corpora, including baseline results without paraphrasing, results for only paraphrasing unknown words, and results for paraphrasing any unseen phrase. Corpus size is measured in sentences.
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-table-3.jpg
         label: Table 3
         caption: Bleu scores for the various training corpora, when the paraphrase feature function is not included
   abstract: Parallel corpora are crucial for training SMT systems. However, for many language pairs they are available only in very limited quantities. For these language pairs a huge portion of phrases encountered at run-time will be unknown. We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. Our results show that augmenting a stateof-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches. 
   bibtex: |
      @InProceedings{callisonburch-koehn-osborne:2006:HLT-NAACL06-Main,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Osborne, Miles},
         title     = {Improved Statistical Machine Translation Using Paraphrases},
         booktitle = {Proceedings of the Human Language Technology Conference of the NAACL, Main Conference},
         month     = {June},
         year      = {2006},
         address   = {New York City, USA},
         publisher = {Association for Computational Linguistics},
         pages     = {17--24},
         url       = {http://www.aclweb.org/anthology/N/N06/N06-1003}
       }
       
-
   title: Re-evaluating the Role of Bleu in Machine Translation Research
   authors: Chris Callison-Burch, Miles Osborne and Philipp Koehn
   venue: EACL
   type: conference
   year: 2006
   url: https://www.cis.upenn.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf
   page_count: 8
   id: re-evaluating-the-role-of-bleu-in-mt-research
   highly_cited: 960
   abstract: We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 
   bibtex: |
      @InProceedings{callisonburch-koehn-osborne:2006:HLT-NAACL06-Main,
         author    = {Callison-Burch, Chris  and  Osborne, Miles and  Koehn, Philipp},
         title     = {Re-evaluating the Role of BLEU in Machine Translation Research},
         booktitle = {11th Conference of the European Chapter of the Association for Computational Linguistics},
         month     = {April},
         year      = {2006},
         address   = {Trento, Italy},
         publisher = {Association for Computational Linguistics},
         pages     = {249--256},
         url       = {http://aclweb.org/anthology-new/E/E06/E06-1032}
       }
       
-
   title: Constraining the Phrase-Based, Joint Probability Statistical Translation Model
   authors: Alexandra Birch, Chris Callison-Burch and Miles Osborne
   venue: WMT
   type: workshop
   year: 2006
   url: https://www.cis.upenn.edu/~ccb/publications/constraining-the-phrase-based-joint-probability-model.pdf
   page_count: 
   id: constraining-the-phrase-based-joint-probability-model
   figures:
      -
         img: figures/constraining-the-phrase-based-joint-probability-model/constraining-the-phrase-based-joint-probability-model-table-1.jpg
         label: Table 1
         caption: The number of possible phrasal alignments for sentence pairs calculated using Stirling numbers of the second kind.
      -
         img: figures/constraining-the-phrase-based-joint-probability-model/constraining-the-phrase-based-joint-probability-model-table-2.jpg
         label: Table 2
         caption: Bleu scores for the Joint Model with IBM constraints and prior counts, corpus size indicates number of sentence pairs
      -
         img: figures/constraining-the-phrase-based-joint-probability-model/constraining-the-phrase-based-joint-probability-model-table-3.jpg
         label: Table 3
         caption: Translation table size in millions of phrase pairs
   abstract: The Joint Probability Model proposed by Marcu and Wong (2002) provides a probabilistic framework for modeling phrase-based statistical machine translation (SMT). The model’s usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present a method of constraining the search space of the Joint Probability Model based on statistically and linguistically motivated word alignments. This method reduces the complexity and size of the Joint Model and allows it to display performance superior to the standard phrase-based models for small amounts of training material. 
   bibtex: |
      @InProceedings{birch-EtAl:2006:WMT,
         author    = {Birch, Alexandra  and  Callison-Burch, Chris  and  Osborne, Miles  and  Koehn, Philipp},
         title     = {Constraining the Phrase-Based, Joint Probability Statistical Translation Model},
         booktitle = {Proceedings on the Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2006},
         address   = {New York City},
         publisher = {Association for Computational Linguistics},
         pages     = {154--157},
         url       = {http://www.aclweb.org/anthology/W/W06/W06-3123}
       }
       
-
   title: Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases
   authors: Chris Callison-Burch, Colin Bannard and Josh Schroeder
   venue: ACL
   type: conference
   year: 2005
   url: https://www.cis.upenn.edu/~ccb/publications/scaling-phrase-based-statistical-machine-translation.pdf
   page_count: 
   highly_cited: 116
   id: scaling-phrase-based-statistical-machine-translation
   figures:
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-1.jpg
         label: Table 1
         caption: Statistics about Arabic phrases in the NIST-2004 large data track.
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-2.jpg
         label: Table 2
         caption: Estimated size of lookup tables for the NIST-2004 Arabic-English data
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-3.jpg
         label: Table 3
         caption: Lengths of phrases from the training data that occur in the NIST-2004 test set
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-4.jpg
         label: Table 4
         caption: Coverage using only repeated phrases of the specified length
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-figure-2.jpg
         label: Figure 2
         caption: A sorted suffix array and its corresponding suffixes
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-figure-1.jpg
         label: Figure 1
         caption: An initialized, unsorted suffix array for a very small corpus
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-5.jpg
         label: Table 5
         caption: Examples of O and calculation times for phrases of different frequencies
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-6.jpg
         label: Table 6
         caption: A comparison of retrieval times and translation quality when the number of translations is capped at various sample sizes
   abstract: In this paper we describe a novel data structure for phrase-based statistical machine translation which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations. We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array-based data structure. We show how sampling can be used to reduce the retrieval time by orders of magnitude with no loss in translation quality. 
   bibtex: |
      @InProceedings{callisonburch-bannard-schroeder:2005:ACL,
         author    = {Callison-Burch, Chris  and  Bannard, Colin  and  Schroeder, Josh},
         title     = {Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases},
         booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)},
         month     = {June},
         year      = {2005},
         address   = {Ann Arbor, Michigan},
         publisher = {Association for Computational Linguistics},
         pages     = {255--262},
         url       = {http://www.aclweb.org/anthology/P05-1032},
       }
       
-
   title: Paraphrasing with Bilingual Parallel Corpora
   authors: Colin Bannard and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2005
   url: https://www.cis.upenn.edu/~ccb/publications/paraphrasing-with-bilingual-parallel-corpora.pdf
   page_count: 
   id: paraphrasing-with-bilingual-parallel-corpora
   highly_cited: 772
   figures:
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: Using a monolingual parallel corpus to extract paraphrases
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: Using a bilingual parallel corpus to extract paraphrases
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-table-1.jpg
         label: Table 1
         caption: Phrases that were selected to paraphrase
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: Phrases highlighted for manual alignment
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-figure-4.jpg
         label: Figure 4
         caption: Paraphrases substituted in for the original phrase
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-table-2.jpg
         label: Table 2
         caption: Paraphrases extracted from a manually word-aligned parallel corpus
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-table-3.jpg
         label: Table 3
         caption: Paraphrase accuracy and correct meaning for the different data conditions
   abstract: Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments. 
   bibtex: |
      @InProceedings{bannard-callisonburch:2005:ACL,
         author    = {Bannard, Colin  and  Callison-Burch, Chris},
         title     = {Paraphrasing with Bilingual Parallel Corpora},
         booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)},
         month     = {June},
         year      = {2005},
         address   = {Ann Arbor, Michigan},
         publisher = {Association for Computational Linguistics},
         pages     = {597--604},
         url       = {http://www.aclweb.org/anthology/P05-1074},
       }
       
-
   title: A Compact Data Structure for Searchable Translation Memories
   authors: Chris Callison-Burch, Colin Bannard and Josh Schroeder
   venue: EAMT
   type: workshop
   year: 2005
   url: https://www.cis.upenn.edu/~ccb/publications/compact-data-structure-for-searchable-translation-memories.pdf
   page_count: 
   id: compact-data-structure-for-searchable-translation-memories
   figures:
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-1.jpg
         label: Figure 1
         caption: Search results for the English phrase “west bank”
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-2.jpg
         label: Figure 2
         caption: A word-level alignment for a sentence pair that occurs in our training data
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-3.jpg
         label: Figure 3
         caption: An initialized, unsorted suffix array for a very small corpus
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-4.jpg
         label: Figure 4
         caption: A sorted suffix array and its corresponding suffixes
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-5.jpg
         label: Figure 5
         caption: A word-level alignment for the sentence in the suffix array
   abstract: In this paper we describe searchable translation memories, which allow translators to search their archives for possible translations of phrases. We describe how statistical machine translation can be used to align sub-sentential units in a translation memory, and rank them by their probability. We detail a data structure that allows for memory-efficient storage of the index. We evaluate the accuracy of translations retrieved from a searchable translation memory built from 50,000 sentence pairs, and find a precision of 86.6% for the top ranked translations. 
   bibtex: |
      @InProceedings{callison-burch-EtAl:2005:EAMT,
         author =  {Chris Callison-Burch and Colin Bannard and Josh Schroeder},
         title =           {A Compact Data Structure for Searchable Translation Memories},
           booktitle = {European Association for Machine Translation}, 
       year =  {2005}
       }
       
-
   title: Linear B System Description for the 2005 NIST MT Evaluation Exercise
   authors: Chris Callison-Burch
   venue: Machine Translation Evaluation Workshop
   type: workshop
   year: 2005
   url: https://www.cis.upenn.edu/~ccb/publications/linear-b-system-description-for-nist-mt-eval-2005.pdf
   page_count: 
   id: linear-b-system-description-for-nist-mt-eval-2005
   figures:
      -
         img: figures/linear-b-system-description-for-nist-mt-eval-2005/linear-b-system-description-for-nist-mt-eval-2005-figure-1.jpg
         label: Figure 1
         caption: In the simple editing condition subjects simply edited the output of a fully-automatic statistical machine translation system
      -
         img: figures/linear-b-system-description-for-nist-mt-eval-2005/linear-b-system-description-for-nist-mt-eval-2005-figure-2.jpg
         label: Figure 2
         caption: In the visualization condition subjects first constructed the translations by selecting from a set of probable translations of the phrases in each Arabic sentence
      -
         img: figures/linear-b-system-description-for-nist-mt-eval-2005/linear-b-system-description-for-nist-mt-eval-2005-figure-3.jpg
         label: Figure 3
         caption: A comparison of Linear B’s human-aided and Edinburgh University’s fully automatic translation for article AFP20041201.0189
      -
         img: figures/linear-b-system-description-for-nist-mt-eval-2005/linear-b-system-description-for-nist-mt-eval-2005-figure-4.jpg
         label: Figure 4
         caption: A comparison for article XIA20050101.0119
   abstract: This document describes Linear B’s entry for the 2005 NIST MT Evaluation exercise. Linear B examined the efficacy of human-aided statistical machine translation by looking at the improvements that could be had by involving non-Arabic speakers in the translation process. We examined two conditions&colon; one in which non-Arabic speakers edited the output of a statistical machine translation system, and one in which they were allowed to select phrasal translations from a chart of possible translations for an Arabic sentence, and then edit the text. 
   bibtex: |
      @InProceedings{callisonburch:2005:NIST,
         author =  {Chris Callison-Burch },
         title =           {A Compact Data Structure for Searchable Translation Memories},
           booktitle = {Proceedings of Machine Translation Evaluation Workshop}, 
       year =  {2005}
       }
       
-
   title: Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation
   authors: Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot
   venue: IWSLT
   type: workshop
   year: 2005
   url: https://www.cis.upenn.edu/~ccb/publications/iwslt05-report.pdf
   page_count: 
   highly_cited: 446
   id: iwslt05-report
   figures:
      -
         img: figures/iwslt05-report/iwslt05-report-figure-1.jpg
         label: Figure 1
         caption: Phrase-based SMT&colon; Input is segmented into phrases, each is mapped into output phrase and may be reordered
      -
         img: figures/iwslt05-report/iwslt05-report-figure-2.jpg
         label: Figure 2
         caption: Obtaining a high precision, low recall word alignment by intersecting two GIZA++ alignments
      -
         img: figures/iwslt05-report/iwslt05-report-figure-3.jpg
         label: Figure 3
         caption: Adding additional alignment points. Potential points are points in the union of the two GIZA++ alignments (grey). In the growing step, neighbouring points are added, when they connect at least one unaligned word. In a final step outlying points may be added (see Section 2.3).
      -
         img: figures/iwslt05-report/iwslt05-report-figure-5.jpg
         label: Figure 5
         caption: Definition of consistent word alignments&colon; Words of an extracted phrase pair have to be aligned to each other and nothing else
      -
         img: figures/iwslt05-report/iwslt05-report-figure-4.jpg
         label: Figure 4
         caption: Figure 4&colon; Pseudo-code of the grow-diag-final method to symmetrise word alignments. See Section 2.3 for variations of this method.
      -
         img: figures/iwslt05-report/iwslt05-report-figure-6.jpg
         label: Figure 6
         caption: Possible orientations of phrases&colon; monotone (m), swap (s), or discontinuous (d)
      -
         img: figures/iwslt05-report/iwslt05-report-table-1.jpg
         label: Table 1
         caption: Different word alignment methods and the effect of the phrase table&colon; Since alignment points restrict possible phrase pairs, fewer alignment points lead to larger phrase tables.
      -
         img: figures/iwslt05-report/iwslt05-report-table-2.jpg
         label: Table 2
         caption: BLEU scores for systems trained using different alignment methods
      -
         img: figures/iwslt05-report/iwslt05-report-table-3.jpg
         label: Table 3
         caption: Best lexicalised reordering methods, compared against the baseline (using only distance-based reordering penalty)&colon; Improvements for all language pairs
      -
         img: figures/iwslt05-report/iwslt05-report-table-4.jpg
         label: Table 4
         caption: Optimising the reordering limit (maximum word distance for phrase movement). The table also shows the effect of dropping unknown words instead of passing them to the output.
      -
         img: figures/iwslt05-report/iwslt05-report-table-5.jpg
         label: Table 5
         caption: Official Results&colon; The scores for our official submission to the IWSLT’05 Evaluation Campaign (length penalty in parenthesis), and rank among participants according to the BLEU score.
      -
         img: figures/iwslt05-report/iwslt05-report-table-6.jpg
         label: Table 6
         caption: Optimisation to average reference sentence length instead of shortest reference length (length penalty in parenthesis)&colon; Note the improved length penalties and vastly improved NIST scores. 4 out of 5 BLEU scores are higher as well (exception is Chinese-English).
   abstract: Our participation in the IWSLT 2005 speech translation task is our first effort to work on limited domain speech data. We adapted our statistical machine translation system that performed successfully in previous DARPA competitions on open domain text translations. We participated in the supplied corpora transcription track. We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs. 
   bibtex: |
      @InProceedings{Koehn-EtAl:2005:IWSLT,
         author =  {Philipp Koehn and Amittai Axelrod and Alexandra Birch and Chris Callison-Burch and Miles Osborne and David Talbot and Michael White},
         title =           {Edinburgh System Description for the 2005 {IWSLT} Speech Translation Evaluation},
           booktitle = {Proceedings of International Workshop on Spoken Language Translation},
       year =  {2005},
         url = {http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf}
       }
       
-
   title: Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora
   authors: Chris Callison-Burch, David Talbot and Miles Osborne
   venue: ACL
   type: conference
   year: 2004
   url: https://www.cis.upenn.edu/~ccb/publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf
   page_count: 
   highly_cited: 161
   id: smt-with-word-and-sentence-aligned-parallel-corpora
   figures:
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-1.jpg
         label: Table 1
         caption: Alignment error rates for the various IBM Models trained with sentence-aligned data
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-2.jpg
         label: Table 2
         caption: Alignment error rates for the various IBM Models trained with word-aligned data
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: Example alignments using sentence-aligned training data (a), using word-aligned data (b), and a reference manual alignment (c)
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-3.jpg
         label: Table 3
         caption: The improved alignment error rates when using a dictionary instead of word-aligned data to constrain word translations
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-4.jpg
         label: Table 4
         caption: Improved AER leads to improved translation quality
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-5.jpg
         label: Table 5
         caption: The effect of weighting word-aligned data more heavily that its proportion in the training data (corpus size 16000 sentence pairs)
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: The effect on AER of varying λ for a train- ing corpus of 16K sentence pairs with various pro- portions of word-alignments
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: The effect on AER of varying the ratio of word-aligned to sentence-aligned data
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-figure-4.jpg
         label: Figure 4
         caption: The effect on Bleu of varying the ratio of word-aligned to sentence-aligned data
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-6.jpg
         label: Table 6
         caption: Summary results for AER and translation quality experiments on Hansards data
   abstract: The parameters of statistical translation models are typically estimated from sentence-aligned parallel corpora. We show that significant improvements in the alignment and translation quality of such models can be achieved by additionally including word-aligned data during training. Incorporating word-level alignments into the parameter estimation of the IBM models reduces alignment error rate and increases the Bleu score when compared to training the same models only on sentence-aligned data. On the Verbmobil data set, we attain a 38% reduction in the alignment error rate and a higher Bleu score with half as many training examples. We discuss how varying the ratio of word-aligned to sentence-aligned data affects the expected performance gain. 
   bibtex: |
      @inproceedings{callisonburch-talbot-osborne:2004:ACL,
         author    = {Callison-Burch, Chris  and  Talbot, David  and  Osborne, Miles},
         title     = {Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora},
         booktitle = {Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL'04), Main Volume},
         year      = {2004},
         month     = {July},
         address   = {Barcelona, Spain},
         pages     = {175--182},
         url       = {http://www.aclweb.org/anthology/P04-1023},
       }
       
-
   title: Searchable Translation Memories
   authors: Chris Callison-Burch, Colin Bannard and Josh Schroeder
   venue: ASLIB Translating and the Computer
   type: workshop
   year: 2004
   url: https://www.cis.upenn.edu/~ccb/publications/searchable-translation-memories.pdf
   page_count: 
   id: searchable-translation-memories
   figures:
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-1.jpg
         label: Figure 1
         caption: Search results for the French phrase “paiement initial”
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-2.jpg
         label: Figure 2
         caption: A word-level alignment for a sentence pair that occurs in our training data
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-3.jpg
         label: Figure 3
         caption: Extracting incrementally larger phrases from a word alignment
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-4.jpg
         label: Figure 4
         caption: A sample of the evaluation data used to produce precision and recall results
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-5.jpg
         label: Figure 5
         caption: A chart of the various sub-sentential segments that were retrieved for an Arabic sentence, along with their associated probabilities
   abstract: In this paper we introduce a technique for creating searchable translation memories. Linear B’s searchable translation memories allow a translator to type in a phrase and retrieve a ranked list of possible translations for that phrase, which is ordered based on the likelihood of the translations. The searchable translation memories use translation models similar to those used in statistical machine translation. In this paper we first describe the technical details of how the TMs are indexed and how translations are assigned probabilities, and then evaluate a searchable TM using precision and recall metrics. 
   bibtex: |
      @inproceedings{Callison-Burch:2004:ASLIB,
        author =  {Chris Callison-Burch and Colin Bannard and Josh Schroeder},
           title = {Searchable Translation Memories},
           booktitle = {Proceedings of ASLIB Translating and the Computer 26}, 
           year = {2004}
       }
       
-
   title: Improved Statistical Translation Through Editing
   authors: Chris Callison-Burch, Colin Bannard and Josh Schroeder
   venue: EAMT
   type: workshop
   year: 2004
   url: https://www.cis.upenn.edu/~ccb/publications/improved-smt-through-editing.pdf
   page_count: 
   id: improved-smt-through-editing
   figures:
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-1.jpg
         label: Figure 1
         caption: A word-level alignment for a sentence pair that occurs in our training data
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-2.jpg
         label: Figure 2
         caption: Extracting incrementally larger phrases from a word alignment
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-3.jpg
         label: Figure 3
         caption: An example of the phrases that were used to translate two sentences
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-4.jpg
         label: Figure 4
         caption: Word alignment produced by the alignment server for an edited translation
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-5.jpg
         label: Figure 5
         caption: Word alignment produced by the alignment server for an edited translation
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-6.jpg
         label: Figure 6
         caption: A software tool which allows an advanced user to correct misaligned sentence pairs from the training data
   abstract: In this paper we introduce Linear B’s statistical machine translation system. We describe how Linear B’s phrase-based translation models are learned from a parallel corpus, and show how the quality of the translations produced by our system can be improved over time through editing. There are two levels at which our translations can be edited. The first is through a simple correction of the text that is produced by our system. The second is through a mechanism which allows an advanced user to examine the sentences that a particular translation was learned from. The learning process can be improved by correcting which phrases in the sentence should be considered translations of each other. 
   bibtex: |
      @inproceedings{Callison-Burch-EtAl:2004:EAMT,
        author =  {Chris Callison-Burch and Colin Bannard and Josh Schroeder},
           title = {Improved Statistical Translation Through Editing},
           booktitle = {European Association for Machine Translation}, 
           year = {2004}
       }
       
-
   title: Statistical Natural Language Processing
   authors: Chris Callison-Burch and Miles Osborne
   venue: A Handbook for Language Engineers, Ali Farghaly, Editor
   type: chapter
   year: 2003
   url: https://www.cis.upenn.edu/~ccb/publications/statistical-natural-language-processing-chapter.pdf
   page_count: 
   id: statistical-natural-language-processing-chapter
   figures:
      -
         img: figures/statistical-natural-language-processing-chapter/statistical-natural-language-processing-chapter-figure-1.jpg
         label: Figure 1
         caption: Two possible parses (T1 and T2) for “Do you sell Apple laptops?”.
      -
         img: figures/statistical-natural-language-processing-chapter/statistical-natural-language-processing-chapter-figure-2.jpg
         label: Figure 2
         caption: A very simple probabilistic grammar for English
      -
         img: figures/statistical-natural-language-processing-chapter/statistical-natural-language-processing-chapter-figure-3.jpg
         label: Figure 3
         caption: Various commonly used corpora
      -
         img: figures/statistical-natural-language-processing-chapter/statistical-natural-language-processing-chapter-figure-4.jpg
         label: Figure 4
         caption: Two possible parses (T1 and T2) for \Do you sell Apple laptops?".
   abstract: Statistical natural language processing (SNLP) is a field lying in the intersection of natural language processing and machine learning. SNLP differs from traditional natural language processing in that instead of having a linguist manually construct some model of a given linguistic phenomenon, that model is instead (semi-) automatically constructed from linguistically annotated resources. Methods for assigning partof-speech tags to words, categories to texts, parse trees to sentences, and so on, are (semi-) automatically acquired using machine learning techniques.The recent trend of applying statistical techniques to natural language processing came largely from industrial speech recognition research groups at AT&T's Bell Laboratories and IBM's T.J. Watson Research Center. Statistical techniques in speech recognition have so vastly outstripped the performance of their non-statistical counterparts that rule-based speech recognition systems are essentially no longer an area of research. The success of machine learning techniques in speech processing led to an interest in applying them to a broader range of NLP applications. In addition to being useful from the perspective of producing high-quality results, as in speech recognition, SNLP systems are useful for a number of practical reasons. They are cheap and fast to produce, and they handle the wide variety of input required by a real-world application. SNLP is therefore especially useful in industry. In particular&colon;SNLP affords rapid prototyping. Whereas fully hand-crafted systems are extremely time consuming to build, statistical systems that are automatically trained using corpora can be produced more quickly. This allows many different approaches to be tried and evaluated in a short time-frame. As an example, Cucerzan and Yarowsky described how one might create a new part-of-speech tagger in a single day (Cucerzan and Yarowsky, 2002). An even more ambitious example is Al-Onaizan et al.'s "machine translation in a day" experiment wherein they used statistical techniques to develop a complete Chinese-English machine translation system in a 24-hour period (AlOnaizan et al., 1999).Statistical systems are "robust" (Junqua and van Noord, 2001). Although this has a wide variety of meanings, in SNLP it generally means that a system will always produce some output no matter how badly formed the input is, and no matter how novel it is. For example, a text classification system may be able to classify a text even if all of the words in that text are previously unseen. Handling all kinds of input is necessary in real-world applications; a system which fails to produce output when it is unable to analyze a sentence will not be useful.Statistical systems are often cheaper to produce than hand-crafted rule-based systems. Because the process of creating a statistical system is more automated than the process of creating a rule-based system, the actual number of participants needed to create a system will often be less. Furthermore, because they are learned from data, statistical systems require less knowledge of the particular language being analyzed. This becomes a budgetary issue on a multi-language project because of the expense of hiring language consultants or staff with specialized skills.A common theme with many early SNLP systems was a pride in minimizing the amount of linguistic knowledge used in the system. For example, Fred Jelinek, the then leader of IBM's speech recognition research group, purportedly said, "Every time I fire a linguist, my performance goes up." The sentiment is rather shocking. Should Jelinek's statement strike fear into the hearts of all linguists reading this chapter? Is there a strong opposition between theoretical linguistics and SNLP? Will SNLP put linguists out of work?We put forth a positive answer in this chapter&colon; there is a useful role for linguistic expertise in statistical systems. Jelinek's infamous quote represents biases of the early days of SNLP. While a decade's worth of research has shown that SNLP can be an extremely powerful tool and is able to produce impressive results, recent trends indicate that using naive approaches that are divorced from linguistics can only go so far. There is therefore a revival of interest in integrating more sophisticated linguistic information into statistical models. For example, language models for speech recognition are moving from being word-based "ngram" models towards incorporating statistical grammars (Chelba and Jelinek, 1998, Charniak, 2001). So there is indeed a role for the linguist. This chapter will provide an entry point for linguists entering into the field of SNLP so that they may apply their expertise to enhance an already powerful approach to natural language processing.Lest we represent SNLP as a completely engineering-oriented discipline, we point the interested reader to Abney (1996) which describes a number of ways in which SNLP might inform academic topics in linguistics. For example, SNLP can be useful for psycholinguistic research since systems typically encode graduated notions of well-formedness. This offers a more psychologically plausible alternative to the traditional binary grammatical/ungrammatical distinction. In a similarly academic vein, Johnson (1998) shows how Optimality Theory can be interpreted in terms of statistical models. This in turn suggests a number of interesting directions that OT might take.The rest of this chapter is as follows&colon; We begin by presenting a simple worked example designed to illustrate some of the aspects of SNLP in Section 1.2. After motivating the usefulness of SNLP, we then move onto the core methods used in SNLP&colon; modeling, learning, data and evaluation (Sections 1.3, 1.4, 1.5, and 1.6 respectively). These core methods are followed by a brief review of some of the many applications of SNLP (Section 1.7). We conclude with a discussion (Section 1.8) where we make some comments about the current state of SNLP and possible future directions it might take.
   bibtex: |
      @incollection{Callison-Burch2003b,
       author = {Chris Callison-Burch and Miles Osborne},
       title = {Statistical Natural Language Processing},
       booktitle = {A Handbook for Language Engineers},
       editor = {Ali Farghaly},
       publisher = {CSLI},
       year = {2003}
       }
       
-
   title: Bootstrapping Parallel Corpora
   authors: Chris Callison-Burch and Miles Osborne
   venue: NAACL workshop Building and Using Parallel Texts
   type: workshop
   year: 2003
   url: https://www.cis.upenn.edu/~ccb/publications/bootstrapping-parallel-corpora.pdf
   page_count: 6
   id: bootstrapping-parallel-corpora
   figures:
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: Translation accuracy plotted against training corpus size
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: Co-training using German, French, and Spanish sources to produce English machine translations
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-table-1.jpg
         label: Table 1
         caption: Co-training results over three rounds
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: “Coaching” of German to English by a French to English translation model
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-figure-4.jpg
         label: Figure 4
         caption: “Coaching” of German to English by multiple translation models
   abstract: We present two methods for the automatic creation of parallel corpora. Whereas previous work into the automatic construction of parallel corpora has focused on harvesting them from the web, we examine the use of existing parallel corpora to bootstrap data for new language pairs. First, we extend existing parallel corpora using co-training, wherein machine translations are selectively added to training corpora with multiple source texts. Retraining translation models yields modest improvements. Second, we simulate the creation of training data for a language pair for which a parallel corpus is not available. Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. This suggests the method may be useful in the creation of parallel corpora for languages with scarce resources. 
   bibtex: |
      @inproceedings{CallisonBurch-Osborne:2003:PARALLEL,
         author = {Callison-Burch, Chris  and  Osborne, Miles},
         title  = {Bootstrapping Parallel Corpora},
         booktitle = {Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond},
         editor = {Rada Mihalcea and Ted Pedersen},
         url    = {http://www.aclweb.org/anthology/W03-0310},
         year   = 2003,
         pages  = {44--49}
       }
       
-
   title: Co-training for Statistical Machine Translation
   authors: Chris Callison-Burch and Miles Osborne
   venue: the 6th Annual CLUK Research Colloquium
   type: workshop
   year: 2003
   url: https://www.cis.upenn.edu/~ccb/publications/co-training-for-smt.pdf
   id: co-training-for-smt
   figures:
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-1.jpg
         label: Figure 1
         caption: Translation accuracy plotted against training corpus size.
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-2.jpg
         label: Figure 2
         caption: Co-training using German, French, and Spanish sources as views on English translations
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-3.jpg
         label: Figure 3
         caption: The co-training algorithm for machine translation
      -
         img: figures/co-training-for-smt/co-training-for-smt-table-1.jpg
         label: Table 1
         caption: Co-training results over three rounds
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-4.jpg
         label: Figure 4
         caption: “Coaching” of German to English by a French to English translation model
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-5.jpg
         label: Figure 5
         caption: “Coaching” of German to English by multiple translation models
   abstract: We present a novel co-training method for statistical machine translation. Since cotraining requires independent views on the data, with each view being sufficient for the labeling task, we use source strings in multiple languages as views on translation. Co-training for statistical machine translation is therefore a type of multi-source translation. We show that using five language pairs our approach can yield improvements of up to 2.5% in word error rates for translation models. Our experiments suggest that co-training is even more effective for languages with highly impoverished parallel corpora&colon; starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. 
   bibtex: |
      @inproceedings{CallisonBurch-Osborne:2003:CLUK,
         author = {Callison-Burch, Chris  and  Osborne, Miles},
         title  = {Co-Training For Statistical Machine Translation},
         booktitle = {Proceedings of the 6th Annual CLUK Research Colloquium},
         year   = {2003}
       }
       
-
   title: Evaluating Question Answering Systems Using FAQ Answer Injection
   authors: Jochen Leidner and Chris Callison-Burch
   venue: the 6th Annual CLUK Research Colloquium
   type: workshop
   year: 2003
   url: https://www.cis.upenn.edu/~ccb/publications/evaluating-question-answering-systems-using-faq-answer-injection.pdf
   page_count: 
   id: evaluating-question-answering-systems-using-faq-answer-injection
   figures:
      -
         img: figures/evaluating-question-answering-systems-using-faq-answer-injection/evaluating-question-answering-systems-using-faq-answer-injection-figure-1.jpg
         label: Figure 1
         caption: FAQ Answer Injection.
      -
         img: figures/evaluating-question-answering-systems-using-faq-answer-injection/evaluating-question-answering-systems-using-faq-answer-injection-figure-2.jpg
         label: Figure 2
         caption: Detection of Components Responsible for Recall Drop.
   abstract: Question answering (NLQA) systems which retrieve a textual fragment from a document collection that represents the answer to a question are an active field of research. But evaluations currently involve a large amount of manual effort. We propose a new evaluation scheme that uses the insertion of answers from Frequently Asked Questions collections (FAQs) to measure the ability of a system to retrieve it from the corresponding question. We describe how the usefulness of the approach can be assessed and discuss advantages and problems. 
   bibtex: |
      @inproceedings{Leidner-CallisonBurch:2003:CLUK,
         author = {Jochen L. Leidner and Chris Callison-Burch},
         title  = {Evaluating Question Answering Systems Using FAQ Answer Injection},
         booktitle = {Proceedings of the 6th Annual CLUK Research Colloquium},
         year   = {2003}
       }
       
-
   title: Co-Training for Statistical Machine Translation
   authors: Chris Callison-Burch
   venue: Master's thesis, School of Informatics, University of Edinburgh
   type: thesis
   year: 2002
   url: https://www.cis.upenn.edu/~ccb/publications/msc-thesis.pdf
   page_count: 64 
   id: msc-thesis
   abstract: I propose a novel co-training method for statistical machine translation. As co-training requires multiple learners trained on views of the data which are disjoint and sufficient for the labeling task, I use multiple source documents as views on translation. Co-training for statistical machine translation is therefore a type of multi-source translation. Unlike previous mutli-source methods, it improves the overall quality of translations produced by a model, rather than single translations. This is achieved by augmenting the parallel corpora on which the statistical translation models are trained. Experiments suggest that co-training is especially effective for languages with highly impoverished parallel corpora. 
   bibtex: |
      @MastersThesis{Callison-Burch2002,
         author =       {Chris Callison-Burch},
         title =        {Co-training for Statistical Machine Translation},
         school =       {University of Edinburgh},
         year =         {2002}
       }
       
-
   title: Upping the Ante for "Best of Breed" Machine Translation Providers
   authors: Chris Callison-Burch
   venue: ASLIB Translating and the Computer
   type: workshop
   year: 2001
   url: https://www.cis.upenn.edu/~ccb/publications/upping-the-ante.pdf
   page_count: 
   id: upping-the-ante
   figures:
      -
         img: figures/upping-the-ante/upping-the-ante-table-1.jpg
         label: Table 1
         caption: Japanese to English Chat Sentences
      -
         img: figures/upping-the-ante/upping-the-ante-table-2.jpg
         label: Table 2
         caption: French to English Web Page Sentences
      -
         img: figures/upping-the-ante/upping-the-ante-table-3.jpg
         label: Table 3
         caption: French to English Chat Sentences
      -
         img: figures/upping-the-ante/upping-the-ante-table-4.jpg
         label: Table 4
         caption: English to French Sentences (Chat and Web Page)
   abstract: The notion of "best of breed" among value-added machine translation technology providers is generally defined as providing access to the single best commercially available machine translation engine for each language pair. This paper describes the efforts of Amikai, Inc. to go beyond that definition of best of breed. Rather than relying on a single engine for each pair, we have written a program that automatically selects the best translation from a set of candidate translations generated by multiple commercial machine translation engines. The program is implemented using a simple statistical language modelling technique, and relies on the simplifying assumption that the most fluent item in the set is the best translation. The program was able to produce the best translation in human ranked data up to 19% more often than the single best performing engine. 
   bibtex: |
      @inproceedings{Callison-Burch:2001:ASLIB,
         title =               {Upping the Ante for "Best of Breed" Machine Translation Providers},
         author =              {Chris Callison-Burch},
         booktitle =   {Proceedings of ASLIB Translating and the Computer 23},
         year =                {2001},
       }
       
-
   title: A program for automatically selecting the best output from multiple machine translation engines
   authors: Chris Callison-Burch and Raymond Flournoy
   venue: MT Summit
   type: workshop
   year: 2001
   url: https://www.cis.upenn.edu/~ccb/publications/multi-engine-mt-with-language-models.pdf
   page_count: 
   id: multi-engine-mt-with-language-models
   figures:
      -
         img: figures/multi-engine-mt-with-language-models/multi-engine-mt-with-language-models-table-1.jpg
         label: Table 1
         caption: Japanese
      -
         img: figures/multi-engine-mt-with-language-models/multi-engine-mt-with-language-models-table-2.jpg
         label: Table 2
         caption: French
      -
         img: figures/multi-engine-mt-with-language-models/multi-engine-mt-with-language-models-table-3.jpg
         label: Table 3
         caption: French
      -
         img: figures/multi-engine-mt-with-language-models/multi-engine-mt-with-language-models-table-4.jpg
         label: Table 4
         caption: English
   abstract: This paper describes a program that automatically selects the best translation from a set of translations produced by multiple commercial machine translation engines. The program is simplified by assuming that the most fluent item in the set is the best translation. Fluency is determined using a trigram language model. Results are provided illustrating how well the program performs for human ranked data as compared to each of its constituent engines. 
   bibtex: |
      @inproceedings{Callison-Burch-Flournoy:2001:MTSummit,
         title =               {A Program for Automatically Selecting the Best Output from Multiple Machine Translation Engines},
         author =              {Chris Callison-Burch and Raymond S. Flournoy},
         booktitle =   {Proceedings of the Machine Translation Summit VIII},
         year =                {2001},
       }
       
-
   title: Secondary Benefits of Feedback and User Interaction in Machine Translation Tools
   authors: Raymond Flournoy and Chris Callison-Burch
   venue: MT Summit Workshop
   type: workshop
   year: 2001
   url: https://www.cis.upenn.edu/~ccb/publications/secondary-benefits-of-user-feedback-in-mt.pdf
   page_count: 
   id: secondary-benefits-of-user-feedback-in-mt
   figures:
      -
         img: figures/secondary-benefits-of-user-feedback-in-mt/secondary-benefits-of-user-feedback-in-mt-figure-1.jpg
         label: Figure 1
         caption: The “Huh?” button in AmiChat
      -
         img: figures/secondary-benefits-of-user-feedback-in-mt/secondary-benefits-of-user-feedback-in-mt-figure-2.jpg
         label: Figure 2
         caption: AmiWeb
   abstract: User feedback has often been proposed as a method for improving the accuracy of machine translation systems, but useful feedback can also serve a number of secondary benefits, including increasing user confidence in the MT technology and expanding the potential audience of users. Amikai, Inc. has produced a number of communication tools which embed translation technology and which attempt to improve the user experience by maximizing useful user interaction and feedback. As MT continues to develop, further attention needs to be paid to developing the overall user experience, which can improve the utility of translation tools even when translation quality itself plateaus 
   bibtex: |
      @inproceedings{Flournoy-Callison-Burch:2001:MTSummit,
         title =               {Secondary Benefits of Feedback and User Interaction in Machine Translation Tools},
         author =              {Raymond S. Flournoy and Chris Callison-Burch},
         booktitle =   {Workshop paper for "MT2010: Towards a Roadmap for MT" of the MT Summit VIII},
         year =                {2001},
       }
       
-
   title: A Computer Model of a Grammar for English Questions
   authors: Chris Callison-Burch
   venue: Undergraduate thesis, Symbolic Systems Program, Stanford University
   type: thesis
   year: 2000
   url: https://www.cis.upenn.edu/~ccb/publications/computer-model-of-a-grammar-for-english-questions.pdf
   page_count: 78
   id: computer-model-of-a-grammar-for-english-questions
   figures:
   abstract: This document describes my senior honors project, which is an implementation of a grammar for English questions. I have created a computer model of Ginzburg and Sag’s theory of English interrogative constructions using the parsing software developed at the Center for Study of Language and Information (CSLI). In this chapter I describe the LKB parsing software, give instructions on downloading the system, and comment on the process of grammar engineering. The next chapter gives a summary of Ginzburg and Sag (2000). Chapter 3 details the discrepancies between the Ginzburg and Sag theory and my implementation. Chapter 4 provides a detailed discussion of a set of key example sentences. The appendices contain tables describing all the grammar constructions, lexical rules, types, and example lexical entries used in my implementation. 
   bibtex: |
      @MISC{Callison-Burch2000,
         author =  {Chris Callison-Burch},
         title =   {A Computer Model of a Grammar for English Questions},
         school = {Stanford University},
         address =   {Palo Alto, California},
         note = {Undergraduate honors thesis},
         year =    {2000}
       }
       
